{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/GroupedSAEs/groups/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/tokenization/tokenization.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  neo_tok_ids_to_ts = torch.load(f\"{current_dir}/neo_tok_ids_to_ts.pt\")\n",
      "/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/tokenization/tokenization.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ts_tok_ids_to_neo = torch.load(f\"{current_dir}/ts_tok_ids_to_neo.pt\")\n",
      "/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/lm.py:403: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TinyModel(\n",
       "  (embed): Embedding(10000, 768)\n",
       "  (torso): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jannik's models are not SAE Lens, so use Baukit\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# tiny model is in a different direction, add current one to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Get the parent directory of current_dir\n",
    "parent_dir = str(Path(\"/root/GroupedSAEs/notebooks/tiny_circuits.ipynb\").parent.parent)\n",
    "sys.path.append(parent_dir)\n",
    "# from tiny_model.tiny_model import TinyModel, tokenizer\n",
    "# from nnsight import NNsight\n",
    "from tiny_model import TinyModel, tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyModel().to(device)\n",
    "\n",
    "model.to(torch.bfloat16)\n",
    "# nn_model = NNsight(model)\n",
    "# model.nnsight_proxy = nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading noanabeshima/TinyStoriesV2\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 643.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to tokenize 0 tokens\n",
      "Number of datapoints w/ 51 tokens: 1000\n",
      "Total Tokens: 0.051M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "def download_dataset(dataset_name, tokenizer, max_length=256, num_datapoints=None):\n",
    "    if(num_datapoints):\n",
    "        split_text = f\"train[:{num_datapoints}]\"\n",
    "    else:\n",
    "        split_text = \"train\"\n",
    "    dataset = load_dataset(dataset_name, split=split_text)\n",
    "    print(dataset)\n",
    "    total_failed_tokens = 0\n",
    "    all_tokens = []\n",
    "    for text in tqdm(dataset[\"text\"]):\n",
    "        try:\n",
    "            tokens = [9996] + tokenizer.encode(text)[:max_length]\n",
    "        except:\n",
    "            total_failed_tokens += 1\n",
    "            continue\n",
    "        # only include if it's at least max_length\n",
    "        if len(tokens) == max_length+1:\n",
    "            all_tokens.append(tokens)\n",
    "    print(f\"Failed to tokenize {total_failed_tokens} tokens\")\n",
    "    # convert into a dataset class\n",
    "    return  Dataset.from_dict({\"input_ids\": all_tokens})\n",
    "\n",
    "\n",
    "dataset_name = \"noanabeshima/TinyStoriesV2\"\n",
    "max_seq_length = 50\n",
    "num_datapoints = 1000\n",
    "# max_seq_length = 100\n",
    "# num_datapoints = 500\n",
    "print(f\"Downloading {dataset_name}\")\n",
    "dataset = download_dataset(dataset_name, tokenizer=tokenizer, max_length=max_seq_length, num_datapoints=num_datapoints)\n",
    "true_num_datapoints = len(dataset)\n",
    "# added BOS\n",
    "max_seq_length +=1\n",
    "total_tokens = max_seq_length * true_num_datapoints\n",
    "print(f\"Number of datapoints w/ {max_seq_length} tokens: {true_num_datapoints}\")\n",
    "print(f\"Total Tokens: {total_tokens / 1e6}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "#set torch grad to zero globally\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from tiny_model import parse_mlp_tag, SparseMLP\n",
    "N = 1\n",
    "# activation_names = [f\"A{N}\", f\"T{N}\"]\n",
    "# activation_names = [f\"A{N}\", f\"A{N+1}\"]\n",
    "# activation_names = [f\"T{N}\", f\"T{N+1}\"]\n",
    "activation_names = [f\"Rm{N}_S-3_R1_P0\", f\"T{N}\"]\n",
    "file, _, _, _ = parse_mlp_tag(activation_names[0])\n",
    "sae_res = SparseMLP.from_pretrained(file).to(device).to(torch.bfloat16)\n",
    "file, _, _, _ = parse_mlp_tag(activation_names[1])\n",
    "skip_sae = SparseMLP.from_pretrained(file).to(device).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae_res_decoder = sae_res.decoder.weight.data\n",
    "# sae_skip_encoder = skip_sae.encoder.weight.data\n",
    "# sae_res_decoder = sae_res_decoder / sae_res_decoder.norm(dim=0, keepdim=True)\n",
    "# sae_skip_encoder = sae_skip_encoder / sae_skip_encoder.norm(dim=1, keepdim=True)\n",
    "# sae_res_decoder.shape, sae_skip_encoder.shape\n",
    "# cos_sim = sae_res_decoder.T @ sae_skip_encoder.T\n",
    "# max_cos_sim = cos_sim.max(dim=-1).values.cpu().numpy()\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.hist(max_cos_sim, bins=100)\n",
    "# plt.title(f\"Cosine Similarity between {activation_names[0]}-Dec and {activation_names[1]}-Enc\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "100%|██████████| 63/63 [04:31<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "skip_first_n_pos = 0\n",
    "# def get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32):\n",
    "num_features, d_model  = sae_res.encoder.weight.data.shape\n",
    "datapoints = dataset.num_rows\n",
    "dictionary_activations_res = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "dictionary_activations_skip = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "all_dictionary_activations = [dictionary_activations_res, dictionary_activations_skip]\n",
    "token_list = torch.zeros((datapoints*max_seq_length), dtype=torch.int64)\n",
    "\n",
    "\n",
    "residual_and_skip_correlation = torch.zeros((num_features, num_features), dtype=torch.int32)\n",
    "residual_correlation = torch.zeros((num_features, num_features), dtype=torch.int32)\n",
    "skip_correlation = torch.zeros((num_features, num_features), dtype=torch.int32)\n",
    "\n",
    "feature_mse = torch.zeros(datapoints*max_seq_length)\n",
    "feature_var_explained = torch.zeros(datapoints*max_seq_length)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(model.device)\n",
    "        token_list[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "        for sae_ind, sae in enumerate([sae_res, skip_sae]):\n",
    "            feature_acts = model[activation_names[sae_ind]](batch)\n",
    "            feature_acts[:, :skip_first_n_pos] = 0\n",
    "            feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\" )\n",
    "\n",
    "            all_dictionary_activations[sae_ind][batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = feature_acts.cpu()\n",
    "            if sae_ind==0:\n",
    "                reconstruction_residual = sae.decoder(feature_acts)\n",
    "                first_feature_mask = (feature_acts !=0).float()\n",
    "                residual_correlation += torch.mm(first_feature_mask.T, first_feature_mask).int().cpu()\n",
    "            else:\n",
    "                second_feature_mask = (feature_acts !=0).float()\n",
    "                skip_correlation += torch.mm(second_feature_mask.T, second_feature_mask).int().cpu()\n",
    "\n",
    "                # Calculate the correlation between the residual and skip activations\n",
    "                residual_and_skip_correlation += torch.mm(first_feature_mask.T, second_feature_mask).int().cpu()\n",
    "\n",
    "\n",
    "                # Check the MSE between the skip features & skip-features-w/-residual\n",
    "                skip_feature_x_hat = skip_sae.encoder(reconstruction_residual)\n",
    "                skip_feature_mse = torch.mean((skip_feature_x_hat - feature_acts)**2, dim=-1)\n",
    "                feature_act_var = torch.var(feature_acts)\n",
    "                skip_feature_var_explained = 1 - skip_feature_mse / feature_act_var\n",
    "                feature_mse[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = skip_feature_mse.cpu()\n",
    "                feature_var_explained[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = skip_feature_var_explained.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=25000, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=25000, bias=True),\n",
       " torch.Size([768, 25000]),\n",
       " torch.Size([25000, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_res.decoder, skip_sae.encoder, sae_res.decoder.weight.shape, sae_res.encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:06<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Attribution shape: torch.Size([25000, 25000])\n",
      "Average Attribution (first few elements):\n",
      " tensor([[ 0.0000e+00, -1.8705e-04, -1.6569e-03, -1.2144e-03, -2.4749e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-8.3330e-10,  2.8526e-04,  2.9122e-04, -1.8285e-04, -2.1777e-05],\n",
      "        [-8.0611e-08,  1.2322e-02,  1.0993e-01, -2.4259e-04,  3.1488e-05],\n",
      "        [ 0.0000e+00, -1.3069e-06, -4.6021e-06,  0.0000e+00,  1.0446e-07]])\n",
      "Token list shape: torch.Size([51000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, interpolated_input):\n",
    "    interpolated_input.requires_grad_(True)\n",
    "    \n",
    "    masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices]\n",
    "    masked_decoder_bias = sae_res.decoder.bias\n",
    "    masked_encoder_weight = skip_sae.encoder.weight[output_nonzero_indices, :]\n",
    "    masked_encoder_bias = skip_sae.encoder.bias[output_nonzero_indices]\n",
    "    \n",
    "    decoder_output = F.linear(interpolated_input, masked_decoder_weight, masked_decoder_bias)\n",
    "    encoder_output = F.linear(decoder_output, masked_encoder_weight, masked_encoder_bias)\n",
    "    encoder_output = F.relu(encoder_output)\n",
    "    \n",
    "    attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=interpolated_input.device)\n",
    "    for i in range(len(output_nonzero_indices)):\n",
    "        grad = torch.autograd.grad(encoder_output[:, i].sum(), interpolated_input, retain_graph=True)[0]\n",
    "        attribution[i] = (grad * interpolated_input).sum(dim=0)\n",
    "    \n",
    "    return attribution\n",
    "\n",
    "def compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length):\n",
    "    device = model.device\n",
    "    attribution_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        feature_acts = model[activation_names[0]](batch)\n",
    "        feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\")\n",
    "\n",
    "        # Identify non-zero input elements\n",
    "        input_nonzero_indices = get_nonzero_indices(feature_acts)\n",
    "\n",
    "        # Perform a forward pass to identify non-zero output elements\n",
    "        with torch.no_grad():\n",
    "            relu = torch.nn.ReLU()\n",
    "            initial_output = relu(skip_sae.encoder(sae_res.decoder(feature_acts)) + skip_sae.encoder.bias)\n",
    "        output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "        # print(f\"shape of input activations: {feature_acts.shape}\")\n",
    "        # print(f\"shape of initial output activations: {initial_output.shape}\")\n",
    "        # print(f\"number of nonzero input indices: {input_nonzero_indices.shape}\")\n",
    "        # print(f\"number of nonzero output indices: {output_nonzero_indices.shape}\")\n",
    "        # print(\"input-output shape: \", input_nonzero_indices.shape, output_nonzero_indices.shape)\n",
    "\n",
    "        # Extract non-zero input values\n",
    "        nonzero_input = feature_acts[:, input_nonzero_indices]\n",
    "        \n",
    "        # Integrated Gradients with Zero Ablation\n",
    "        batch_attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=device)\n",
    "        \n",
    "        for alpha in np.linspace(0, 1, 7):\n",
    "            interpolated_input = alpha * nonzero_input\n",
    "            \n",
    "            # Compute attribution for interpolated input\n",
    "            interp_attribution = compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, interpolated_input)\n",
    "            \n",
    "            batch_attribution += interp_attribution\n",
    "        \n",
    "        # Average the attribution\n",
    "        batch_attribution /= 7\n",
    "\n",
    "        # Assertions for shape verification\n",
    "        assert batch_attribution.shape == (len(output_nonzero_indices), len(input_nonzero_indices)), f\"batch_attribution shape {batch_attribution.shape} doesn't match expected shape {(len(output_nonzero_indices), len(input_nonzero_indices))}\"\n",
    "\n",
    "        # Update attribution sum\n",
    "        if attribution_sum is None:\n",
    "            attribution_sum = torch.zeros(skip_sae.encoder.weight.shape[0], sae_res.decoder.weight.shape[1], device=\"cpu\")\n",
    "        \n",
    "        output_indices = output_nonzero_indices.cpu().unsqueeze(1).expand(-1, len(input_nonzero_indices))\n",
    "        input_indices = input_nonzero_indices.cpu().unsqueeze(0).expand(len(output_nonzero_indices), -1)\n",
    "        attribution_sum[output_indices, input_indices] += batch_attribution.detach().cpu()\n",
    "\n",
    "        sample_count += nonzero_input.shape[0]\n",
    "\n",
    "        # Print out memory usage\n",
    "        # print(f\" Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Normalize the attribution sum by the total number of samples\n",
    "    average_attribution = attribution_sum / sample_count\n",
    "\n",
    "    return average_attribution\n",
    "\n",
    "# Usage\n",
    "average_attribution = compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\n",
    "\n",
    "print(\"Average Attribution shape:\", average_attribution.shape)\n",
    "print(\"Average Attribution (first few elements):\\n\", average_attribution[:5, :5])\n",
    "print(\"Token list shape:\", token_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcoder/outpu:  tensor(77)\n",
      "torch.return_types.topk(\n",
      "values=tensor([3.6365e-01, 1.1933e-07, 1.8892e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([  1, 105, 489,   8,   7,   3,   4,   9,   5,   0]))\n",
      "Transcoder/outpu:  tensor(104)\n",
      "torch.return_types.topk(\n",
      "values=tensor([2.5870e-01, 2.5823e-05, 1.5498e-05, 2.8637e-09, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([    4, 12227,  1424,  1273,     1,     7,     8,     0,     9,     5]))\n",
      "Transcoder/outpu:  tensor(86)\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.2377, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000]),\n",
      "indices=tensor([ 6,  4,  8,  9,  7,  1,  0, 10,  2,  5]))\n",
      "Transcoder/outpu:  tensor(91)\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.6202e-01, 2.8668e-06, 1.8054e-06, 2.2294e-07, 6.0864e-08, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([   11, 10707,  2762,  1841,  1671,     7,     1,     8,     0,     4]))\n",
      "Transcoder/outpu:  tensor(2)\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.0993e-01, 2.9122e-04, 2.7641e-04, 1.2090e-04, 9.4362e-05, 8.4088e-05,\n",
      "        4.9720e-05, 4.1590e-05, 3.8395e-05, 3.1055e-05]),\n",
      "indices=tensor([   3,    2, 1644,  347,  289, 2228, 3247,   34,  748,  597]))\n",
      "Transcoder/outpu:  tensor(101)\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.0933e-01, 6.6089e-06, 1.4908e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([ 10, 574,   5,   8,   7,   1,   0,   4,   9,   2]))\n",
      "Transcoder/outpu:  tensor(12)\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.0155e-01, 6.1835e-04, 5.6653e-04, 3.6850e-04, 2.2956e-04, 1.7603e-04,\n",
      "        1.5894e-04, 1.4844e-04, 1.3629e-04, 8.1889e-05]),\n",
      "indices=tensor([  0, 147, 350, 141, 252,  69, 113, 201, 248, 816]))\n",
      "Transcoder/outpu:  tensor(287)\n",
      "torch.return_types.topk(\n",
      "values=tensor([7.6101e-02, 9.4113e-05, 7.2216e-06, 6.3542e-06, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([ 7, 25, 36,  3,  8,  4,  9,  1,  5,  0]))\n",
      "Transcoder/outpu:  tensor(263)\n",
      "torch.return_types.topk(\n",
      "values=tensor([6.8140e-02, 8.7447e-05, 2.4335e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([ 7, 25, 35,  8,  4,  9,  1,  0,  5, 10]))\n",
      "Transcoder/outpu:  tensor(115)\n",
      "torch.return_types.topk(\n",
      "values=tensor([6.6594e-02, 2.5155e-04, 8.1825e-06, 2.9105e-06, 4.5135e-08, 2.7568e-09,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([ 16, 158,   3, 821, 362,  25,   7,   8,   1,   4]))\n"
     ]
    }
   ],
   "source": [
    "top_ind = average_attribution.max(dim=0).values.topk(10).indices\n",
    "for t_i in top_ind:\n",
    "    print(\"Transcoder/outpu: \", t_i)\n",
    "    print(average_attribution[:, t_i].topk(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot as hist\n",
    "import matplotlib.pyplot as plt\n",
    "nz_att = (average_attribution.abs() > 0).sum(dim=0).cpu().numpy()\n",
    "plt.hist(nz_att, bins=np.logspace(0, 3, 100))\n",
    "plt.title(\"Number of non-zero attributions per feature\")\n",
    "# x on log scale\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save average_attribution\n",
    "torch.save(average_attribution, \"average_attribution.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([2.2425e-01, 2.1201e-04, 1.1401e-05, 5.1755e-06, 1.8601e-06, 1.0901e-06,\n",
       "         7.3858e-07, 4.8418e-07, 4.7597e-07, 1.8464e-07]),\n",
       " indices=tensor([    6,     1,     4, 12227,  3353,     8, 11682,  6176, 17684,  1607])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([0.2242, 0.0011, 0.0007, 0.0007, 0.0006, 0.0006, 0.0005, 0.0004, 0.0004,\n",
       "         0.0004]),\n",
       " indices=tensor([  38, 1392, 1246,  768,  661,   34, 1180,  490, 1350,  854])))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_attribution[:, 38].topk(10), average_attribution[6].topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[6, 2]}, size=[6]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Run the toy example\u001b[39;00m\n\u001b[1;32m     90\u001b[0m model \u001b[38;5;241m=\u001b[39m ToyModel()\n\u001b[0;32m---> 91\u001b[0m average_attribution \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_toy_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoy_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Attribution shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_attribution\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Attribution:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, average_attribution)\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mcompute_toy_jacobian\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     70\u001b[0m     interpolated_input \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m batch\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Compute attribution for interpolated input\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     interp_attribution \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficient_attribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_nonzero_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_nonzero_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolated_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     batch_attribution \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m interp_attribution\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Average the attribution\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m, in \u001b[0;36mcompute_efficient_attribution\u001b[0;34m(model, input_nonzero_indices, output_nonzero_indices, interpolated_input)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_nonzero_indices)):\n\u001b[1;32m     47\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(encoder_output[:, i]\u001b[38;5;241m.\u001b[39msum(), interpolated_input, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mattribution\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (grad \u001b[38;5;241m*\u001b[39m interpolated_input)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[input_nonzero_indices]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribution\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[6, 2]}, size=[6]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Toy model\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(3, 4)\n",
    "        self.encoder = nn.Linear(4, 2)\n",
    "        \n",
    "        # Set known weights for predictability\n",
    "        self.decoder.weight.data = torch.tensor([[1., 2., 3.],\n",
    "                                                 [4., 5., 6.],\n",
    "                                                 [7., 8., 9.],\n",
    "                                                 [10., 11., 12.]])\n",
    "        self.decoder.bias.data = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "        \n",
    "        self.encoder.weight.data = torch.tensor([[1., 2., 3., 4.],\n",
    "                                                 [5., 6., 7., 8.]])\n",
    "        self.encoder.bias.data = torch.tensor([0.5, 0.6])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = torch.tensor([[1., 0., 1.],\n",
    "                         [0., 1., 1.],\n",
    "                         [1., 1., 0.]])\n",
    "\n",
    "# Simplified version of our functions\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).nonzero().squeeze(1)\n",
    "\n",
    "def compute_efficient_attribution(model, input_nonzero_indices, output_nonzero_indices, interpolated_input):\n",
    "    interpolated_input.requires_grad_(True)\n",
    "    \n",
    "    decoder_output = model.decoder(interpolated_input)\n",
    "    encoder_output = model.encoder(F.relu(decoder_output))\n",
    "    \n",
    "    attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=interpolated_input.device)\n",
    "    for i in range(len(output_nonzero_indices)):\n",
    "        grad = torch.autograd.grad(encoder_output[:, i].sum(), interpolated_input, retain_graph=True)[0]\n",
    "        attribution[i] = (grad * interpolated_input).sum(dim=0)[input_nonzero_indices]\n",
    "    \n",
    "    return attribution\n",
    "def compute_toy_jacobian(model, data):\n",
    "    device = next(model.parameters()).device\n",
    "    attribution_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    batch = data.to(device)  # Process all data at once\n",
    "    \n",
    "    # Identify non-zero input elements\n",
    "    input_nonzero_indices = get_nonzero_indices(batch)\n",
    "\n",
    "    # Perform a forward pass to identify non-zero output elements\n",
    "    with torch.no_grad():\n",
    "        initial_output = model(batch)\n",
    "    output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "    # Integrated Gradients with Zero Ablation\n",
    "    batch_attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=device)\n",
    "    \n",
    "    for alpha in np.linspace(0, 1, 7):\n",
    "        interpolated_input = alpha * batch\n",
    "        \n",
    "        # Compute attribution for interpolated input\n",
    "        interp_attribution = compute_efficient_attribution(model, input_nonzero_indices, output_nonzero_indices, interpolated_input)\n",
    "        \n",
    "        batch_attribution += interp_attribution\n",
    "    \n",
    "    # Average the attribution\n",
    "    batch_attribution /= 7\n",
    "\n",
    "    # Update attribution sum\n",
    "    attribution_sum = torch.zeros(model.encoder.weight.shape[0], model.decoder.weight.shape[1], device=device)\n",
    "    attribution_sum[output_nonzero_indices, input_nonzero_indices] = batch_attribution\n",
    "\n",
    "    # Normalize the attribution sum by the total number of samples\n",
    "    average_attribution = attribution_sum / batch.shape[0]\n",
    "\n",
    "    return average_attribution\n",
    "\n",
    "# Run the toy example\n",
    "model = ToyModel()\n",
    "average_attribution = compute_toy_jacobian(model, toy_data)\n",
    "\n",
    "print(\"Average Attribution shape:\", average_attribution.shape)\n",
    "print(\"Average Attribution:\\n\", average_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input activations: torch.Size([816, 25000])\n",
      "shape of initial output activations: torch.Size([816, 25000])\n",
      "number of nonzero input indices: torch.Size([100])\n",
      "number of nonzero output indices: torch.Size([100])\n",
      "torch.Size([100]) torch.Size([100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.81 GiB. GPU 0 has a total capacity of 47.54 GiB of which 16.14 GiB is free. Process 3126992 has 31.39 GiB memory in use. Of the allocated memory 28.33 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m average_jacobian\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m average_jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficient_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_sae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Jacobian shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_jacobian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Jacobian (first few elements):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, average_jacobian[:\u001b[38;5;241m5\u001b[39m, :\u001b[38;5;241m5\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 72\u001b[0m, in \u001b[0;36mcompute_efficient_jacobian\u001b[0;34m(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\u001b[0m\n\u001b[1;32m     68\u001b[0m nonzero_input \u001b[38;5;241m=\u001b[39m feature_acts[:, input_nonzero_indices]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Compute Jacobian\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# return input_nonzero_indices, nonzero_input\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonzero_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Update Jacobian sum\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jacobian_sum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/autograd/functional.py:817\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    814\u001b[0m                 jac_i_el\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mzeros_like(inp_el))\n\u001b[1;32m    816\u001b[0m     jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 817\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    818\u001b[0m             torch\u001b[38;5;241m.\u001b[39mstack(jac_i_el, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    819\u001b[0m                 out\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m+\u001b[39m inputs[el_idx]\u001b[38;5;241m.\u001b[39msize()  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (el_idx, jac_i_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jac_i)\n\u001b[1;32m    822\u001b[0m         ),\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    825\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m _grad_postprocess(jacobian, create_graph)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(jacobian, (is_outputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/autograd/functional.py:818\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    814\u001b[0m                 jac_i_el\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mzeros_like(inp_el))\n\u001b[1;32m    816\u001b[0m     jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 818\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_i_el\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    819\u001b[0m                 out\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m+\u001b[39m inputs[el_idx]\u001b[38;5;241m.\u001b[39msize()  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (el_idx, jac_i_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jac_i)\n\u001b[1;32m    822\u001b[0m         ),\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    825\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m _grad_postprocess(jacobian, create_graph)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(jacobian, (is_outputs_tuple, is_inputs_tuple))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.81 GiB. GPU 0 has a total capacity of 47.54 GiB of which 16.14 GiB is free. Process 3126992 has 31.39 GiB memory in use. Of the allocated memory 28.33 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "def create_masked_sae_to_sae_function(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices):\n",
    "    # set input_nonzero_indices to the first N indices\n",
    "    masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices]\n",
    "    masked_decoder_bias = sae_res.decoder.bias\n",
    "\n",
    "    masked_encoder_weight = skip_sae.encoder.weight[output_nonzero_indices, :]\n",
    "    masked_encoder_bias = skip_sae.encoder.bias[output_nonzero_indices]\n",
    "\n",
    "    def masked_sae_to_sae_function(nonzero_input):\n",
    "        decoder_output = F.linear(nonzero_input, masked_decoder_weight, masked_decoder_bias)\n",
    "        # TODO: Add layernorm for GPT-2 if needed\n",
    "        encoder_output = F.linear(decoder_output, masked_encoder_weight, masked_encoder_bias)\n",
    "        # add ReLU\n",
    "        encoder_output = F.relu(encoder_output)\n",
    "        return encoder_output\n",
    "\n",
    "    return masked_sae_to_sae_function\n",
    "\n",
    "def compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length):\n",
    "    device = model.device\n",
    "    jacobian_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        feature_acts = model[activation_names[0]](batch)\n",
    "        feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\")\n",
    "\n",
    "        # return feature_acts\n",
    "\n",
    "        # Identify non-zero input elements\n",
    "        input_nonzero_indices = get_nonzero_indices(feature_acts)\n",
    "\n",
    "        # Perform a forward pass to identify non-zero output elements\n",
    "        with torch.no_grad():\n",
    "            # initial_output = skip_sae.encoder(sae_res.decoder(feature_acts))\n",
    "            # add bias and ReLU\n",
    "            relu = torch.nn.ReLU()\n",
    "            initial_output = relu(skip_sae.encoder(sae_res.decoder(feature_acts)) + skip_sae.encoder.bias)\n",
    "        output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "        N = 100\n",
    "        input_nonzero_indices = torch.arange(N)\n",
    "        output_nonzero_indices = torch.arange(N)\n",
    "        # return initial_output\n",
    "        print(f\"shape of input activations: {feature_acts.shape}\")\n",
    "        print(f\"shape of initial output activations: {initial_output.shape}\")\n",
    "        print(f\"number of nonzero input indices: {input_nonzero_indices.shape}\")\n",
    "        print(f\"number of nonzero output indices: {output_nonzero_indices.shape}\")\n",
    "\n",
    "        print(output_nonzero_indices.shape, input_nonzero_indices.shape)\n",
    "\n",
    "        # Create the masked function\n",
    "        masked_function = create_masked_sae_to_sae_function(\n",
    "            sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices\n",
    "        )\n",
    "\n",
    "        # Extract non-zero input values\n",
    "        nonzero_input = feature_acts[:, input_nonzero_indices]\n",
    "\n",
    "        # Compute Jacobian\n",
    "        # return input_nonzero_indices, nonzero_input\n",
    "        jacobian = torch.autograd.functional.jacobian(masked_function, nonzero_input)\n",
    "\n",
    "        # Update Jacobian sum\n",
    "        if jacobian_sum is None:\n",
    "            jacobian_sum = torch.zeros(skip_sae.encoder.weight.shape[0], sae_res.decoder.weight.shape[1], device=\"cpu\")\n",
    "        \n",
    "        jacobian_sum[output_nonzero_indices.unsqueeze(1).cpu(), input_nonzero_indices.cpu()] += jacobian.sum(dim=0).cpu()\n",
    "        sample_count += jacobian.shape[0]\n",
    "        # print out memory\n",
    "        print(f\" Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Normalize the Jacobian sum by the total number of samples\n",
    "    average_jacobian = jacobian_sum / sample_count.cpu()\n",
    "\n",
    "    return average_jacobian\n",
    "\n",
    "# Usage\n",
    "average_jacobian = compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\n",
    "\n",
    "print(\"Average Jacobian shape:\", average_jacobian.shape)\n",
    "print(\"Average Jacobian (first few elements):\\n\", average_jacobian[:5, :5])\n",
    "print(\"Token list shape:\", token_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1632, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_nonzero_indices = average_jacobian\n",
    "masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices]\n",
    "masked_decoder_bias = sae_res.decoder.bias\n",
    "F.linear(nz_inp, masked_decoder_weight, masked_decoder_bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 4\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msae_to_sae_function\u001b[39m(feature_act):\n\u001b[1;32m      7\u001b[0m     res_act \u001b[38;5;241m=\u001b[39m sae_res\u001b[38;5;241m.\u001b[39mdecoder(feature_act)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "\n",
    "def sae_to_sae_function(feature_act):\n",
    "    res_act = sae_res.decoder(feature_act)\n",
    "    #TODO: for gpt2, add layernorm\n",
    "    return skip_sae.encoder(res_act)\n",
    "\n",
    "for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "    batch = batch.to(model.device)\n",
    "    token_list[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "    feature_acts = model[activation_names[0]](batch)\n",
    "    feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\" )\n",
    "\n",
    "    # calculate Jacobian, sum over batch dimension\n",
    "    # jacobian = torch.autograd.functional.jacobian(sae_to_sae_function, feature_acts)\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Layer L0: 31, Second Layer L0: 0\n",
      "First Layer features_per_pos: tensor([ 14.,  76., 101.,  37.,  71.,  46., 110.,  62.,  31., 133., 137., 115.,\n",
      "         97., 193., 199., 129.,  66.,  87., 126.,  83.]) \n",
      " Second Layer features_per_pos: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[BEGIN]Once upon a time, there was a reliable otter named Ollie. He lived in'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L0_first = (dictionary_activations_res[:10000] !=0).float().sum(-1).mean().item()\n",
    "first_N_pos = (dictionary_activations_res[:20] !=0).float().sum(-1)\n",
    "L0_sec = (dictionary_activations_skip[:10000] !=0).float().sum(-1).mean()\n",
    "sec_N_pos = (dictionary_activations_skip[:20] !=0).float().sum(-1)\n",
    "print(f\"First Layer L0: {L0_first:.0f}, Second Layer L0: {L0_sec:.0f}\")\n",
    "print(f\"First Layer features_per_pos: {first_N_pos} \\n Second Layer features_per_pos: {sec_N_pos}\")\n",
    "tokenizer.decode(dataset[0][\"input_ids\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('nnsight').setLevel(logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.backends.cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:25<01:15, 25.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad shape torch.Size([32, 51, 25000])\n",
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:49<02:27, 49.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad shape torch.Size([32, 51, 25000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, saved_grad\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m clean_upstream\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dl)\n\u001b[1;32m     68\u001b[0m us_attrib \u001b[38;5;241m=\u001b[39m us_attrib\u001b[38;5;241m/\u001b[39mnum_batches\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def get_upstream_attrs(model, upstream_tags, target_tag, target_feat_idx, batch_size=6, train_size=3000):\n",
    "'''\n",
    "Get the attribution of features earlier in the network.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "upstream_attribs = {}\n",
    "upstream_tags = [activation_names[0]]\n",
    "target_tag = activation_names[1]\n",
    "# for upstream_tag, upstream_mlp in model.get_upstream(target_tag).items():\n",
    "for upstream_tag in upstream_tags:\n",
    "    nn_model.wipe_sparse()\n",
    "    nn_model.register_sparse([upstream_tag, target_tag])\n",
    "    upstream_mlp = nn_model.sparse_mlps[upstream_tag]\n",
    "    target = nn_model.sparse_mlps[target_tag].act\n",
    "    # upstream_mlp = sae_res\n",
    "    # target = skip_sae\n",
    "    \n",
    "    \n",
    "    num_us_feats = upstream_mlp.encoder.weight.shape[0]\n",
    "    upstream = upstream_mlp.act\n",
    "\n",
    "    us_attrib = torch.zeros((num_us_feats, num_us_feats))\n",
    "\n",
    "    # for batch_idx in tqdm(range(0, doc_ids.shape[0]-batch_size, batch_size), desc=f'Getting {upstream_tag} attribution'):\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(nn_model.device)\n",
    "        # clean_ids, noise_ids = batch_ids[...,0], batch_ids[...,1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with nn_model.trace(batch) as tracer:\n",
    "                clean_upstream = upstream.output.save()\n",
    "                clean_target = target.output.save()\n",
    "            clean_upstream = clean_upstream.detach()\n",
    "        \n",
    "        batch_attrib = torch.zeros_like(us_attrib).to(nn_model.device)\n",
    "        clean_upstream.requires_grad = True\n",
    "        noise_upstream = torch.zeros_like(clean_upstream)\n",
    "        for alpha in np.linspace(0, 1, 2):\n",
    "            print(alpha)\n",
    "            with nn_model.trace(batch) as tracer:\n",
    "                upstream.output = clean_upstream + alpha*(noise_upstream - clean_upstream).detach()\n",
    "                # upstream_grad = upstream.output.grad.save()\n",
    "                saved_og_loss = ((clean_target.detach() - target.output)**2).sum(dim=[0,1]).save()\n",
    "                # saved_og_loss.backward()\n",
    "                # Compute gradients with respect to the upstream output\n",
    "                saved_og_loss = ((clean_target.detach() - target.output[:,:,target_feat_idx])**2).sum(dim=[0,1]).save()\n",
    "\n",
    "                saved_grad = upstream_grad.save()\n",
    "\n",
    "                sample_attrib = (upstream_grad*(noise_upstream - clean_upstream).detach()).sum(dim=[0,1]).save()\n",
    "                # saved_og_loss = ((clean_target.detach() - target.output)**2).sum().save()\n",
    "                # sample_attrib = ((clean_upstream.detach()).sum(dim=[0,1])\n",
    "                # sample_attrib = upstream.output.save()\n",
    "            batch_attrib += sample_attrib\n",
    "            \n",
    "        us_attrib += batch_attrib.detach().cpu()\n",
    "        print(\"grad shape\", saved_grad.shape)\n",
    "        del clean_upstream\n",
    "        torch.cuda.empty_cache()\n",
    "    num_batches = len(dl)\n",
    "    us_attrib = us_attrib/num_batches\n",
    "    upstream_attribs[upstream_tag] = us_attrib.clone().cpu()\n",
    "\n",
    "\n",
    "nn_model.wipe_sparse()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# attrs = get_upstream_attrs(model, upstream_tags=['A0', 'M0', 'A1', 'M1'], target_tag='A2', target_feat_idx=3049, train_size=1_000, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero outputs: [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute gradients for all non-zero outputs simultaneously\u001b[39;00m\n\u001b[1;32m     42\u001b[0m selected_outputs \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m, non_zero_outputs[:, \u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 43\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_zero_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, jacobian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, jacobian)\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/autograd/functional.py:674\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    671\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 674\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[1;32m    676\u001b[0m     outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute gradients for all non-zero outputs simultaneously\u001b[39;00m\n\u001b[1;32m     42\u001b[0m selected_outputs \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m, non_zero_outputs[:, \u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 43\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, non_zero_outputs[:, \u001b[38;5;241m1\u001b[39m]], sparse_values)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, jacobian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, jacobian)\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mSimpleNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 10x10)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "input_size = 10\n",
    "hidden_size = 10\n",
    "output_size = 5\n",
    "net = SimpleNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Create sparse input tensor\n",
    "non_zero_inputs = [1, 4, 7]  # Indices of non-zero inputs\n",
    "sparse_values = torch.randn(len(non_zero_inputs), requires_grad=True)\n",
    "x = torch.zeros(1, input_size)\n",
    "x[0, non_zero_inputs] = sparse_values\n",
    "\n",
    "# Forward pass\n",
    "y_pred = net(x)\n",
    "\n",
    "# Identify non-zero outputs\n",
    "non_zero_outputs = torch.nonzero(y_pred.abs() > 1e-6).squeeze()\n",
    "if non_zero_outputs.dim() == 0:  # Handle case when there's only one non-zero output\n",
    "    non_zero_outputs = non_zero_outputs.unsqueeze(0)\n",
    "\n",
    "print(\"Non-zero outputs:\", non_zero_outputs.tolist())\n",
    "\n",
    "# Compute gradients for all non-zero outputs simultaneously\n",
    "selected_outputs = y_pred[0, non_zero_outputs[:, 1]]\n",
    "jacobian = torch.autograd.functional.jacobian(lambda x: net(x.unsqueeze(0))[0, non_zero_outputs[:, 1]], sparse_values)\n",
    "\n",
    "print(\"Jacobian shape:\", jacobian.shape)\n",
    "print(\"Jacobian:\\n\", jacobian)\n",
    "\n",
    "# Interpret the results\n",
    "print(\"\\nGradient interpretation:\")\n",
    "for i, (_, output_idx) in enumerate(non_zero_outputs):\n",
    "    print(f\"Output {output_idx + 1}:\")\n",
    "    for j, input_idx in enumerate(non_zero_inputs):\n",
    "        print(f\"  ∂y{output_idx + 1}/∂x{input_idx + 1} = {jacobian[i, j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    }
   ],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total res = first_only + cross_connection - intersection\n",
      "1197 = 938 + 411 - 152\n",
      "Total skip = first_only + cross_connection - intersection\n",
      "1193 = 762 + 580 - 149\n"
     ]
    }
   ],
   "source": [
    "# I need a specific set of information for each node.\n",
    "'''\n",
    "0. Remove features that activate < 10 times\n",
    "1. Each input's jaccard sim\n",
    "2. Each output's jaccard sim\n",
    "3. Eachl input's jaccard sim to each output\n",
    "3.2 Weight similarity of input-to-output\n",
    "4. Each nz_features top-5 common tokens\n",
    "'''\n",
    "\n",
    "def jaccard_similarity(co_occurrences, total_occurrences_1, total_occurrences_2):\n",
    "    # Union of occurrences\n",
    "    union_occurrences = total_occurrences_1 + total_occurrences_2.T - co_occurrences\n",
    "    # Jaccard similarity\n",
    "    jacc_sim = co_occurrences / union_occurrences\n",
    "    return jacc_sim\n",
    "\n",
    "\n",
    "\n",
    "# Remove features that activate < 10 times\n",
    "activation_threshold = 10\n",
    "frequency_threshold = 0.1\n",
    "# cross_connection = \"correlation\"\n",
    "cross_connection = \"cos_sim\"\n",
    "\n",
    "# Residual features\n",
    "resid_diag = residual_correlation.diag()\n",
    "nz_res_features = resid_diag >  activation_threshold\n",
    "# Also remove high frequency features\n",
    "res_freq = resid_diag / total_tokens\n",
    "nz_res_features = nz_res_features & (res_freq < frequency_threshold)\n",
    "nz_res_corr = residual_correlation[nz_res_features][:, nz_res_features]\n",
    "\n",
    "# Repeat for skip\n",
    "skip_diag = skip_correlation.diag()\n",
    "nz_skip_features = skip_diag > activation_threshold\n",
    "skip_freq = skip_diag / total_tokens\n",
    "nz_skip_features = nz_skip_features & (skip_freq < frequency_threshold)\n",
    "nz_skip_corr = skip_correlation[nz_skip_features][:, nz_skip_features]\n",
    "\n",
    "# Repeat for residual to skip\n",
    "# switch statement for cross_connection\n",
    "if cross_connection == \"correlation\":\n",
    "    nz_res_skip_corr = residual_and_skip_correlation[nz_res_features][:, nz_skip_features]\n",
    "elif cross_connection == \"cos_sim\":\n",
    "    nz_res_skip_corr = cos_sim[nz_res_features][:, nz_skip_features].cpu()\n",
    "\n",
    "# Calculate the jaccard similarity between each feature\n",
    "total_occur_res = resid_diag[nz_res_features][:, None]\n",
    "total_occur_skip = skip_diag[nz_skip_features][:, None]\n",
    "jacc_sim_res_dim0_original = jaccard_similarity(total_occurrences_1 = total_occur_res, total_occurrences_2 = total_occur_res, co_occurrences = nz_res_corr)\n",
    "jacc_sim_skip_dim0_original = jaccard_similarity(total_occurrences_1 = total_occur_skip, total_occurrences_2 = total_occur_skip, co_occurrences = nz_skip_corr)\n",
    "if(cross_connection == \"correlation\"):\n",
    "    jacc_sim_res_to_skip_original = jaccard_similarity(total_occurrences_1 = total_occur_res, total_occurrences_2 = total_occur_skip, co_occurrences = nz_res_skip_corr)\n",
    "elif(cross_connection == \"cos_sim\"):\n",
    "    jacc_sim_res_to_skip_original = nz_res_skip_corr\n",
    "\n",
    "# save the jaccard diag\n",
    "# save the diagonals\n",
    "jacc_res_diag = jacc_sim_res_dim0_original.diag()\n",
    "jacc_skip_diag = jacc_sim_skip_dim0_original.diag()\n",
    "\n",
    "# set the diagonal to zero\n",
    "jacc_sim_res_dim0_original.fill_diagonal_(0)\n",
    "jacc_sim_skip_dim0_original.fill_diagonal_(0)\n",
    "\n",
    "# Find index of features above a correlation threshold\n",
    "correlation_threshold = 0.55\n",
    "cos_sim_threshold = 0.45\n",
    "\n",
    "#TODO: set threshold to be a quantile (or a max number of nodes)\n",
    "# Set upper triangle to lower triangle\n",
    "res_index_above_corr = (jacc_sim_res_dim0_original > correlation_threshold).sum(0).nonzero()[:, 0]\n",
    "skip_index_above_corr = (jacc_sim_skip_dim0_original > correlation_threshold).sum(0).nonzero()[:, 0]\n",
    "extra_res_nodes = (jacc_sim_res_to_skip_original > cos_sim_threshold).sum(1).nonzero()[:, 0]\n",
    "extra_skip_nodes = (jacc_sim_res_to_skip_original > cos_sim_threshold).sum(0).nonzero()[:, 0]\n",
    "\n",
    "# index into the above features\n",
    "jacc_sim_res_dim0 = jacc_sim_res_dim0_original[res_index_above_corr][:, res_index_above_corr]\n",
    "jacc_sim_skip_dim0 = jacc_sim_skip_dim0_original[skip_index_above_corr][:, skip_index_above_corr]\n",
    "jacc_sim_res_to_skip = jacc_sim_res_to_skip_original[extra_res_nodes][:, extra_skip_nodes]\n",
    "\n",
    "nz_res_ind = nz_res_features.nonzero()[:, 0]\n",
    "combined_res_ind = torch.tensor(list(set(res_index_above_corr.tolist() + extra_res_nodes.tolist())))\n",
    "global_id_res_group = nz_res_ind[combined_res_ind]\n",
    "res_only_global_id = nz_res_ind[res_index_above_corr]\n",
    "cross_connection_res_global_id = nz_res_ind[extra_res_nodes.cpu()]\n",
    "\n",
    "# skip\n",
    "nz_skip_ind = nz_skip_features.nonzero()[:, 0]\n",
    "combined_skip_ind = torch.tensor(list(set(skip_index_above_corr.tolist() + extra_skip_nodes.tolist())))\n",
    "global_id_skip_group = nz_skip_ind[combined_skip_ind]\n",
    "skip_only_global_id = nz_skip_ind[skip_index_above_corr]\n",
    "cross_connection_skip_global_id = nz_skip_ind[extra_skip_nodes.cpu()]\n",
    "\n",
    "# Logging\n",
    "num_of_res_intersection = len(set(res_index_above_corr.tolist()).intersection(set(extra_res_nodes.tolist())))\n",
    "num_of_skip_intersection = len(set(skip_index_above_corr.tolist()).intersection(set(extra_skip_nodes.tolist())))\n",
    "print(f\"Total res = first_only + cross_connection - intersection\")\n",
    "print(f\"{len(global_id_res_group)} = {len(res_only_global_id)} + {len(cross_connection_res_global_id)} - {num_of_res_intersection}\")\n",
    "print(f\"Total skip = first_only + cross_connection - intersection\")\n",
    "print(f\"{len(global_id_skip_group)} = {len(skip_only_global_id)} + {len(cross_connection_skip_global_id)} - {num_of_skip_intersection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot res_freq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# do log bins\n",
    "# plt.hist(res_freq, bins=np.logspace(np.log10(1e-5),np.log10(1e-0), 50))\n",
    "plt.hist(skip_freq, bins=np.logspace(np.log10(1e-6),np.log10(1e-0), 50))\n",
    "plt.xlabel(\"Frequency of Activation\")\n",
    "plt.ylabel(\"Number of Features\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total zeroed out: 0\n",
      " Total zeroed out: 0\n",
      " Total zeroed out: 4\n",
      "Res Connections: 2184\n",
      "Skip Connections: 1720\n",
      "Cross Connections: 619\n"
     ]
    }
   ],
   "source": [
    "# TODO change display corre threshold\n",
    "# TODO: check if triu-> tril fixes things\n",
    "# alt, just allow duplicates (by erasing diagonal?)\n",
    "def get_connection_weights(jacc_sim, global_id_x, global_id_y, display_correlation_threshold=0.4, max_connections=10):\n",
    "    # Remove all connections that have more than max_conn (they clog up display)\n",
    "    above_threshold = jacc_sim > display_correlation_threshold\n",
    "    # (above_threshold.sum(0) > max_num).nonzero(), above_threshold.sum(1) > max_num\n",
    "    too_high_rows = (above_threshold.sum(1) > max_connections).nonzero()[:, 0]\n",
    "    too_high_cols = (above_threshold.sum(0) > max_connections).nonzero()[:, 0]\n",
    "    print(f\" Total zeroed out: {len(too_high_rows) + len(too_high_cols)}\")\n",
    "    # Set those to zero \n",
    "    jacc_sim[too_high_rows] = 0\n",
    "    jacc_sim[:, too_high_cols] = 0\n",
    "    row, col = (jacc_sim > display_correlation_threshold).nonzero().T\n",
    "    weight = jacc_sim[row, col]\n",
    "    row = global_id_x[row]\n",
    "    col = global_id_y[col]\n",
    "    # if(len(row) > max_length):\n",
    "    #     print(f\"Too many connections, truncating to {max_length}\")\n",
    "    #     row = row[:max_length]\n",
    "    #     col = col[:max_length]\n",
    "    #     weight = weight[:max_length]\n",
    "    return [[row[i].item(), col[i].item(), weight[i].item()] for i in range(len(row))]\n",
    "\n",
    "res_connections = get_connection_weights(jacc_sim_res_dim0, res_only_global_id, res_only_global_id)\n",
    "skip_connections = get_connection_weights(jacc_sim_skip_dim0, skip_only_global_id, skip_only_global_id)\n",
    "cross_connections = get_connection_weights(jacc_sim_res_to_skip, cross_connection_res_global_id, cross_connection_skip_global_id, display_correlation_threshold= 0.25)\n",
    "print(f\"Res Connections: {len(res_connections)}\")\n",
    "print(f\"Skip Connections: {len(skip_connections)}\")\n",
    "print(f\"Cross Connections: {len(cross_connections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1197/1197 [00:20<00:00, 57.00it/s]\n",
      "100%|██████████| 1193/1193 [00:19<00:00, 59.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# nz_dict_act_res is size (tokens, features)\n",
    "\n",
    "# For each feature, get the top-5 tokens by occurance\n",
    "def get_top_tokens(dictionary_activations, token_list, tokenizer):\n",
    "    feature_labels = []\n",
    "    feature_hovertips = []\n",
    "    for feature in tqdm(dictionary_activations.T):\n",
    "        nz_feature_ind = feature != 0\n",
    "        total_nz = nz_feature_ind.sum()\n",
    "        specific_tokens = token_list[nz_feature_ind].numpy()\n",
    "        counter = Counter(specific_tokens).most_common(5)\n",
    "        hovertip_str = \"\"\n",
    "        for token_loop_ind, (token, count) in enumerate(counter):\n",
    "            token_str = tokenizer.decode(token.item())\n",
    "            percentage = count / total_nz\n",
    "            token_str = token_str.replace(\"\\n\", \"\\\\n\").replace(\" \", \"_\")\n",
    "            # token_str = token_str.replace(\"Ġ\", \"_\")\n",
    "            hovertip_str += f\"{token_str}: {percentage:.0%}\"\n",
    "            if(token_loop_ind == 0):\n",
    "                feature_labels.append(token_str)\n",
    "        feature_hovertips.append(hovertip_str)\n",
    "    # replace \\n w/ \\\\newline\n",
    "    # feature_hovertips = [hovertip.replace(\"\\n\", \"\\\\n\") for hovertip in feature_hovertips]\n",
    "    # feature_labels = [label.replace(\"\\n\", \"\\\\n\") for label in feature_labels]\n",
    "    # replace 'Ġ' w/ _\n",
    "    # feature_hovertips = [hovertip.replace(\"Ġ\", \"_\") for hovertip in feature_hovertips]\n",
    "    # feature_labels = [label.replace(\"Ġ\", \"_\") for label in feature_labels]\n",
    "\n",
    "\n",
    "    return feature_labels, feature_hovertips\n",
    "\n",
    "# Get the top-5 tokens for each feature\n",
    "# token_list, dictionary_activations_res, dictionary_activations_skip\n",
    "nz_dict_act_res = dictionary_activations_res[:, global_id_res_group].numpy()\n",
    "nz_dict_act_skip = dictionary_activations_skip[:, global_id_skip_group].numpy()\n",
    "\n",
    "res_feature_labels, res_feature_hovertips = get_top_tokens(nz_dict_act_res, token_list, tokenizer)\n",
    "skip_feature_labels, skip_feature_hovertips = get_top_tokens(nz_dict_act_skip, token_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Generate HTML content with correct variable substitution\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        body, html {{\n",
    "            margin: 0;\n",
    "            height: 100%;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .tooltip {{\n",
    "            position: absolute;\n",
    "            text-align: center;\n",
    "            width: auto;\n",
    "            height: auto;\n",
    "            padding: 5px;\n",
    "            font: 12px sans-serif;\n",
    "            background: lightsteelblue;\n",
    "            border: 0px;\n",
    "            border-radius: 8px;\n",
    "            pointer-events: none;\n",
    "            opacity: 0;\n",
    "            transition: opacity 0.3s;\n",
    "        }}\n",
    "        .zoomable {{\n",
    "            cursor: grab;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"100%\" height=\"100%\" class=\"zoomable\"></svg>\n",
    "    <script>\n",
    "        const resFeatureLabels = {res_feature_labels};\n",
    "        const skipFeatureLabels = {skip_feature_labels};\n",
    "\n",
    "        const resFeatureHovertips = {res_feature_hovertips};\n",
    "        const skipFeatureHovertips = {skip_feature_hovertips};\n",
    "\n",
    "        const resGlobalID = {res_global_id};\n",
    "        const skipGlobalID = {skip_global_id};\n",
    "\n",
    "        const res_connections = {res_connections};\n",
    "        const skip_connections = {skip_connections};\n",
    "        const cross_connections = {cross_connections};\n",
    "\n",
    "        const nodes = [];\n",
    "\n",
    "        resGlobalID.forEach((global_id, i) => {{\n",
    "            const full_id_name = `res_${{global_id}}`;\n",
    "            nodes.push({{ id: full_id_name, group: 'res', color: 'blue', defaultLabel: resFeatureLabels[i], hoverLabel: resFeatureHovertips[i]+ full_id_name}});\n",
    "        }});\n",
    "\n",
    "        skipGlobalID.forEach((global_id, i) => {{\n",
    "            const full_id_name = `skip_${{global_id}}`;\n",
    "            nodes.push({{ id: full_id_name, group: 'skip', color: 'red', defaultLabel: skipFeatureLabels[i], hoverLabel: skipFeatureHovertips[i]+full_id_name }});\n",
    "        }});\n",
    "\n",
    "\n",
    "        const links = [];\n",
    "        res_connections.forEach(([res_ind_x, res_ind_y, weight]) => {{\n",
    "            const source = `res_${{res_ind_x}}`;\n",
    "            const target = `res_${{res_ind_y}}`;\n",
    "            links.push({{ source: source, target: target, value: weight, color: 'blue' }});\n",
    "        }});\n",
    "\n",
    "        skip_connections.forEach(([skip_ind_x, skip_ind_y, weight]) => {{\n",
    "            const source = `skip_${{skip_ind_x}}`;\n",
    "            const target = `skip_${{skip_ind_y}}`;\n",
    "            links.push({{ source: source, target: target, value: weight, color: 'red' }});\n",
    "        }});\n",
    "\n",
    "        cross_connections.forEach(([res_ind_x, skip_ind_y, weight]) => {{\n",
    "            const source = `res_${{res_ind_x}}`;\n",
    "            const target = `skip_${{skip_ind_y}}`;\n",
    "            links.push({{ source: source, target: target, value: weight, color: 'purple' }});\n",
    "        }});\n",
    "\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = window.innerWidth,\n",
    "            height = window.innerHeight;\n",
    "\n",
    "        const tooltip = d3.select(\"body\").append(\"div\")\n",
    "            .attr(\"class\", \"tooltip\");\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(1))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-30))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2))\n",
    "            .force(\"x\", d3.forceX(width / 2).strength(0.09))\n",
    "            .force(\"y\", d3.forceY(height / 2).strength(0.09));\n",
    "\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(Math.abs(d.value)))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 5)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", (event, d) => {{\n",
    "                tooltip.transition().duration(100).style(\"opacity\", 0.9);\n",
    "                tooltip.html(d.hoverLabel)\n",
    "                    .style(\"left\", (event.pageX + 5) + \"px\")\n",
    "                    .style(\"top\", (event.pageY - 28) + \"px\");\n",
    "            }})\n",
    "            .on(\"mouseout\", () => {{\n",
    "                tooltip.transition().duration(350).style(\"opacity\", 0);\n",
    "            }});\n",
    "\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        svg.call(d3.zoom().on(\"zoom\", (event) => {{\n",
    "            svg.attr(\"transform\", event.transform);\n",
    "        }}));\n",
    "\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    res_feature_labels=json.dumps(res_feature_labels),\n",
    "    skip_feature_labels=json.dumps(skip_feature_labels),\n",
    "    res_feature_hovertips=json.dumps(res_feature_hovertips),\n",
    "    skip_feature_hovertips=json.dumps(skip_feature_hovertips),\n",
    "    res_global_id=json.dumps(global_id_res_group.tolist()),\n",
    "    skip_global_id=json.dumps(global_id_skip_group.tolist()),\n",
    "    res_connections=json.dumps(res_connections),\n",
    "    skip_connections=json.dumps(skip_connections),\n",
    "    cross_connections=json.dumps(cross_connections),\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('tiny_graph.html', 'w') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3292, device='cuda:0'), torch.Size([25000, 25000]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim[61].mean(), cos_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([0.3292, 0.2422, 0.2302, 0.2213, 0.2169, 0.2168, 0.2141, 0.2138, 0.2094,\n",
       "         0.2081], device='cuda:0'),\n",
       " indices=tensor([   61, 11614,  6768, 10452, 10741, 10453,  7194,  6326, 12940,  7447],\n",
       "        device='cuda:0')),\n",
       " tensor(0.1370, device='cuda:0'))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim.mean(dim=-1).topk(10), cos_sim.mean(dim=-1)[4288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3292, device='cuda:0') tensor(0.3417, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGzCAYAAAAxPS2EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOuElEQVR4nO3deVwVVeM/8M8FvJf1gshywRAUF0RxyYVQQS0Ul0zLXVNQFCu0b5qmlilqiWFPatpTj+b2mKZZai5lgvuCuJIiSmoomVzcuaIJAuf3hz/m8coO9wIDn/frNa8Xc+bMzDlykQ9nzswohBACRERERDJiUtkNICIiIiotBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGKqyFAoFIiIiKrsZRfLw8EBISIhBj/l8v1evXg2FQoGrV68a9DxdunRBly5dDHrM8jpx4gQ6dOgAKysrKBQKxMfHV3aTqBgRERFQKBR6Zcb4uSB6HgMMlciVK1cwbtw4NGjQAObm5lCr1ejYsSMWL16Mf/75p7KbZ3Dnzp3DgAED4O7uDnNzc9StWxfdunXDkiVLKrtpRnPjxg1ERERUWmh48uQJBg4ciLt372LhwoVYu3Yt3N3d89Xz8PCAQqEodlm9erW0z7Zt2/Diiy/C3Nwc9erVw6xZs5CdnV2idoWEhOgd19raGg0aNMCAAQPw008/ITc311D/BBXm3r17MDMzww8//FAp53/+37SwJS8EHT9+HO+88w7atGmDWrVq5QtMxdm/f3+R59mwYYMReknGZlbZDaCqb+fOnRg4cCBUKhVGjhyJ5s2bIysrC4cPH8aUKVNw/vx5LFu2zODn/eeff2BmVvEf0aNHj6Jr166oV68exo4dC41Gg7/++gvHjh3D4sWLMWHCBKluUlISTEwM+3dARfV79+7deus3btzA7Nmz4eHhgVatWhn9/M+7cuUKrl27huXLl2PMmDGF1lu0aBEyMjKk9V9++QXff/89Fi5cCAcHB6m8Q4cOAIBff/0V/fr1Q5cuXbBkyRKcO3cOn3zyCW7evImvv/66RG1TqVT49ttvATz9/ly7dg3bt2/HgAED0KVLF/z8889Qq9Vl6Xal+O2336BQKNC9e/dKOf+4ceMQGBgorScnJ2PmzJkICwuDv7+/VO7p6Qng6ff422+/RYsWLdCgQQP88ccfZTrvu+++i3bt2uUr9/PzK9PxqJIJoiL8+eefwtraWnh5eYkbN27k237p0iWxaNGiSmiZ8fTq1Us4OjqKe/fu5duWlpZW4e1ZtWqVACCSk5MNcryHDx8WWH7ixAkBQKxatcog5ymtAwcOCABi06ZNpdpvwYIFRf77eHt7i5YtW4onT55IZR999JFQKBTiwoULxR4/ODhYWFlZFbgtMjJSABCDBg0qVZsr24gRI0Tnzp0NcqxZs2aJ53+VuLu7i+Dg4BIfo7jPnlarFY8ePRJCCBEeHp7vfMXZt29fmT5bVLXxEhIVKSoqChkZGVixYgVcXFzybW/YsCH+7//+T1rPzs7G3Llz4enpCZVKBQ8PD3z44YfIzMzU2+/kyZMICgqCg4MDLCwsUL9+fYwePVqvzvNzQfKutV++fBkhISGws7ODra0tRo0ahUePHuVr23fffYc2bdrAwsIC9vb2GDJkCP76669i+3zlyhU0a9YMdnZ2+bY5OTnprT9/rT9vvsrhw4fx7rvvwtHREXZ2dhg3bhyysrJw//59jBw5ErVr10bt2rXxwQcfQDz3QviSzP35+eef0bt3b7i6ukKlUsHT0xNz585FTk6OXr0uXbqgefPmOHXqFAICAmBpaYkPP/xQ2pY3B2b//v3SX6ajRo3Suwwza9Ys1KpVC7du3crXjrCwMNjZ2eHx48dFtnfv3r3w9/eHlZUV7Ozs0LdvX1y4cEHaHhISgs6dOwMABg4cCIVCYZD5OYmJiUhMTERYWJjeqNY777wDIQR+/PHHch1/2rRp6N69OzZt2pRvVODXX3+V+mxjY4PevXvj/Pnz+Y5x8eJFDBo0CI6OjrCwsECTJk3w0UcfFXpOIQQcHBwwadIkqSw3Nxd2dnYwNTXF/fv3pfLPPvsMZmZmeiNWubm52LVrF3r37l1k3w4dOoSBAweiXr16UKlUcHNzw8SJEyvlkrGzszMsLCwq5FwKhQLjx4/H1q1b0bx5c6hUKjRr1gy7du3KV/fvv/9GaGio9HNYv359vP3228jKyqqQttZ0vIRERdq+fTsaNGggDccXZ8yYMVizZg0GDBiA999/H3FxcYiMjMSFCxewZcsWAMDNmzfRvXt3ODo6Ytq0abCzs8PVq1exefPmEp1j0KBBqF+/PiIjI3H69Gl8++23cHJywmeffSbV+fTTT/Hxxx9j0KBBGDNmDG7duoUlS5YgICAAZ86cKTCc5HF3d0dsbCwSEhLQvHnzErXpeRMmTIBGo8Hs2bNx7NgxLFu2DHZ2djh69Cjq1auHefPm4ZdffsGCBQvQvHlzjBw5slTHX716NaytrTFp0iRYW1tj7969mDlzJnQ6HRYsWKBX986dO+jZsyeGDBmCN998E87OzvmO17RpU8yZMyffMH6HDh3QqVMnzJkzBxs3bsT48eOlfbKysvDjjz+if//+MDc3L7StMTEx6NmzJxo0aICIiAj8888/WLJkCTp27IjTp0/Dw8MD48aNQ926dTFv3jxpmL+gdpbWmTNnAABt27bVK3d1dcULL7wgbS+PESNGYPfu3YiOjkbjxo0BAGvXrkVwcDCCgoLw2Wef4dGjR/j666/RqVMnnDlzBh4eHgCAs2fPwt/fH7Vq1UJYWBg8PDxw5coVbN++HZ9++mmB51MoFOjYsSMOHjwolZ09exbp6ekwMTHBkSNHpHBy6NAhtG7dGtbW1lLdEydO4NatW+jVq1eR/dq0aRMePXqEt99+G3Xq1MHx48exZMkSXL9+HZs2bSrPP1mlefDgAW7fvp2vvE6dOnrzag4fPozNmzfjnXfegY2NDb788kv0798fKSkpqFOnDoCnl1zbt2+P+/fvIywsDF5eXvj777/x448/4tGjR1AqlRXWrxqrkkeAqApLT08XAETfvn1LVD8+Pl4AEGPGjNErnzx5sgAg9u7dK4QQYsuWLQKAOHHiRJHHAyBmzZolrecNVY8ePVqv3uuvvy7q1KkjrV+9elWYmpqKTz/9VK/euXPnhJmZWb7y5+3evVuYmpoKU1NT4efnJz744APx22+/iaysrHx1nx8qz7vcExQUJHJzc6VyPz8/oVAoxFtvvSWVZWdnixdeeCHfUP7z/S7oElLecPqzxo0bJywtLcXjx4+lss6dOwsA4ptvvslXv3PnznrnLmoY38/PT/j6+uqVbd68WQAQ+/bty1f/Wa1atRJOTk7izp07Utnvv/8uTExMxMiRI6Wysg7zF3UJKW9bSkpKvm3t2rUTL730UrHHL+oSkhBCnDlzRgAQEydOFEII8eDBA2FnZyfGjh2rV0+r1QpbW1u98oCAAGFjYyOuXbumV/fZz05BFixYIExNTYVOpxNCCPHll18Kd3d30b59ezF16lQhhBA5OTnCzs5Oaleejz/+WLi7uxfdaVHwZywyMlIoFAq99lbEJaRnlecSUmFLamqqVBeAUCqV4vLly1LZ77//LgCIJUuWSGUjR44UJiYmBf4/Vtz3jwyDl5CoUDqdDgBgY2NTovq//PILAOgNbQPA+++/D+DpZGAA0ujHjh078OTJk1K366233tJb9/f3x507d6T2bt68Gbm5uRg0aBBu374tLRqNBo0aNcK+ffuKPH63bt0QGxuL1157Db///juioqIQFBSEunXrYtu2bSVqY2hoqN5fdL6+vhBCIDQ0VCozNTVF27Zt8eeff5a065Jnh9Pz/qr09/fHo0ePcPHiRb26KpUKo0aNKvU5njVy5EjExcXhypUrUtm6devg5uYmXfopSGpqKuLj4xESEgJ7e3upvEWLFujWrZv0mTGWvMsdKpUq3zZzc3ODXA7JG9148OABACA6Ohr379/H0KFD9T5/pqam8PX1lT5/t27dwsGDBzF69GjUq1dP75jF3WXj7++PnJwcHD16FMDTkRZ/f3/4+/vj0KFDAICEhATcv39fb1Is8PTntLjLR4D+Z+zhw4e4ffs2OnToACGEQUauKsPMmTMRHR2db3n2swkAgYGB0gRi4OnnVa1WSz+rubm52Lp1K/r06ZNvdA8o/vtHhsEAQ4XKu6si7z/m4ly7dg0mJiZo2LChXrlGo4GdnR2uXbsGAOjcuTP69++P2bNnw8HBAX379sWqVavyzZMpzPP/2deuXRvA01tDAeDSpUsQQqBRo0ZwdHTUWy5cuICbN28We4527dph8+bNuHfvHo4fP47p06fjwYMHGDBgABITE0vdRltbWwCAm5tbvvK8dpfG+fPn8frrr8PW1hZqtRqOjo548803AQDp6el6devWrVvu4ezBgwdDpVJh3bp10jl27NiB4cOHF/mfdd73vEmTJvm2NW3aFLdv38bDhw/L1bai5P0SLuiz9fjxY2n7P//8A61Wq7eUVN78krygf+nSJQDAyy+/nO/zt3v3bunzl/fLsCyXKV988UVYWlpKYSUvwAQEBODkyZN4/PixtK1Tp07SflqtFqdPny5RgElJSZGCp7W1NRwdHaWw+vxnrKp4/nv4fED18fFBYGBgvuX5n4/nf36Bp//P5P2s3rp1CzqdrsyXmMkwOAeGCqVWq+Hq6oqEhIRS7VfcXx8KhQI//vgjjh07hu3bt+O3337D6NGj8a9//QvHjh3Tu15fEFNT0wLLxf+fDJubmwuFQoFff/21wLrFHf9ZSqUS7dq1Q7t27dC4cWOMGjUKmzZtwqxZs8rUxoLKxXOTeItz//59dO7cGWq1GnPmzIGnpyfMzc1x+vRpTJ06Nd9zSQwx+bF27dp49dVXsW7dOsycORM//vgjMjMzpdBUVeVNPE9NTc0XHlNTU9G+fXsAwMaNG/ONUpX0+5L385EX3PP+/deuXQuNRpOvviFuka9VqxZ8fX1x8OBBXL58GVqtFv7+/nB2dsaTJ08QFxeHQ4cOwcvLC46OjtJ+v/76K8zNzdG1a9cij5+Tk4Nu3brh7t27mDp1Kry8vGBlZYW///4bISEhVfbZN8/faLBq1aoyPVCvuP9jqGpggKEivfrqq1i2bBliY2OLfVaCu7s7cnNzcenSJTRt2lQqT0tLw/379/M9lOyll17CSy+9hE8//RTr16/H8OHDsWHDhiKfAVISnp6eEEKgfv360qRKQ8gbKk5NTTXYMcti//79uHPnDjZv3oyAgACpPDk5uVzHLS54jhw5En379sWJEyewbt06tG7dGs2aNStyn7zveVJSUr5tFy9ehIODA6ysrMre6GLkPc/m5MmTUlgBnk7AvH79OsLCwgAAQUFBiI6OLtM51q5dC4VCgW7dugH437NLnJyc9J518rwGDRoAQKn/QMjj7++Pzz77DDExMXBwcICXlxcUCgWaNWuGQ4cO4dChQ3j11Vf19tm5cye6du1abKg9d+4c/vjjD6xZs0ZvgnlZ/40qyvPtK+7zWVaOjo5Qq9Vl/t6RYfASEhXpgw8+gJWVFcaMGYO0tLR8269cuYLFixcDgHRXw6JFi/TqfPHFFwAgDVvfu3cv318yeb9oSnoZqShvvPEGTE1NMXv27HznEULgzp07Re6/b9++Av/SypuvUdDlkIqU99fhs23MysrCv//973IdNy9IPHsb7rN69uwJBwcHfPbZZzhw4ECJRl9cXFzQqlUrrFmzRu+4CQkJ2L17d7F3wpRXs2bN4OXlhWXLlundYv71119DoVBgwIABUjufv6xQEvPnz8fu3bsxePBgNGrUCMDTMKRWqzFv3rwC53jl3Y7u6OiIgIAArFy5EikpKXp1SvKXvr+/PzIzM7Fo0SJ06tRJCqD+/v5Yu3Ytbty4oTf/5cmTJ4iOji7R5aOCPmNCCOlnvap6/ntY0KMfDMHExAT9+vXD9u3bcfLkyXzbOVJTMTgCQ0Xy9PTE+vXrMXjwYDRt2lTvSbxHjx7Fpk2bpCHali1bIjg4GMuWLZMucxw/fhxr1qxBv379pGHrNWvW4N///jdef/11eHp64sGDB1i+fDnUarVBfqF5enrik08+wfTp03H16lX069cPNjY2SE5OxpYtWxAWFobJkycXuv+ECRPw6NEjvP766/Dy8pL6unHjRnh4eJR7Qmx5dejQAbVr10ZwcDDeffddKBQKrF27ttz/aXp6esLOzg7ffPMNbGxsYGVlBV9fX9SvXx/A08sWQ4YMwdKlS2FqaoqhQ4eW6LgLFixAz5494efnh9DQUOk2altb2wp519WCBQvw2muvoXv37hgyZAgSEhKwdOlSjBkzRm+ksCjZ2dn47rvvADydO3Pt2jVs27YNZ8+eRdeuXfWeRK1Wq/H1119jxIgRePHFFzFkyBA4OjoiJSUFO3fuRMeOHbF06VIAwJdffolOnTrhxRdfRFhYGOrXr4+rV69i586dxb7Swc/PD2ZmZkhKSpJGkgAgICBAesLwswHm8OHD0Ol0JQowXl5e8PT0xOTJk/H3339DrVbjp59+KtN8LUO4du0a1q5dCwBSYPjkk08APB3lGzFiRImOc+jQoQKfWdSiRQu0aNGiVG2aN28edu/ejc6dOyMsLAxNmzZFamoqNm3ahMOHDxf5qAYykIq+7Ynk6Y8//hBjx44VHh4eQqlUChsbG9GxY0exZMkSvdt2nzx5ImbPni3q168vatWqJdzc3MT06dP16pw+fVoMHTpU1KtXT6hUKuHk5CReffVVcfLkSb1zopDbqG/duqVXr7An1f7000+iU6dOwsrKSlhZWQkvLy8RHh4ukpKSiuzrr7/+KkaPHi28vLyEtbW1UCqVomHDhmLChAn5nsRb2G3Uz99aWVjbC7pF9/l+F9S/I0eOiJdeeklYWFgIV1dX6VZvPHdbc+fOnUWzZs0K7Ofzt1ELIcTPP/8svL29hZmZWYG3tR4/flwAEN27dy/wmIWJiYkRHTt2FBYWFkKtVos+ffqIxMREvTrGuI06z5YtW0SrVq2ESqUSL7zwgpgxY0aBt8UXJDg4WO+WW0tLS+Hh4SH69+8vfvzxR5GTk1Pgfvv27RNBQUHC1tZWmJubC09PTxESEpLvc56QkCBef/11YWdnJ8zNzUWTJk3Exx9/XKK2tWvXTgAQcXFxUtn169cFAOHm5qZXd/LkycLb27tExxVCiMTERBEYGCisra2Fg4ODGDt2rHQ78bOfi4q4jbqo26BL8kTh4m6jfvbnDYAIDw/Pd4yC+nTt2jUxcuRI4ejoKFQqlWjQoIEIDw8XmZmZJe47lZ1CCI51EVHJ/P7772jVqhX++9//lvivXqoavL298eqrryIqKqqym0JkELyEREQltnz5clhbW+ONN96o7KZQKWRlZWHw4MEYNGhQZTeFyGA4AkNExdq+fTsSExPx8ccfY/z48dLEbCKiysIAQ0TF8vDwQFpaGoKCgrB27doSP52ZiMhYGGCIiIhIdvgcGCIiIpIdBhgiIiKSnWp7F1Jubi5u3LgBGxsbvhmUiIhIJoQQePDgAVxdXWFiUvg4S7UNMDdu3Mj38jYiIiKSh7/++gsvvPBCodurbYDJu0vir7/+glqtruTWEBERUUnodDq4ubkVe7djtQ0weZeN1Go1AwwREZHMFDf9g5N4iYiISHYYYIiIiEh2GGCIiIhIdqrtHBgiIiodIQSys7ORk5NT2U2haszU1BRmZmblfsQJAwwRESErKwupqal49OhRZTeFagBLS0u4uLhAqVSW+RgMMERENVxubi6Sk5NhamoKV1dXKJVKPgCUjEIIgaysLNy6dQvJyclo1KhRkQ+rKwoDDBFRDZeVlYXc3Fy4ubnB0tKysptD1ZyFhQVq1aqFa9euISsrC+bm5mU6DifxEhERAJT5L2Gi0jLEZ42fViIiIpIdBhgiIiKSHc6BISKiQoWuPlGh51sR0q5Cz0fyxREYIiKSrZCQECgUCmmpU6cOevTogbNnz0p1cnJysHDhQvj4+MDc3By1a9dGz549ceTIEb1j5eTkYP78+fDy8oKFhQXs7e3h6+uLb7/9tth2PNuGgpaIiAgAQEpKCnr37g1LS0s4OTlhypQpyM7OLnF/MzMz8dFHH8Hd3R0qlQoeHh5YuXKltP38+fPo378/PDw8oFAosGjRohIfW244AkNERLLWo0cPrFq1CgCg1WoxY8YMvPrqq0hJSYEQAkOGDEFMTAwWLFiAV155BTqdDl999RW6dOmCTZs2oV+/fgCA2bNn4z//+Q+WLl2Ktm3bQqfT4eTJk7h3716xbUhNTZW+3rhxI2bOnImkpCSpzNraGjk5Oejduzc0Gg2OHj2K1NRUjBw5ErVq1cK8efNK1NdBgwYhLS0NK1asQMOGDZGamorc3Fxp+6NHj9CgQQMMHDgQEydOLNEx5YoBhoiIZE2lUkGj0QAANBoNpk2bBn9/f9y6dQt79+7Fjz/+iG3btqFPnz7SPsuWLcOdO3cwZswYdOvWDVZWVti2bRveeecdDBw4UKrXsmXLErUh7/wAYGtrC4VCoVcGAL/++isSExMRExMDZ2dntGrVCnPnzsXUqVMRERFR7EPddu3ahQMHDuDPP/+Evb09AMDDw0OvTrt27dCu3dPLcNOmTStR2+WKl5Co5lo/WH8hItnLyMjAd999h4YNG6JOnTpYv349GjdurBde8rz//vu4c+cOoqOjATwNIXv37sWtW7eM0rbY2Fj4+PjA2dlZKgsKCoJOp8P58+eL3X/btm1o27YtoqKiULduXTRu3BiTJ0/GP//8Y5T2VnUcgSEiIlnbsWMHrK2tAQAPHz6Ei4sLduzYARMTE/zxxx9o2rRpgfvllf/xxx8AgC+++AIDBgyARqNBs2bN0KFDB/Tt2xc9e/Y0SDu1Wq1eeAEgrWu12mL3//PPP3H48GGYm5tjy5YtuH37Nt555x3cuXNHuoRWk3AEhigPR2OIZKlr166Ij49HfHw8jh8/jqCgIPTs2RPXrl0D8PTx9SXh7e2NhIQEHDt2DKNHj8bNmzfRp08fjBkzxpjNL7Hc3FwoFAqsW7cO7du3R69evfDFF19gzZo1NXIUhgGGiIhkzcrKCg0bNkTDhg3Rrl07fPvtt3j48CGWL1+Oxo0b48KFCwXul1feuHFjqczExATt2rXDe++9h82bN2P16tVYsWIFkpOTy91OjUaDtLQ0vbK89efnyxTExcUFdevWha2trVTWtGlTCCFw/fr1crdPbhhgiIioWlEoFDAxMcE///yDIUOG4NKlS9i+fXu+ev/6179Qp04ddOvWrdBjeXt7A3h6aaq8/Pz8cO7cOdy8eVMqi46Ohlqtls5TlI4dO+LGjRvIyMiQyv744w+YmJjghRdeKHf75IZzYIiISNYyMzOlOST37t3D0qVLkZGRgT59+qBz587YtGkTgoOD891GvW3bNmzatAlWVlYAgAEDBqBjx47o0KEDNBoNkpOTMX36dDRu3BheXl7lbmf37t3h7e2NESNGICoqSrrlOzw8HCqVqtj9hw0bhrlz52LUqFGYPXs2bt++jSlTpmD06NGwsLAA8PTFnImJidLXf//9N+Lj42FtbY2GDRuWuw9VCQMMEREVSg5Pxt21axdcXFwAADY2NvDy8sKmTZvQpUsXAMAPP/yARYsWYeHChXjnnXdgbm4OPz8/7N+/Hx07dpSOExQUhO+//x6RkZFIT0+HRqPByy+/jIiICJiZlf/XpampKXbs2IG3334bfn5+sLKyQnBwMObMmVOi/a2trREdHY0JEyagbdu2qFOnDgYNGoRPPvlEqnPjxg20bt1aWv/888/x+eefo3Pnzti/f3+5+1CVKERJZzfJjE6ng62tLdLT06FWqyu7OVQVFTVZd9jGimsHUSV7/PgxkpOTUb9+fZibm1d2c6gGKOozV9Lf35wDQ0RERLLDAENERFSMlJQUWFtbF7qkpKSU+xzNmjUr9Pjr1q0zQC+qF86BISIiKoarqyvi4+OL3F5ev/zyC548eVLgtucfgEcMMERERMUyMzMz+l087u7uRj1+dcNLSERERCQ7DDBEREQkOwwwREREJDsMMERERCQ7DDBEREQkOwwwREREZaBQKLB169bKbkaNxduoiYiocEW9csMYSvkaj5CQEKxZswbA01udX3jhBQwcOBBz5syp1q9FWL16NUaNGlVkneTkZHh4eGDTpk34+OOPcfXqVTRq1AifffYZevXqVeJzXbhwAVOnTsWBAweQnZ0Nb29v/PTTT6hXrx7u3r2LWbNmYffu3UhJSYGjoyP69euHuXPnwtbWtrzdLBJHYIiISNZ69OiB1NRU/Pnnn1i4cCH+85//YNasWZXdLKMaPHgwUlNTpcXPzw9jx47VK3Nzc8PRo0cxdOhQhIaG4syZM+jXrx/69euHhISEEp3nypUr6NSpE7y8vLB//36cPXsWH3/8sRQOb9y4gRs3buDzzz9HQkICVq9ejV27diE0NNSY3QfAAENERDKnUqmg0Wjg5uaGfv36ITAwENHR0QCAO3fuYOjQoahbty4sLS3h4+OD77//Xm//Ll264N1338UHH3wAe3t7aDQaRERE6NW5dOkSAgICYG5uDm9vb+n4zzp37hxefvllWFhYoE6dOggLC0NGRoa0PSQkBP369cO8efPg7OwMOzs7zJkzB9nZ2ZgyZQrs7e3xwgsvYNWqVcX22cLCAhqNRlqUSiUsLS31ykxNTbF48WL06NEDU6ZMQdOmTTF37ly8+OKLWLp0aYn+bT/66CP06tULUVFRaN26NTw9PfHaa6/ByckJANC8eXP89NNP6NOnDzw9PfHyyy/j008/xfbt25GdnV2ic5QVAwwREVUbCQkJOHr0KJRKJYCnbz1u06YNdu7ciYSEBISFhWHEiBE4fvy43n5r1qyBlZUV4uLiEBUVhTlz5kghJTc3F2+88QaUSiXi4uLwzTffYOrUqXr7P3z4EEFBQahduzZOnDiBTZs2ISYmBuPHj9ert3fvXty4cQMHDx7EF198gVmzZuHVV19F7dq1ERcXh7feegvjxo3D9evXDfLvERsbi8DAQL2yoKAgxMbGFrtvbm4udu7cicaNGyMoKAhOTk7w9fUtdt5P3lukzcyMO0uFAYaIiGRtx44dsLa2hrm5OXx8fHDz5k1MmTIFAFC3bl1MnjwZrVq1QoMGDTBhwgT06NEDP/zwg94xWrRogVmzZqFRo0YYOXIk2rZtiz179gAAYmJicPHiRfz3v/9Fy5YtERAQgHnz5untv379ejx+/Bj//e9/0bx5c7z88stYunQp1q5di7S0NKmevb09vvzySzRp0gSjR49GkyZN8OjRI3z44Ydo1KgRpk+fDqVSicOHDxvk30ar1eZ7j5KzszO0Wm2x+968eRMZGRmYP38+evTogd27d+P111/HG2+8gQMHDhS4z+3btzF37lyEhYUZpP1FKXWAOXjwIPr06QNXV9cCZ2ArFIoClwULFkh1PDw88m2fP3++3nHOnj0Lf39/mJubw83NDVFRUWXrIRERVWtdu3ZFfHw84uLiEBwcjFGjRqF///4AgJycHMydOxc+Pj6wt7eHtbU1fvvtt3xvj27RooXeuouLC27evAng6SRWNzc3vRc2+vn56dW/cOECWrZsCSsrK6msY8eOyM3NRVJSklTWrFkzmJj871evs7MzfHx8pHVTU1PUqVNHOndlys3NBQD07dsXEydORKtWrTBt2jS8+uqr+Oabb/LV1+l06N27N7y9vfNdgjOGUo/vPHz4EC1btsTo0aPxxhtv5Nuempqqt/7rr78iNDRU+jDlmTNnDsaOHSut29jYSF/rdDp0794dgYGB+Oabb3Du3DmMHj0adnZ2FZLqqBqr6DsqiMjorKyspBctrly5Ei1btsSKFSsQGhqKBQsWYPHixVi0aBF8fHxgZWWF9957D1lZWXrHqFWrlt66QqGQfoEbUkHnMea5NRqN3ggQAKSlpUGj0RS7r4ODA8zMzODt7a1X3rRp03wjRA8ePECPHj1gY2ODLVu25OuTMZQ6wPTs2RM9e/YsdPvz/yg///wzunbtigYNGuiV29jYFPoPuG7dOmRlZWHlypVQKpVo1qwZ4uPj8cUXXxQaYDIzM5GZmSmt63S6knaJiIiqCRMTE3z44YeYNGkShg0bhiNHjqBv37548803ATwdVfjjjz/y/VIuStOmTfHXX38hNTUVLi4uAIBjx47lq7N69Wo8fPhQGoU5cuQITExM0KRJEwP1rvT8/PywZ88evPfee1JZdHR0vhGkgiiVSrRr105vBAkA/vjjD703Z+t0OgQFBUGlUmHbtm0Vdvu6UefApKWlYefOnQXeTjV//nzUqVMHrVu3xoIFC/RmK8fGxiIgIECahAU8nXSUlJSEe/fuFXiuyMhI2NraSoubm5vhO0RERFXewIEDYWpqiq+++gqNGjVCdHQ0jh49igsXLmDcuHH5RiSKExgYiMaNGyM4OBi///47Dh06hI8++kivzvDhw2Fubo7g4GAkJCRg3759mDBhAkaMGJFvDkpF+r//+z/s2rUL//rXv3Dx4kVERETg5MmT+SYXF2bKlCnYuHEjli9fjsuXL2Pp0qXYvn073nnnHQD/u2Ly8OFDrFixAjqdDlqtFlqtFjk5OcbsmnEfZLdmzRrY2Njku9T07rvv4sUXX4S9vT2OHj2K6dOnIzU1FV988QWAp5OO6tevr7dP3gdAq9Widu3a+c41ffp0TJo0SVrX6XQMMURE5VXKB8tVBWZmZhg/fjyioqJw5swZ/PnnnwgKCoKlpSXCwsLQr18/pKenl/h4JiYm2LJlC0JDQ9G+fXt4eHjgyy+/RI8ePaQ6lpaW+O233/B///d/aNeuHSwtLdG/f3/p91pl6dChA9avX48ZM2ZIE4W3bt2K5s2bl2j/119/Hd988w0iIyPx7rvvokmTJvjpp5/QqVMnAMDp06cRFxcHANJlvDx5D9IzFoUQQpR5Z4UCW7ZsQb9+/Qrc7uXlhW7dumHJkiVFHmflypUYN24cMjIyoFKp0L17d9SvXx//+c9/pDqJiYlo1qwZEhMT0bRp02LbptPpYGtrK93ORQSg5HNgZPifNlFZPX78GMnJyahfv361fnotVR1FfeZK+vvbaJeQDh06hKSkJIwZM6bYur6+vsjOzsbVq1cBFD7pKG8bERER1WxGCzArVqxAmzZt0LJly2LrxsfHw8TERHqyn5+fHw4ePIgnT55IdaKjo9GkSZMCLx8RERFVN/PmzYO1tXWBS1E305TUoUOHCj2+tbW1AXpgXKWeA5ORkYHLly9L68nJyYiPj4e9vT3q1asH4Onwz6ZNm/Cvf/0r3/6xsbGIi4tD165dYWNjg9jYWEycOBFvvvmmFE6GDRuG2bNnIzQ0FFOnTkVCQgIWL16MhQsXlrWfRKXz/KUmXlIiogr21ltvYdCgQQVus7CwKPfx27Zti/j4+HIfp7KUOsCcPHkSXbt2ldbzJs4GBwdj9erVAIANGzZACIGhQ4fm21+lUmHDhg2IiIhAZmYm6tevj4kTJ+pNwLW1tcXu3bsRHh6ONm3awMHBATNnzuQzYIiIqMawt7eHvb290Y5vYWGRb+KtnJRrEm9Vxkm8VKCyPsiOIzBUjeVNqPTw8DDIX/ZExfnnn39w9erVqjmJl4iI5CHvqamPHj2q5JZQTZH3WSvPE3uN+6pIIiKq8kxNTWFnZye9f8fS0hIKhaKSW0XVkRACjx49ws2bN2FnZwdTU9MyH4sBhqgkOKmXqrm8R1RUhZcIUvVnZ2dX7seiMMAQEREUCgVcXFzg5OSk9wgLIkOrVatWuUZe8jDAUPXGt08TlYqpqalBfrkQGRsn8RIREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkeyUOsAcPHgQffr0gaurKxQKBbZu3aq3PSQkBAqFQm/p0aOHXp27d+9i+PDhUKvVsLOzQ2hoKDIyMvTqnD17Fv7+/jA3N4ebmxuioqJK3zsiIiKqlkodYB4+fIiWLVviq6++KrROjx49kJqaKi3ff/+93vbhw4fj/PnziI6Oxo4dO3Dw4EGEhYVJ23U6Hbp37w53d3ecOnUKCxYsQEREBJYtW1ba5hIREVE1ZFbaHXr27ImePXsWWUelUkGj0RS47cKFC9i1axdOnDiBtm3bAgCWLFmCXr164fPPP4erqyvWrVuHrKwsrFy5EkqlEs2aNUN8fDy++OILvaBDRERENZNR5sDs378fTk5OaNKkCd5++23cuXNH2hYbGws7OzspvABAYGAgTExMEBcXJ9UJCAiAUqmU6gQFBSEpKQn37t0r8JyZmZnQ6XR6C5HRrB/8v4WIiCqcwQNMjx498N///hd79uzBZ599hgMHDqBnz57IyckBAGi1Wjg5OentY2ZmBnt7e2i1WqmOs7OzXp289bw6z4uMjIStra20uLm5GbprREREVEWU+hJScYYMGSJ97ePjgxYtWsDT0xP79+/HK6+8YujTSaZPn45JkyZJ6zqdjiGGiIiomjL6bdQNGjSAg4MDLl++DADQaDS4efOmXp3s7GzcvXtXmjej0WiQlpamVydvvbC5NSqVCmq1Wm8hIiKi6snoAeb69eu4c+cOXFxcAAB+fn64f/8+Tp06JdXZu3cvcnNz4evrK9U5ePAgnjx5ItWJjo5GkyZNULt2bWM3mYiIiKq4UgeYjIwMxMfHIz4+HgCQnJyM+Ph4pKSkICMjA1OmTMGxY8dw9epV7NmzB3379kXDhg0RFBQEAGjatCl69OiBsWPH4vjx4zhy5AjGjx+PIUOGwNXVFQAwbNgwKJVKhIaG4vz589i4cSMWL16sd4mIiIiIaq5SB5iTJ0+idevWaN26NQBg0qRJaN26NWbOnAlTU1OcPXsWr732Gho3bozQ0FC0adMGhw4dgkqlko6xbt06eHl54ZVXXkGvXr3QqVMnvWe82NraYvfu3UhOTkabNm3w/vvvY+bMmbyFmoiIiAAACiGEqOxGGINOp4OtrS3S09M5H6Ymq4jbnIdtNP45iIhqiJL+/ua7kIiIiEh2GGCIiIhIdhhgiIiISHYYYIiIiEh2GGCIiIhIdhhgiIiISHYYYIiIiEh2DP4yR6JKVxHPfiEiokrFERgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSnVIHmIMHD6JPnz5wdXWFQqHA1q1bpW1PnjzB1KlT4ePjAysrK7i6umLkyJG4ceOG3jE8PDygUCj0lvnz5+vVOXv2LPz9/WFubg43NzdERUWVrYdERERU7ZQ6wDx8+BAtW7bEV199lW/bo0ePcPr0aXz88cc4ffo0Nm/ejKSkJLz22mv56s6ZMwepqanSMmHCBGmbTqdD9+7d4e7ujlOnTmHBggWIiIjAsmXLSttcIiIiqobMSrtDz5490bNnzwK32draIjo6Wq9s6dKlaN++PVJSUlCvXj2p3MbGBhqNpsDjrFu3DllZWVi5ciWUSiWaNWuG+Ph4fPHFFwgLCytwn8zMTGRmZkrrOp2utF0jIiIimTD6HJj09HQoFArY2dnplc+fPx916tRB69atsWDBAmRnZ0vbYmNjERAQAKVSKZUFBQUhKSkJ9+7dK/A8kZGRsLW1lRY3Nzej9IeIiIgqn1EDzOPHjzF16lQMHToUarVaKn/33XexYcMG7Nu3D+PGjcO8efPwwQcfSNu1Wi2cnZ31jpW3rtVqCzzX9OnTkZ6eLi1//fWXEXpEREREVUGpLyGV1JMnTzBo0CAIIfD111/rbZs0aZL0dYsWLaBUKjFu3DhERkZCpVKV6XwqlarM+xIREZG8GCXA5IWXa9euYe/evXqjLwXx9fVFdnY2rl69iiZNmkCj0SAtLU2vTt56YfNmiCrN+sH668M2Vk47iIhqEINfQsoLL5cuXUJMTAzq1KlT7D7x8fEwMTGBk5MTAMDPzw8HDx7EkydPpDrR0dFo0qQJateubegmExERkcyUegQmIyMDly9fltaTk5MRHx8Pe3t7uLi4YMCAATh9+jR27NiBnJwcac6Kvb09lEolYmNjERcXh65du8LGxgaxsbGYOHEi3nzzTSmcDBs2DLNnz0ZoaCimTp2KhIQELF68GAsXLjRQt4mIiEjOFEIIUZod9u/fj65du+YrDw4ORkREBOrXr1/gfvv27UOXLl1w+vRpvPPOO7h48SIyMzNRv359jBgxApMmTdKbw3L27FmEh4fjxIkTcHBwwIQJEzB16tQSt1On08HW1hbp6enFXsKiaub5SzoVjZeQiIjKrKS/v0sdYOSCAaYGY4AhIpKtkv7+5ruQiIiISHYYYIiIiEh2GGCIiIhIdhhgiIiISHYYYIiIiEh2GGCIiIhIdhhgiIiISHYYYIiIiEh2GGCIiIhIdozyNmqiClXZT94lIqIKxxEYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikp1SB5iDBw+iT58+cHV1hUKhwNatW/W2CyEwc+ZMuLi4wMLCAoGBgbh06ZJenbt372L48OFQq9Wws7NDaGgoMjIy9OqcPXsW/v7+MDc3h5ubG6KiokrfOyIiIqqWSh1gHj58iJYtW+Krr74qcHtUVBS+/PJLfPPNN4iLi4OVlRWCgoLw+PFjqc7w4cNx/vx5REdHY8eOHTh48CDCwsKk7TqdDt27d4e7uztOnTqFBQsWICIiAsuWLStDF4mIiKi6UQghRJl3ViiwZcsW9OvXD8DT0RdXV1e8//77mDx5MgAgPT0dzs7OWL16NYYMGYILFy7A29sbJ06cQNu2bQEAu3btQq9evXD9+nW4urri66+/xkcffQStVgulUgkAmDZtGrZu3YqLFy+WqG06nQ62trZIT0+HWq0uaxdJDtYPruwW6Bu2sbJbQEQkWyX9/W3QOTDJycnQarUIDAyUymxtbeHr64vY2FgAQGxsLOzs7KTwAgCBgYEwMTFBXFycVCcgIEAKLwAQFBSEpKQk3Lt3r8BzZ2ZmQqfT6S1ERERUPRk0wGi1WgCAs7OzXrmzs7O0TavVwsnJSW+7mZkZ7O3t9eoUdIxnz/G8yMhI2NraSoubm1v5O0RUFusH/28hIiKjqDZ3IU2fPh3p6enS8tdff1V2k4iIiMhIDBpgNBoNACAtLU2vPC0tTdqm0Whw8+ZNve3Z2dm4e/euXp2CjvHsOZ6nUqmgVqv1FiIiIqqeDBpg6tevD41Ggz179khlOp0OcXFx8PPzAwD4+fnh/v37OHXqlFRn7969yM3Nha+vr1Tn4MGDePLkiVQnOjoaTZo0Qe3atQ3ZZCIiIpKhUgeYjIwMxMfHIz4+HsDTibvx8fFISUmBQqHAe++9h08++QTbtm3DuXPnMHLkSLi6ukp3KjVt2hQ9evTA2LFjcfz4cRw5cgTjx4/HkCFD4OrqCgAYNmwYlEolQkNDcf78eWzcuBGLFy/GpEmTDNZxIiIiki+z0u5w8uRJdO3aVVrPCxXBwcFYvXo1PvjgAzx8+BBhYWG4f/8+OnXqhF27dsHc3FzaZ926dRg/fjxeeeUVmJiYoH///vjyyy+l7ba2tti9ezfCw8PRpk0bODg4YObMmXrPiiEiIqKaq1zPganK+ByYGqQq3+3DZ8IQEZVKpTwHhoiIiKgiMMAQERGR7JR6DgwREZExha4+IX29IqRdJbaEqjIGGJKfqjznhYiIKgQvIREREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkezwNmoiIpINPiOG8jDAEBFRpXs2mBCVBC8hERERkewwwBAREZHs8BISERHJEufD1GwMMEREVCEYOMiQeAmJiIiIZIcBhoiIiGSHl5CIiKjC8bZpKi8GGCIiqrIYdKgwDDAkD+sHV3YLiKgEng8cnKxLxsI5MERERCQ7DDBEREQkOwwwREREJDucA0NEREZTUZNwOfem5mGAITKm5ycfD9tYOe0gIqpmeAmJiIiIZIcjMEREVO3wvUvVH0dgiIiISHYYYIiIiEh2eAmJiIjKhY/7p8rAERgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHU7iJSKiao3PhKmeDD4C4+HhAYVCkW8JDw8HAHTp0iXftrfeekvvGCkpKejduzcsLS3h5OSEKVOmIDs729BNJSIiIpky+AjMiRMnkJOTI60nJCSgW7duGDhwoFQ2duxYzJkzR1q3tLSUvs7JyUHv3r2h0Whw9OhRpKamYuTIkahVqxbmzZtn6OYSERGRDBk8wDg6Ouqtz58/H56enujcubNUZmlpCY1GU+D+u3fvRmJiImJiYuDs7IxWrVph7ty5mDp1KiIiIqBUKg3dZCIiKiU++4Uqm1En8WZlZeG7777D6NGjoVAopPJ169bBwcEBzZs3x/Tp0/Ho0SNpW2xsLHx8fODs7CyVBQUFQafT4fz584WeKzMzEzqdTm8hIiKi6smok3i3bt2K+/fvIyQkRCobNmwY3N3d4erqirNnz2Lq1KlISkrC5s2bAQBarVYvvACQ1rVabaHnioyMxOzZsw3fCSIiIqpyjBpgVqxYgZ49e8LV1VUqCwsLk7728fGBi4sLXnnlFVy5cgWenp5lPtf06dMxadIkaV2n08HNza3MxyMiIqKqy2gB5tq1a4iJiZFGVgrj6+sLALh8+TI8PT2h0Whw/PhxvTppaWkAUOi8GQBQqVRQqVTlbDURERHJgdHmwKxatQpOTk7o3bt3kfXi4+MBAC4uLgAAPz8/nDt3Djdv3pTqREdHQ61Ww9vb21jNJSIiIhkxyghMbm4uVq1aheDgYJiZ/e8UV65cwfr169GrVy/UqVMHZ8+excSJExEQEIAWLVoAALp37w5vb2+MGDECUVFR0Gq1mDFjBsLDwznCQkRERACMFGBiYmKQkpKC0aNH65UrlUrExMRg0aJFePjwIdzc3NC/f3/MmDFDqmNqaoodO3bg7bffhp+fH6ysrBAcHKz33BgiIiKq2YwSYLp37w4hRL5yNzc3HDhwoNj93d3d8csvvxijaURERFQN8GWOREREJDsMMERERCQ7fBs1UUVaP/h/Xw/bWHntIColvjqAqhqOwBAREZHsMMAQERGR7DDAEBERkexwDgwREdUYz8/lWRHSrpJaQuXFERgiIiKSHQYYIiIikh0GGCIiIpIdBhgiIiKSHQYYIiIikh3ehURV07NPrCUiInoOR2CIiIhIdjgCQ0RENdazz4XhM2HkhSMwREREJDsMMERERCQ7DDBEREQkOwwwREREJDsMMERERCQ7DDBEREQkOwwwREREJDt8DgwREUn4XBSSC47AEBERkexwBIaIiAr07GgMUVXDAENERARePpMbXkIiIiIi2WGAISIiItlhgCEiIiLZYYAhIiIi2WGAISIiItlhgCEiIiLZYYAhIiIi2eFzYIiIajA+rI7kigGGqLKsH6y/Pmxj5bSDiEiGDH4JKSIiAgqFQm/x8vKStj9+/Bjh4eGoU6cOrK2t0b9/f6SlpekdIyUlBb1794alpSWcnJwwZcoUZGdnG7qpREREJFNGGYFp1qwZYmJi/ncSs/+dZuLEidi5cyc2bdoEW1tbjB8/Hm+88QaOHDkCAMjJyUHv3r2h0Whw9OhRpKamYuTIkahVqxbmzZtnjOYSERGRzBglwJiZmUGj0eQrT09Px4oVK7B+/Xq8/PLLAIBVq1ahadOmOHbsGF566SXs3r0biYmJiImJgbOzM1q1aoW5c+di6tSpiIiIgFKpNEaTiYiISEaMchfSpUuX4OrqigYNGmD48OFISUkBAJw6dQpPnjxBYGCgVNfLywv16tVDbGwsACA2NhY+Pj5wdnaW6gQFBUGn0+H8+fOFnjMzMxM6nU5vISIiourJ4CMwvr6+WL16NZo0aYLU1FTMnj0b/v7+SEhIgFarhVKphJ2dnd4+zs7O0Gq1AACtVqsXXvK2520rTGRkJGbPnm3YzlDFen5SKxERUSEMHmB69uwpfd2iRQv4+vrC3d0dP/zwAywsLAx9Osn06dMxadIkaV2n08HNzc1o5yMiIqLKY/QH2dnZ2aFx48a4fPkyNBoNsrKycP/+fb06aWlp0pwZjUaT766kvPWC5tXkUalUUKvVegsRERFVT0YPMBkZGbhy5QpcXFzQpk0b1KpVC3v27JG2JyUlISUlBX5+fgAAPz8/nDt3Djdv3pTqREdHQ61Ww9vb29jNJSIiIhkw+CWkyZMno0+fPnB3d8eNGzcwa9YsmJqaYujQobC1tUVoaCgmTZoEe3t7qNVqTJgwAX5+fnjppZcAAN27d4e3tzdGjBiBqKgoaLVazJgxA+Hh4VCpVIZuLhERUT7PP6F4RUi7SmoJFcbgAeb69esYOnQo7ty5A0dHR3Tq1AnHjh2Do6MjAGDhwoUwMTFB//79kZmZiaCgIPz73/+W9jc1NcWOHTvw9ttvw8/PD1ZWVggODsacOXMM3VQiIiKSKYUQQlR2I4xBp9PB1tYW6enpnA8jFzX9LiS+SoAqAd+FVDIcgak4Jf39zbdRExERkezwZY5ERDUMR12oOuAIDBEREckOAwwRERHJDgMMERERyQ7nwBARERXj2XlDvCOpauAIDBEREckOAwwRERHJDi8hEVUVzz7Ijw+1IyIqEkdgiIiISHYYYIiIiEh2GGCIiIhIdjgHhoioBuDrA6i64QgMERERyQ5HYIiIiEqBD7WrGhhgqPI8e9swERFRKfASEhEREckOAwwRERHJDgMMERERyQ7nwBARVUO8bZqqO47AEBERkewwwBAREZHs8BISUVX0/C3mfDs1EZEejsAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7DDAEBERkewwwBAREZHsMMAQERGR7PA5MERERGX07CsbVoS0q8SW1DwcgSEiIiLZYYAhIiIi2WGAISIiItlhgCEiIiLZ4SReIiIZ4yTSquPZ7wXA74exGXwEJjIyEu3atYONjQ2cnJzQr18/JCUl6dXp0qULFAqF3vLWW2/p1UlJSUHv3r1haWkJJycnTJkyBdnZ2YZuLlW09YP/txCRQYWuPiEtRNWdwUdgDhw4gPDwcLRr1w7Z2dn48MMP0b17dyQmJsLKykqqN3bsWMyZM0dat7S0lL7OyclB7969odFocPToUaSmpmLkyJGoVasW5s2bZ+gmExERkcwYPMDs2rVLb3316tVwcnLCqVOnEBAQIJVbWlpCo9EUeIzdu3cjMTERMTExcHZ2RqtWrTB37lxMnToVERERUCqVhm42UdX2/IjVsI2V0w4ioirC6JN409PTAQD29vZ65evWrYODgwOaN2+O6dOn49GjR9K22NhY+Pj4wNnZWSoLCgqCTqfD+fPnCzxPZmYmdDqd3kJERETVk1En8ebm5uK9995Dx44d0bx5c6l82LBhcHd3h6urK86ePYupU6ciKSkJmzdvBgBotVq98AJAWtdqtQWeKzIyErNnzzZST4iIiKgqMWqACQ8PR0JCAg4fPqxXHhYWJn3t4+MDFxcXvPLKK7hy5Qo8PT3LdK7p06dj0qRJ0rpOp4Obm1vZGk5ERERVmtEuIY0fPx47duzAvn378MILLxRZ19fXFwBw+fJlAIBGo0FaWppenbz1wubNqFQqqNVqvYWIiIiqJ4OPwAghMGHCBGzZsgX79+9H/fr1i90nPj4eAODi4gIA8PPzw6effoqbN2/CyckJABAdHQ21Wg1vb29DN5mISDZ4izTRUwYPMOHh4Vi/fj1+/vln2NjYSHNWbG1tYWFhgStXrmD9+vXo1asX6tSpg7Nnz2LixIkICAhAixYtAADdu3eHt7c3RowYgaioKGi1WsyYMQPh4eFQqVSGbjIRERHJjMEDzNdffw3g6cPqnrVq1SqEhIRAqVQiJiYGixYtwsOHD+Hm5ob+/ftjxowZUl1TU1Ps2LEDb7/9Nvz8/GBlZYXg4GC958YQERFVZXxKsnEZ5RJSUdzc3HDgwIFij+Pu7o5ffvnFUM0iIiKiaoQvcyQiIiLZYYAhIiIi2WGAISIiItlhgCEiIiLZYYAhIiIi2THqqwSI8r1FmYgK9PwD6njbbfXCW6oNjwGGiKgK4hN3iYrGAEMkR8+ObA3bWHntICKqJAwwREQlZOjLPBxlISo7BhgiqpEMMSehsGNwPguR8fEuJCIiIpIdBhgiIiKSHV5CIpK7529V56TeQpV0zklZLi+V5dhUM/ESo2EwwBBRtcLnbRDVDAwwRFTjGXtUhKMuRIbHAEOGx6fvUhVhiODA8EFUNXESLxEREckOR2CIqMrg5EYiKikGGCKqsgq7fMNgQ9UJJ56XDQMMEVUqzjEhorJggKHy46RdqmAMPUTEAENU3fBN1USyxctJJccAQ0RGx8m5RGRovI2aiIiIZIcjMETVGd+TRCRbHLksGgMMEVU4TsIlovLiJSQiIiKSHQYYIiIikh1eQqLS43NfiIgqHG+x1scAQ1STGPkZMZzbQkQVhQGGSoajLlQABhYiqiwMMESUD2/fJKKqjgGGiEqFoy5ElY/zYRhgqDC8ZFT9Ffk9nqy3xtBCRFUNAwwREZGM1dRLvgohhKjsRhTmq6++woIFC6DVatGyZUssWbIE7du3L9G+Op0Otra2SE9Ph1qtNnJLqwmOutRo8X/dL3HdJc6fGK8hRGQwcgwzJf39XWVHYDZu3IhJkybhm2++ga+vLxYtWoSgoCAkJSXBycmpspsnX0a+jZaqttKEFCKSv+o8V6bKjsD4+vqiXbt2WLp0KQAgNzcXbm5umDBhAqZNm1bs/hyBKQRHWWo0YwSYokZjJqTNKHFdIqo8VSncyHoEJisrC6dOncL06dOlMhMTEwQGBiI2NrbAfTIzM5GZmSmtp6enA3j6D1Hj/BBS2S2gCnb27/RKO3fWPxl66+Nu/i+kZBRTl4iqhhFf75O+/mp4m0psyf9+bxc3vlIlA8zt27eRk5MDZ2dnvXJnZ2dcvHixwH0iIyMxe/bsfOVubm5GaSMR5dmrt/ZdKeoSUdXz3TuV3YKnHjx4AFtb20K3V8kAUxbTp0/HpEmTpPXc3FzcvXsXderUgUKhMPj5dDod3Nzc8Ndff9WoS1Tsd83qN1Bz+85+s981QVXstxACDx48gKura5H1qmSAcXBwgKmpKdLS0vTK09LSoNFoCtxHpVJBpVLpldnZ2RmriRK1Wl1lvukVif2ueWpq39nvmoX9rhqKGnnJY1IB7Sg1pVKJNm3aYM+ePVJZbm4u9uzZAz8/v0psGREREVUFVXIEBgAmTZqE4OBgtG3bFu3bt8eiRYvw8OFDjBo1qrKbRkRERJWsygaYwYMH49atW5g5cya0Wi1atWqFXbt25ZvYW1lUKhVmzZqV77JVdcd+16x+AzW37+w3+10TyLnfVfY5MERERESFqZJzYIiIiIiKwgBDREREssMAQ0RERLLDAENERESywwBDREREssMAUwp3797F8OHDoVarYWdnh9DQUGRkFP5yurt372LChAlo0qQJLCwsUK9ePbz77rvSiyblorT9BoBly5ahS5cuUKvVUCgUuH//fsU0thy++uoreHh4wNzcHL6+vjh+/HiR9Tdt2gQvLy+Ym5vDx8cHv/zySwW11LBK0+/z58+jf//+8PDwgEKhwKJFiyquoUZQmr4vX74c/v7+qF27NmrXro3AwMBiPyNVVWn6vXnzZrRt2xZ2dnawsrJCq1atsHbt2gpsreGU9mc8z4YNG6BQKNCvXz/jNtBIStPv1atXQ6FQ6C3m5uYV2NpSEFRiPXr0EC1bthTHjh0Thw4dEg0bNhRDhw4ttP65c+fEG2+8IbZt2yYuX74s9uzZIxo1aiT69+9fga0uv9L2WwghFi5cKCIjI0VkZKQAIO7du1cxjS2jDRs2CKVSKVauXCnOnz8vxo4dK+zs7ERaWlqB9Y8cOSJMTU1FVFSUSExMFDNmzBC1atUS586dq+CWl09p+338+HExefJk8f333wuNRiMWLlxYsQ02oNL2fdiwYeKrr74SZ86cERcuXBAhISHC1tZWXL9+vYJbXj6l7fe+ffvE5s2bRWJiorh8+bJYtGiRMDU1Fbt27arglpdPafudJzk5WdStW1f4+/uLvn37VkxjDai0/V61apVQq9UiNTVVWrRabQW3umQYYEooMTFRABAnTpyQyn799VehUCjE33//XeLj/PDDD0KpVIonT54Yo5kGV95+79u3TxYBpn379iI8PFxaz8nJEa6uriIyMrLA+oMGDRK9e/fWK/P19RXjxo0zajsNrbT9fpa7u7usA0x5+i6EENnZ2cLGxkasWbPGWE00ivL2WwghWrduLWbMmGGM5hlNWfqdnZ0tOnToIL799lsRHBwsywBT2n6vWrVK2NraVlDryoeXkEooNjYWdnZ2aNu2rVQWGBgIExMTxMXFlfg46enpUKvVMDOrsg9B1mOofldlWVlZOHXqFAIDA6UyExMTBAYGIjY2tsB9YmNj9eoDQFBQUKH1q6Ky9Lu6METfHz16hCdPnsDe3t5YzTS48vZbCIE9e/YgKSkJAQEBxmyqQZW133PmzIGTkxNCQ0MropkGV9Z+Z2RkwN3dHW5ubujbty/Onz9fEc0tNQaYEtJqtXByctIrMzMzg729PbRabYmOcfv2bcydOxdhYWHGaKJRGKLfVd3t27eRk5OT7zUVzs7OhfZRq9WWqn5VVJZ+VxeG6PvUqVPh6uqaL8hWZWXtd3p6OqytraFUKtG7d28sWbIE3bp1M3ZzDaYs/T58+DBWrFiB5cuXV0QTjaIs/W7SpAlWrlyJn3/+Gd999x1yc3PRoUMHXL9+vSKaXCo1PsBMmzYt34Sl55eLFy+W+zw6nQ69e/eGt7c3IiIiyt/wcqqofhNVR/Pnz8eGDRuwZcuWqjvB0YBsbGwQHx+PEydO4NNPP8WkSZOwf//+ym6W0Tx48AAjRozA8uXL4eDgUNnNqVB+fn4YOXIkWrVqhc6dO2Pz5s1wdHTEf/7zn8puWj7yuI5hRO+//z5CQkKKrNOgQQNoNBrcvHlTrzw7Oxt3796FRqMpcv8HDx6gR48esLGxwZYtW1CrVq3yNrvcKqLfcuHg4ABTU1OkpaXplaelpRXaR41GU6r6VVFZ+l1dlKfvn3/+OebPn4+YmBi0aNHCmM00uLL228TEBA0bNgQAtGrVChcuXEBkZCS6dOlizOYaTGn7feXKFVy9ehV9+vSRynJzcwE8HYFOSkqCp6encRttAIb4Ga9VqxZat26Ny5cvG6OJ5VLjR2AcHR3h5eVV5KJUKuHn54f79+/j1KlT0r579+5Fbm4ufH19Cz2+TqdD9+7doVQqsW3btirz15qx+y0nSqUSbdq0wZ49e6Sy3Nxc7NmzB35+fgXu4+fnp1cfAKKjowutXxWVpd/VRVn7HhUVhblz52LXrl1688LkwlDf89zcXGRmZhqjiUZR2n57eXnh3LlziI+Pl5bXXnsNXbt2RXx8PNzc3Cqy+WVmiO93Tk4Ozp07BxcXF2M1s+wqexaxnPTo0UO0bt1axMXFicOHD4tGjRrp3U58/fp10aRJExEXFyeEECI9PV34+voKHx8fcfnyZb3b0rKzsyurG6VW2n4LIURqaqo4c+aMWL58uQAgDh48KM6cOSPu3LlTGV0o1oYNG4RKpRKrV68WiYmJIiwsTNjZ2Um3D44YMUJMmzZNqn/kyBFhZmYmPv/8c3HhwgUxa9Ys2d5GXZp+Z2ZmijNnzogzZ84IFxcXMXnyZHHmzBlx6dKlyupCmZW27/PnzxdKpVL8+OOPej/LDx48qKwulElp+z1v3jyxe/duceXKFZGYmCg+//xzYWZmJpYvX15ZXSiT0vb7eXK9C6m0/Z49e7b47bffxJUrV8SpU6fEkCFDhLm5uTh//nxldaFQDDClcOfOHTF06FBhbW0t1Gq1GDVqlN5/XsnJyQKA2LdvnxDif7cQF7QkJydXTifKoLT9FkKIWbNmFdjvVatWVXwHSmjJkiWiXr16QqlUivbt24tjx45J2zp37iyCg4P16v/www+icePGQqlUimbNmomdO3dWcIsNozT9zvteP7907ty54htuAKXpu7u7e4F9nzVrVsU3vJxK0++PPvpINGzYUJibm4vatWsLPz8/sWHDhkpodfmV9mf8WXINMEKUrt/vvfeeVNfZ2Vn06tVLnD59uhJaXTyFEEJU3HgPERERUfnV+DkwREREJD8MMERERCQ7DDBEREQkOwwwREREJDsMMERERCQ7DDBEREQkOwwwREREJDsMMERERCQ7DDBEREQkOwwwREREJDsMMERERCQ7/w/wr+yxpbaelwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bos_cos_sim = sae_res_decoder[:, 61] @ sae_skip_encoder.T\n",
    "print(bos_cos_sim.mean(), bos_cos_sim.median())\n",
    "random_other_point = sae_res_decoder[:, 62] @ sae_skip_encoder.T\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(bos_cos_sim.cpu().numpy(), bins=100, alpha=0.7, label=\"BOS_T0_61\")\n",
    "plt.hist(random_other_point.cpu().numpy(), bins=100, alpha=0.7, label=\"Random_T0_62\")\n",
    "plt.title(\"Cosine Similarity of T0-Dec w/ all T1-Enc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Commented out example data, assume these are defined earlier\n",
    "# jacc_sim_res_dim0 = torch.load('path_to_res_dim0_tensor.pt')  # Replace with your actual tensor path\n",
    "# jacc_sim_skip_dim0 = torch.load('path_to_skip_dim0_tensor.pt')  # Replace with your actual tensor path\n",
    "# jacc_sim_res_to_skip = torch.load('path_to_res_to_skip_tensor.pt')  # Replace with your actual tensor path\n",
    "\n",
    "# # Set a threshold for filtering weak connections\n",
    "# threshold = 0.8\n",
    "\n",
    "# def filter_connections(matrix, threshold):\n",
    "#     indices = torch.nonzero(matrix >= threshold, as_tuple=False)\n",
    "#     filtered_connections = []\n",
    "#     for i in range(matrix.size(0)):\n",
    "#         connections = [[j.item(), matrix[i, j].item()] for j in indices[indices[:, 0] == i][:, 1]]\n",
    "#         filtered_connections.append(connections)\n",
    "#     return filtered_connections\n",
    "\n",
    "# filtered_within_res = filter_connections(jacc_sim_res_dim0, threshold)\n",
    "# filtered_within_skip = filter_connections(jacc_sim_skip_dim0, threshold)\n",
    "# filtered_res_to_skip = filter_connections(jacc_sim_res_to_skip, threshold)\n",
    "\n",
    "# def get_connected_nodes(filtered_connections):\n",
    "#     connected_nodes = set()\n",
    "#     for i, connections in enumerate(filtered_connections):\n",
    "#         if connections:\n",
    "#             connected_nodes.add(i)\n",
    "#             for j, _ in connections:\n",
    "#                 connected_nodes.add(j)\n",
    "#     return connected_nodes\n",
    "\n",
    "# connected_res_nodes = get_connected_nodes(filtered_within_res)\n",
    "# connected_skip_nodes = get_connected_nodes(filtered_within_skip)\n",
    "# connected_res_to_skip_nodes = set()\n",
    "# for i, connections in enumerate(filtered_res_to_skip):\n",
    "#     if connections:\n",
    "#         connected_res_to_skip_nodes.add(i)\n",
    "#         for j, _ in connections:\n",
    "#             connected_skip_nodes.add(j)\n",
    "\n",
    "# connected_res_nodes = connected_res_nodes.union(connected_res_to_skip_nodes)\n",
    "\n",
    "# Derive shapes from the tensor sizes\n",
    "# res_feature_labels = [\"Res_Feature_\" + str(i) for i in range(jacc_sim_res_dim0.size(0)) if i in connected_res_nodes]\n",
    "# skip_feature_labels = [\"Skip_Feature_\" + str(i) for i in range(jacc_sim_skip_dim0.size(0)) if i in connected_skip_nodes]\n",
    "# res_feature_hovertips = [\"Res_Hover_\" + str(i) for i in range(jacc_sim_res_dim0.size(0)) if i in connected_res_nodes]\n",
    "# skip_feature_hovertips = [\"Skip_Hover_\" + str(i) for i in range(jacc_sim_skip_dim0.size(0)) if i in connected_skip_nodes]\n",
    "\n",
    "# Generate HTML content with correct variable substitution\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        body, html {{\n",
    "            margin: 0;\n",
    "            height: 100%;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .hover-label {{\n",
    "            visibility: hidden;\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "            pointer-events: none;\n",
    "            background-color: white;\n",
    "            padding: 2px;\n",
    "            border-radius: 2px;\n",
    "        }}\n",
    "        .zoomable {{\n",
    "            cursor: grab;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"100%\" height=\"100%\" class=\"zoomable\"></svg>\n",
    "    <script>\n",
    "        const resFeatureLabels = {res_feature_labels};\n",
    "        const skipFeatureLabels = {skip_feature_labels};\n",
    "        const resFeatureHovertips = {res_feature_hovertips};\n",
    "        const skipFeatureHovertips = {skip_feature_hovertips};\n",
    "        const withinResCorrelation = {within_res_correlation};\n",
    "        const withinSkipCorrelation = {within_skip_correlation};\n",
    "        const resSkipCorrelation = {res_skip_correlation};\n",
    "\n",
    "        const nodes = [];\n",
    "        const nodeIdMap = new Map();\n",
    "\n",
    "        resFeatureLabels.forEach((label, i) => {{\n",
    "            const nodeId = `res_${{i}}`;\n",
    "            nodes.push({{ id: nodeId, group: 'res', color: 'blue', defaultLabel: label, hoverLabel: resFeatureHovertips[i] }});\n",
    "            nodeIdMap.set(i, nodeId);\n",
    "        }});\n",
    "\n",
    "        skipFeatureLabels.forEach((label, i) => {{\n",
    "            const nodeId = `skip_${{i}}`;\n",
    "            nodes.push({{ id: nodeId, group: 'skip', color: 'red', defaultLabel: label, hoverLabel: skipFeatureHovertips[i] }});\n",
    "            nodeIdMap.set(i, nodeId);\n",
    "        }});\n",
    "\n",
    "        const links = [];\n",
    "        withinResCorrelation.forEach((connections, i) => {{\n",
    "            if (!Array.isArray(connections) || connections.length === 0) {{\n",
    "                return;\n",
    "            }}\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                const source = nodeIdMap.get(i);\n",
    "                const target = nodeIdMap.get(j);\n",
    "                links.push({{ source: source, target: target, value: value, color: 'blue' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        withinSkipCorrelation.forEach((connections, i) => {{\n",
    "            if (!Array.isArray(connections) || connections.length === 0) {{\n",
    "                return;\n",
    "            }}\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                const source = nodeIdMap.get(i);\n",
    "                const target = nodeIdMap.get(j);\n",
    "                links.push({{ source: source, target: target, value: value, color: 'red' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        resSkipCorrelation.forEach((connections, i) => {{\n",
    "            if (!Array.isArray(connections) || connections.length === 0) {{\n",
    "                return;\n",
    "            }}\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                const source = nodeIdMap.get(i);\n",
    "                const target = nodeIdMap.get(j + resFeatureLabels.length);\n",
    "                links.push({{ source: source, target: target, value: value, color: 'green' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = window.innerWidth,\n",
    "            height = window.innerHeight;\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(1))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-30))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2))\n",
    "            .force(\"x\", d3.forceX(width / 2).strength(0.1))\n",
    "            .force(\"y\", d3.forceY(height / 2).strength(0.1));\n",
    "\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(Math.abs(d.value)))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 5)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 1);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"visible\");\n",
    "            }})\n",
    "            .on(\"mouseout\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 0.6);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"hidden\");\n",
    "            }});\n",
    "\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"hover-labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"hover-label\")\n",
    "            .attr(\"id\", d => `hover-label-${{d.id}}`)\n",
    "            .attr(\"dy\", -30)\n",
    "            .text(d => d.hoverLabel);\n",
    "\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".hover-label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        svg.call(d3.zoom().on(\"zoom\", (event) => {{\n",
    "            svg.attr(\"transform\", event.transform);\n",
    "        }}));\n",
    "\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    res_feature_labels=json.dumps(res_feature_labels),\n",
    "    skip_feature_labels=json.dumps(skip_feature_labels),\n",
    "    res_feature_hovertips=json.dumps(res_feature_hovertips),\n",
    "    skip_feature_hovertips=json.dumps(skip_feature_hovertips),\n",
    "    within_res_correlation=json.dumps(filtered_within_res),\n",
    "    within_skip_correlation=json.dumps(filtered_within_skip),\n",
    "    res_skip_correlation=json.dumps(filtered_res_to_skip)\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('graph.html', 'w') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2539, 0.5762711763381958), (3130, 1.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_within_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Your actual data should be assigned to these variables\n",
    "# res_feature_labels = [...]  # Replace with your actual data\n",
    "# skip_feature_labels = [...]  # Replace with your actual data\n",
    "# jacc_sim_res_dim0 = torch.tensor([...])  # Replace with your actual tensor\n",
    "# jacc_sim_skip_dim0 = torch.tensor([...])  # Replace with your actual tensor\n",
    "# jacc_sim_res_to_skip = torch.tensor([...])  # Replace with your actual tensor\n",
    "\n",
    "# Set a threshold for filtering weak connections\n",
    "threshold = 0.1\n",
    "\n",
    "def filter_connections(matrix, threshold):\n",
    "    filtered_connections = []\n",
    "    for i in tqdm(range(matrix.size(0))):\n",
    "        connections = []\n",
    "        for j in range(matrix.size(1)):\n",
    "            if matrix[i, j] >= threshold:\n",
    "                connections.append((j, matrix[i, j].item()))\n",
    "        filtered_connections.append(connections)\n",
    "    return filtered_connections\n",
    "\n",
    "# Filter the matrices based on the threshold\n",
    "filtered_within_res = filter_connections(jacc_sim_res_dim0, threshold)\n",
    "filtered_within_skip = filter_connections(jacc_sim_skip_dim0, threshold)\n",
    "filtered_res_to_skip = filter_connections(jacc_sim_res_to_skip, threshold)\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .hover-label {{\n",
    "            visibility: hidden;\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "            pointer-events: none;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"1600\" height=\"900\"></svg>\n",
    "    <script>\n",
    "        // Data for the graph\n",
    "        const resFeatureLabels = {res_feature_labels};\n",
    "        const skipFeatureLabels = {skip_feature_labels};\n",
    "        const withinResCorrelation = {within_res_correlation};\n",
    "        const withinSkipCorrelation = {within_skip_correlation};\n",
    "        const resSkipCorrelation = {res_skip_correlation};\n",
    "\n",
    "        // Create nodes\n",
    "        const nodes = [];\n",
    "        for (let i = 0; i < resFeatureLabels.length; i++) {{\n",
    "            nodes.push({{ id: `res_${{i}}`, group: 'res', color: 'blue', defaultLabel: resFeatureLabels[i], hoverLabel: `Hover info for {{resFeatureLabels[i]}}` }});\n",
    "        }}\n",
    "        for (let i = 0; i < skipFeatureLabels.length; i++) {{\n",
    "            nodes.push({{ id: `skip_${{i}}`, group: 'skip', color: 'red', defaultLabel: skipFeatureLabels[i], hoverLabel: `Hover info for {{skipFeatureLabels[i]}}` }});\n",
    "        }}\n",
    "\n",
    "        // Create links (only for non-zero weights)\n",
    "        const links = [];\n",
    "        withinResCorrelation.forEach((connections, i) => {{\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                links.push({{ source: `res_${{i}}`, target: `res_${{j}}`, value: value, color: 'blue' }});\n",
    "            }});\n",
    "        }});\n",
    "        withinSkipCorrelation.forEach((connections, i) => {{\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                links.push({{ source: `skip_${{i}}`, target: `skip_${{j}}`, value: value, color: 'red' }});\n",
    "            }});\n",
    "        }});\n",
    "        resSkipCorrelation.forEach((connections, i) => {{\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                links.push({{ source: `res_${{i}}`, target: `skip_${{j}}`, value: value, color: 'green' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        // Set up SVG and simulation\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = +svg.attr(\"width\"),\n",
    "            height = +svg.attr(\"height\");\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(d => Math.exp(-d.value)))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-500))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
    "\n",
    "        // Add links\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(Math.abs(d.value)))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        // Add nodes\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 5)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 1);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"visible\");\n",
    "            }})\n",
    "            .on(\"mouseout\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 0.6);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"hidden\");\n",
    "            }});\n",
    "\n",
    "        // Add visible labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        // Add hover labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"hover-labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"hover-label\")\n",
    "            .attr(\"id\", d => `hover-label-${{d.id}}`)\n",
    "            .attr(\"dy\", -30)\n",
    "            .text(d => d.hoverLabel);\n",
    "\n",
    "        // Update simulation on each tick\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".hover-label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        // Drag functions\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    res_feature_labels=res_feature_labels,\n",
    "    skip_feature_labels=skip_feature_labels,\n",
    "    within_res_correlation=filtered_within_res,\n",
    "    within_skip_correlation=filtered_within_skip,\n",
    "    res_skip_correlation=filtered_res_to_skip\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('graph.html', 'w') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_within_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2020103 connections above 0.01\n",
      " 1856736 connections above 0.01\n",
      " 2166155 connections above 0.01\n",
      "------------------------------------------------------------\n",
      " 982590 connections above 0.05\n",
      " 590663 connections above 0.05\n",
      " 1078060 connections above 0.05\n",
      "------------------------------------------------------------\n",
      " 395397 connections above 0.1\n",
      " 228890 connections above 0.1\n",
      " 467454 connections above 0.1\n",
      "------------------------------------------------------------\n",
      " 107448 connections above 0.2\n",
      " 55632 connections above 0.2\n",
      " 116926 connections above 0.2\n",
      "------------------------------------------------------------\n",
      " 47336 connections above 0.3\n",
      " 20386 connections above 0.3\n",
      " 43322 connections above 0.3\n",
      "------------------------------------------------------------\n",
      " 25744 connections above 0.4\n",
      " 9977 connections above 0.4\n",
      " 21734 connections above 0.4\n",
      "------------------------------------------------------------\n",
      " 15718 connections above 0.5\n",
      " 6015 connections above 0.5\n",
      " 13693 connections above 0.5\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0.01,0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    print(f\" {(jacc_sim_skip_dim0 > threshold).sum()} connections above {threshold}\")\n",
    "    print(f\" {(jacc_sim_res_dim0 > threshold).sum()} connections above {threshold}\")\n",
    "    print(f\" {(jacc_sim_res_to_skip > threshold).sum()} connections above {threshold}\")\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example data\n",
    "num_features = 5\n",
    "within_res_correlation = torch.randn(num_features, num_features).tolist()\n",
    "res_skip_correlation = torch.randn(num_features, num_features).tolist()\n",
    "within_skip_correlation = torch.randn(num_features, num_features).tolist()\n",
    "default_labels = [f'Feature_{i}' for i in range(num_features)]\n",
    "hover_labels = [f'Hover info for Feature_{i}' for i in range(num_features)]\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .hover-label {{\n",
    "            visibility: hidden;\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "            pointer-events: none;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"960\" height=\"600\"></svg>\n",
    "    <script>\n",
    "        // Data for the graph\n",
    "        const numFeatures = {num_features};\n",
    "        const withinResCorrelation = {within_res_correlation};\n",
    "        const resSkipCorrelation = {res_skip_correlation};\n",
    "        const withinSkipCorrelation = {within_skip_correlation};\n",
    "        const defaultLabels = {default_labels};\n",
    "        const hoverLabels = {hover_labels};\n",
    "\n",
    "        // Create nodes\n",
    "        const nodes = [];\n",
    "        for (let i = 0; i < numFeatures; i++) {{\n",
    "            nodes.push({{ id: `res_${{i}}`, group: 'res', color: 'blue', defaultLabel: defaultLabels[i], hoverLabel: hoverLabels[i] }});\n",
    "            nodes.push({{ id: `skip_${{i}}`, group: 'skip', color: 'red', defaultLabel: defaultLabels[i], hoverLabel: hoverLabels[i] }});\n",
    "        }}\n",
    "\n",
    "        // Create links\n",
    "        const links = [];\n",
    "        for (let i = 0; i < numFeatures; i++) {{\n",
    "            for (let j = 0; j < numFeatures; j++) {{\n",
    "                if (i !== j) {{\n",
    "                    links.push({{ source: `res_${{i}}`, target: `res_${{j}}`, value: withinResCorrelation[i][j], color: 'blue' }});\n",
    "                    links.push({{ source: `skip_${{i}}`, target: `skip_${{j}}`, value: withinSkipCorrelation[i][j], color: 'red' }});\n",
    "                }}\n",
    "                links.push({{ source: `res_${{i}}`, target: `skip_${{j}}`, value: resSkipCorrelation[i][j], color: 'green' }});\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        // Set up SVG and simulation\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = +svg.attr(\"width\"),\n",
    "            height = +svg.attr(\"height\");\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(1))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-400))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
    "\n",
    "        // Add links\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(d.value))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        // Add nodes\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 10)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 1);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"visible\");\n",
    "            }})\n",
    "            .on(\"mouseout\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 0.6);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"hidden\");\n",
    "            }});\n",
    "\n",
    "        // Add visible labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        // Add hover labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"hover-labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"hover-label\")\n",
    "            .attr(\"id\", d => `hover-label-${{d.id}}`)\n",
    "            .attr(\"dy\", -30)\n",
    "            .text(d => d.hoverLabel);\n",
    "\n",
    "        // Update simulation on each tick\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".hover-label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        // Drag functions\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    num_features=num_features,\n",
    "    within_res_correlation=within_res_correlation,\n",
    "    res_skip_correlation=res_skip_correlation,\n",
    "    within_skip_correlation=within_skip_correlation,\n",
    "    default_labels=default_labels,\n",
    "    hover_labels=hover_labels\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('graph.html', 'w') as f:\n",
    "    f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 8192])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now let's just naively multiply the decoder weight by the encoder weight\n",
    "res_dec = sae_res.decoder.weight.data\n",
    "skip_enc = skip_sae.encoder.weight.data\n",
    "combined = skip_enc @ res_dec\n",
    "# res_dec = sae_res.encoder.weight.data\n",
    "# skip_dec = skip_sae.decoder.weight.data\n",
    "# combined = res_dec@ skip_dec\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_writing -= W_writing.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'd like to fold in layer norm\n",
    "ln_mlp = model.gpt_neox.layers[layer].post_attention_layernorm\n",
    "# 1. center the weights that write into res\n",
    "centered_decoder = res_dec - res_dec.mean(dim=1, keepdim=True)\n",
    "# 2. Add in layer norm weight to the read in weight\n",
    "ln_w = ln_mlp.weight.data\n",
    "ln_b = ln_mlp.bias.data\n",
    "\n",
    "# Get weights and biases of the next linear layer\n",
    "W = skip_sae.encoder.weight.data.T  # The next linear layer weights\n",
    "# B = skip_sae.mag_bias.data  # The next linear layer biases\n",
    "B = skip_sae.gate_bias.data  # The next linear layer biases\n",
    "\n",
    "# Calculate W_eff and B_eff\n",
    "W_eff = torch.diag(ln_w) @ W\n",
    "B_eff = B + ln_b @ W\n",
    "\n",
    "# Centering the reading weights\n",
    "W_eff -= W_eff.mean(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 8192]), torch.Size([512, 8192]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centered_decoder.shape, W_eff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Point 1\n",
      "False\n",
      "fold_in_test: 0.53, MSE LN: 0.42\n",
      "Data Point 2\n",
      "False\n",
      "fold_in_test: 0.22, MSE LN: 0.26\n",
      "Data Point 3\n",
      "False\n",
      "fold_in_test: 0.49, MSE LN: 0.51\n",
      "Data Point 4\n",
      "False\n",
      "fold_in_test: 0.78, MSE LN: 0.66\n",
      "Data Point 5\n",
      "False\n",
      "fold_in_test: 0.48, MSE LN: 0.25\n",
      "Data Point 6\n",
      "False\n",
      "fold_in_test: 0.48, MSE LN: 0.54\n",
      "Data Point 7\n",
      "False\n",
      "fold_in_test: 0.60, MSE LN: 0.70\n",
      "Data Point 8\n",
      "False\n",
      "fold_in_test: 0.36, MSE LN: 0.35\n",
      "Data Point 9\n",
      "False\n",
      "fold_in_test: 0.40, MSE LN: 0.34\n",
      "Data Point 10\n",
      "False\n",
      "fold_in_test: 0.93, MSE LN: 0.68\n",
      "Data Point 11\n",
      "False\n",
      "fold_in_test: 0.92, MSE LN: 0.88\n",
      "Data Point 12\n",
      "False\n",
      "fold_in_test: 0.36, MSE LN: 0.32\n",
      "Data Point 13\n",
      "False\n",
      "fold_in_test: 0.93, MSE LN: 0.94\n",
      "Data Point 14\n",
      "False\n",
      "fold_in_test: 0.98, MSE LN: 1.08\n",
      "Data Point 15\n",
      "False\n",
      "fold_in_test: 0.81, MSE LN: 0.87\n",
      "Data Point 16\n",
      "False\n",
      "fold_in_test: 0.34, MSE LN: 0.38\n",
      "Data Point 17\n",
      "False\n",
      "fold_in_test: 0.44, MSE LN: 0.71\n",
      "Data Point 18\n",
      "False\n",
      "fold_in_test: 2.45, MSE LN: 2.28\n",
      "Data Point 19\n",
      "False\n",
      "fold_in_test: 0.47, MSE LN: 0.49\n"
     ]
    }
   ],
   "source": [
    "ln_mlp = model.gpt_neox.layers[2].post_attention_layernorm\n",
    "\n",
    "for d_point_ind in range(1,20):\n",
    "    print(f\"Data Point {d_point_ind}\")\n",
    "    f1 = dictionary_activations_res[d_point_ind].to(device)\n",
    "    f2 = dictionary_activations_skip[d_point_ind].to(device)\n",
    "    f2_hat = torch.clamp(combined @ f1, min=0)\n",
    "    test_f2 = torch.clamp(skip_enc @ ln_mlp(res_dec@f1), min=0)\n",
    "    new_f2 = torch.clamp(skip_sae.encode(ln_mlp(sae_res.decode(f1))), min=0)\n",
    "    fold_in_test = torch.clamp((W_eff.T @ centered_decoder @ f1) + B_eff, min=0) \n",
    "    print(torch.allclose(test_f2, f2_hat, atol=1e-5))\n",
    "    top_ind = f2.topk(5).indices\n",
    "    # for ind in top_ind:\n",
    "    #     print(f\"{fold_in_test[ind]:.2f}, {test_f2[ind]:.2f}, {f2[ind]:.2f}\")\n",
    "    mse = torch.mean((f2[top_ind] - new_f2[top_ind])**2)\n",
    "    mse_ln = torch.mean((f2[top_ind] - test_f2[top_ind])**2)\n",
    "    print(f\"fold_in_test: {mse:.2f}, MSE LN: {mse_ln:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Point 0\n",
      "False\n",
      "19.93, 4.91, 18.92\n",
      "7.09, 1.42, 4.08\n",
      "5.26, 1.19, 2.86\n",
      "5.00, 1.00, 2.73\n",
      "4.04, 0.76, 2.53\n",
      "3.57, 0.69, 2.49\n",
      "4.49, 0.83, 2.47\n",
      "4.04, 0.82, 2.46\n",
      "3.16, 0.65, 2.27\n",
      "3.43, 0.46, 2.26\n",
      "3.73, 0.68, 2.26\n",
      "3.67, 0.78, 2.06\n",
      "3.16, 0.66, 1.88\n",
      "3.16, 0.62, 1.87\n",
      "3.45, 0.52, 1.82\n",
      "Data Point 1\n",
      "False\n",
      "-12.38, -38.03, 2.54\n",
      "1.54, 4.47, 1.85\n",
      "-0.70, -3.41, 0.76\n",
      "0.08, 0.11, 0.26\n",
      "-0.45, -1.92, 0.24\n",
      "-0.89, -3.04, 0.21\n",
      "0.22, 0.24, 0.21\n",
      "0.85, 1.73, 0.20\n",
      "0.46, 0.84, 0.20\n",
      "0.34, 0.13, 0.19\n",
      "0.18, 0.20, 0.19\n",
      "0.33, 0.59, 0.17\n",
      "0.36, 0.59, 0.17\n",
      "0.35, 0.56, 0.17\n",
      "0.42, 0.77, 0.17\n",
      "Data Point 2\n",
      "False\n",
      "1.35, 4.18, 1.89\n",
      "0.13, 0.07, 0.30\n",
      "0.14, 0.28, 0.27\n",
      "0.15, -0.02, 0.24\n",
      "-0.06, -0.07, 0.22\n",
      "0.14, 0.39, 0.19\n",
      "0.00, 0.12, 0.19\n",
      "0.20, 0.31, 0.19\n",
      "-0.07, -0.31, 0.18\n",
      "0.42, 0.65, 0.18\n",
      "0.19, 0.38, 0.18\n",
      "0.20, 0.26, 0.17\n",
      "0.16, 0.21, 0.17\n",
      "0.19, 0.28, 0.16\n",
      "0.21, 0.45, 0.15\n",
      "Data Point 3\n",
      "False\n",
      "-9.75, -32.72, 3.05\n",
      "1.00, 3.33, 1.38\n",
      "-0.02, -1.41, 0.87\n",
      "0.39, 0.76, 0.58\n",
      "0.44, 0.75, 0.45\n",
      "0.26, 0.01, 0.41\n",
      "0.28, 0.41, 0.32\n",
      "-0.48, -1.96, 0.27\n",
      "0.15, 0.58, 0.22\n",
      "0.21, 0.41, 0.22\n",
      "0.52, 0.94, 0.21\n",
      "0.43, 0.65, 0.20\n",
      "-0.11, -0.01, 0.20\n",
      "0.19, 0.24, 0.19\n",
      "-0.52, -1.84, 0.19\n",
      "Data Point 4\n",
      "False\n",
      "-10.06, -31.18, 1.87\n",
      "0.92, 3.30, 1.33\n",
      "-0.31, -1.99, 0.74\n",
      "0.12, 0.05, 0.48\n",
      "0.55, 0.96, 0.30\n",
      "0.08, -0.44, 0.25\n",
      "0.20, 0.29, 0.24\n",
      "0.11, 0.36, 0.23\n",
      "-0.59, -2.19, 0.22\n",
      "0.13, 0.30, 0.22\n",
      "0.23, 0.22, 0.20\n",
      "-0.11, -0.65, 0.20\n",
      "0.27, 0.38, 0.20\n",
      "0.47, 0.64, 0.19\n",
      "-0.26, -1.44, 0.19\n",
      "Data Point 5\n",
      "False\n",
      "-7.05, -23.88, 5.85\n",
      "1.17, 3.56, 1.54\n",
      "-0.61, -2.14, 0.38\n",
      "-0.58, -1.85, 0.29\n",
      "0.46, 0.79, 0.28\n",
      "-0.36, -1.41, 0.27\n",
      "0.44, 0.65, 0.27\n",
      "0.35, 0.47, 0.25\n",
      "0.38, 0.54, 0.24\n",
      "0.11, -0.08, 0.24\n",
      "0.12, 0.14, 0.23\n",
      "0.41, 0.62, 0.23\n",
      "-0.31, -1.31, 0.23\n",
      "-0.50, -1.83, 0.20\n",
      "0.20, 0.05, 0.20\n",
      "Data Point 6\n",
      "False\n",
      "-8.88, -29.72, 2.93\n",
      "0.33, -0.34, 1.39\n",
      "1.02, 3.65, 1.34\n",
      "0.32, 0.41, 0.60\n",
      "-0.08, -1.11, 0.52\n",
      "-0.48, -1.99, 0.48\n",
      "0.10, -0.24, 0.40\n",
      "-0.09, -0.81, 0.35\n",
      "0.55, 1.72, 0.34\n",
      "-4.67, -14.83, 0.32\n",
      "0.08, -0.41, 0.30\n",
      "0.28, 0.73, 0.28\n",
      "0.13, 0.61, 0.25\n",
      "0.34, 0.41, 0.25\n",
      "0.30, 0.47, 0.24\n",
      "Data Point 7\n",
      "False\n",
      "1.71, 4.41, 2.08\n",
      "0.46, 0.54, 0.43\n",
      "0.32, 0.09, 0.41\n",
      "-0.30, -1.78, 0.33\n",
      "0.25, 0.43, 0.32\n",
      "0.22, 0.22, 0.22\n",
      "0.26, 0.29, 0.19\n",
      "0.17, -0.01, 0.19\n",
      "-0.38, -0.23, 0.18\n",
      "0.33, 0.37, 0.17\n",
      "0.47, 0.53, 0.16\n",
      "0.45, 0.57, 0.15\n",
      "0.42, 0.58, 0.15\n",
      "0.46, 0.50, 0.15\n",
      "-0.11, -0.38, 0.14\n",
      "Data Point 8\n",
      "False\n",
      "1.48, 4.10, 1.90\n",
      "-0.61, -2.27, 0.50\n",
      "1.13, 2.01, 0.44\n",
      "0.64, 1.08, 0.34\n",
      "0.39, 0.40, 0.33\n",
      "0.10, 0.66, 0.29\n",
      "0.56, 0.98, 0.27\n",
      "0.41, 0.58, 0.23\n",
      "0.05, -0.20, 0.23\n",
      "0.59, 0.87, 0.22\n",
      "0.03, -0.03, 0.22\n",
      "0.13, 0.28, 0.21\n",
      "0.21, 0.21, 0.21\n",
      "0.35, 0.48, 0.20\n",
      "0.26, 0.27, 0.19\n",
      "Data Point 9\n",
      "False\n",
      "1.68, 3.79, 1.94\n",
      "0.64, 1.62, 0.47\n",
      "0.28, 0.17, 0.37\n",
      "0.26, 0.45, 0.36\n",
      "0.27, 0.34, 0.33\n",
      "0.42, 0.71, 0.31\n",
      "-0.66, -2.50, 0.25\n",
      "0.60, 1.01, 0.25\n",
      "0.55, 1.02, 0.24\n",
      "-0.89, -2.42, 0.24\n",
      "0.43, 0.40, 0.24\n",
      "-0.35, -0.45, 0.23\n",
      "0.31, 0.42, 0.22\n",
      "0.47, 0.80, 0.21\n",
      "0.33, 0.43, 0.20\n",
      "Data Point 10\n",
      "False\n",
      "0.85, 3.38, 1.26\n",
      "-0.03, -1.16, 1.08\n",
      "-0.13, -1.04, 0.59\n",
      "0.40, 0.98, 0.49\n",
      "0.44, 0.69, 0.48\n",
      "-0.21, -1.18, 0.45\n",
      "0.39, 0.16, 0.43\n",
      "0.52, 1.56, 0.39\n",
      "-0.79, -2.72, 0.34\n",
      "0.43, 0.75, 0.33\n",
      "0.72, 0.93, 0.29\n",
      "0.17, 0.46, 0.29\n",
      "0.51, 0.96, 0.28\n",
      "0.58, 0.54, 0.28\n",
      "0.14, -0.03, 0.27\n",
      "Data Point 11\n",
      "False\n",
      "-9.34, -30.71, 3.21\n",
      "-4.19, -13.50, 1.34\n",
      "0.65, 2.99, 1.02\n",
      "0.39, 0.74, 0.63\n",
      "-0.46, -2.25, 0.61\n",
      "0.52, 0.68, 0.54\n",
      "0.28, 0.41, 0.54\n",
      "-0.32, -1.46, 0.53\n",
      "-0.41, -1.81, 0.52\n",
      "-0.50, -1.81, 0.50\n",
      "-0.67, -2.41, 0.48\n",
      "-0.12, -0.70, 0.47\n",
      "-0.26, -1.43, 0.45\n",
      "-0.34, -1.64, 0.43\n",
      "0.05, -0.11, 0.42\n",
      "Data Point 12\n",
      "False\n",
      "1.18, 3.97, 1.63\n",
      "-0.26, -2.18, 0.76\n",
      "0.40, 0.56, 0.50\n",
      "-0.53, -2.17, 0.47\n",
      "0.19, 0.12, 0.30\n",
      "0.65, 1.18, 0.25\n",
      "-0.55, -1.99, 0.25\n",
      "0.08, 0.08, 0.22\n",
      "-0.69, -2.78, 0.22\n",
      "0.49, 0.84, 0.21\n",
      "0.36, 0.44, 0.21\n",
      "0.27, 0.37, 0.17\n",
      "-0.04, -0.15, 0.16\n",
      "0.30, 0.41, 0.16\n",
      "0.11, 0.19, 0.16\n",
      "Data Point 13\n",
      "False\n",
      "1.34, 4.10, 1.63\n",
      "0.40, -0.58, 1.46\n",
      "-13.92, -39.73, 0.65\n",
      "0.55, 0.86, 0.34\n",
      "-0.31, -1.90, 0.31\n",
      "-0.74, -3.15, 0.27\n",
      "0.39, 1.17, 0.23\n",
      "0.47, 0.30, 0.22\n",
      "0.15, 0.20, 0.21\n",
      "0.40, 0.48, 0.21\n",
      "0.39, 0.27, 0.21\n",
      "0.45, 0.58, 0.21\n",
      "0.19, 0.06, 0.20\n",
      "0.09, -0.20, 0.20\n",
      "0.27, 0.27, 0.19\n",
      "Data Point 14\n",
      "False\n",
      "-7.25, -19.81, 5.56\n",
      "0.82, 3.11, 1.28\n",
      "-4.81, -11.78, 1.09\n",
      "-0.04, -1.40, 1.02\n",
      "0.25, 0.17, 0.53\n",
      "-0.51, -1.62, 0.52\n",
      "0.68, 0.45, 0.40\n",
      "0.89, 0.91, 0.39\n",
      "0.78, 0.56, 0.35\n",
      "0.89, 0.95, 0.33\n",
      "-0.19, -1.26, 0.33\n",
      "-0.14, -1.01, 0.32\n",
      "0.38, 0.73, 0.31\n",
      "0.73, 0.91, 0.30\n",
      "0.80, 0.82, 0.28\n",
      "Data Point 15\n",
      "False\n",
      "-10.44, -29.31, 2.68\n",
      "1.29, 3.78, 1.61\n",
      "-0.22, 0.23, 0.41\n",
      "0.61, 0.59, 0.36\n",
      "0.17, -0.11, 0.32\n",
      "0.21, 0.32, 0.29\n",
      "0.58, 0.95, 0.28\n",
      "0.83, 1.23, 0.28\n",
      "0.36, 0.71, 0.27\n",
      "0.55, 0.62, 0.27\n",
      "0.58, 0.80, 0.26\n",
      "0.37, 0.65, 0.23\n",
      "-0.72, -2.31, 0.22\n",
      "0.55, 0.58, 0.21\n",
      "0.58, 0.92, 0.19\n",
      "Data Point 16\n",
      "False\n",
      "-3.86, -16.25, 7.56\n",
      "0.73, 2.81, 1.41\n",
      "-3.64, -11.39, 1.32\n",
      "-0.14, 0.50, 0.49\n",
      "-0.27, -0.99, 0.43\n",
      "0.56, 0.56, 0.39\n",
      "0.10, -0.25, 0.34\n",
      "-0.05, -0.75, 0.34\n",
      "-0.41, -1.61, 0.34\n",
      "-0.09, -0.49, 0.34\n",
      "0.70, 1.06, 0.34\n",
      "0.40, 0.98, 0.33\n",
      "0.89, 1.42, 0.32\n",
      "0.55, 0.95, 0.31\n",
      "0.32, 0.51, 0.31\n",
      "Data Point 17\n",
      "False\n",
      "0.89, 2.80, 1.41\n",
      "-6.89, -22.78, 1.08\n",
      "0.93, 1.45, 0.71\n",
      "-0.13, -1.06, 0.63\n",
      "-0.51, -1.84, 0.63\n",
      "0.83, 1.27, 0.62\n",
      "-0.29, -1.49, 0.53\n",
      "-0.20, -1.11, 0.47\n",
      "0.59, 0.65, 0.47\n",
      "0.02, -0.76, 0.46\n",
      "-0.14, -0.82, 0.40\n",
      "0.47, 0.68, 0.39\n",
      "-0.64, -2.88, 0.38\n",
      "0.39, 0.89, 0.37\n",
      "-0.16, -1.07, 0.37\n",
      "Data Point 18\n",
      "False\n",
      "-4.09, -18.97, 4.17\n",
      "0.59, 0.02, 1.47\n",
      "0.65, 2.71, 1.24\n",
      "0.08, -0.50, 0.73\n",
      "0.18, -0.57, 0.61\n",
      "-0.38, -1.18, 0.57\n",
      "-0.33, -1.75, 0.56\n",
      "-0.26, -1.80, 0.54\n",
      "-0.08, -1.15, 0.51\n",
      "-0.14, -0.92, 0.45\n",
      "-0.25, -1.35, 0.43\n",
      "-0.03, -0.95, 0.41\n",
      "0.33, 0.07, 0.40\n",
      "-0.13, -1.15, 0.40\n",
      "-0.08, -0.87, 0.39\n",
      "Data Point 19\n",
      "False\n",
      "-2.98, -17.91, 8.70\n",
      "-2.74, -11.49, 2.95\n",
      "1.02, 1.60, 1.96\n",
      "0.70, 3.07, 1.07\n",
      "0.43, 0.28, 0.96\n",
      "-0.44, -2.05, 0.68\n",
      "-0.22, -1.47, 0.61\n",
      "-0.23, -1.28, 0.59\n",
      "-0.29, -1.37, 0.58\n",
      "-0.06, -0.78, 0.55\n",
      "0.34, 0.87, 0.54\n",
      "-0.34, -1.71, 0.49\n",
      "0.26, 0.37, 0.49\n",
      "-0.33, -1.61, 0.47\n",
      "-0.64, -2.35, 0.46\n"
     ]
    }
   ],
   "source": [
    "ln_mlp = model.gpt_neox.layers[2].post_attention_layernorm\n",
    "\n",
    "for d_point_ind in range(20):\n",
    "    print(f\"Data Point {d_point_ind}\")\n",
    "    f1 = dictionary_activations_res[d_point_ind].to(device)\n",
    "    f2 = dictionary_activations_skip[d_point_ind].to(device)\n",
    "    f2_hat = combined @ f1\n",
    "    test_f2 = skip_enc@ ln_mlp(res_dec@f1)\n",
    "    print(torch.allclose(test_f2, f2_hat, atol=1e-5))\n",
    "    top_ind = f2.topk(15).indices\n",
    "    for ind in top_ind:\n",
    "        print(f\"{f2_hat[ind]:.2f}, {test_f2[ind]:.2f}, {f2[ind]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([18.9184,  4.0784,  2.8593,  2.7319,  2.5296,  2.4910,  2.4732,  2.4591,\n",
       "         2.2727,  2.2623,  2.2605,  2.0583,  1.8783,  1.8740,  1.8242,  1.8031,\n",
       "         1.7778,  1.7658,  1.7558,  1.7549,  1.7464,  1.7331,  1.7292,  1.7254,\n",
       "         1.6944,  1.6513,  1.6329,  1.6224,  1.6099,  1.6048,  1.5902,  1.5876,\n",
       "         1.5840,  1.5729,  1.5443,  1.5320,  1.5310,  1.5199,  1.5186,  1.5115,\n",
       "         1.4694,  1.4676,  1.4634,  1.4531,  1.4170,  1.3952,  1.3948,  1.3786,\n",
       "         1.3783,  1.3693,  1.3407,  1.3399,  1.3214,  1.2816,  1.2780,  1.2769,\n",
       "         1.2681,  1.2581,  1.2279,  1.2220,  1.2199,  1.2103,  1.1947,  1.1850,\n",
       "         1.1832,  1.1656,  1.1646,  1.1210,  1.1034,  1.1017,  1.0879,  1.0501,\n",
       "         1.0356,  1.0296,  0.9993,  0.9948,  0.9869,  0.9855,  0.9817,  0.9657,\n",
       "         0.9584,  0.9549,  0.9480,  0.9450,  0.9389,  0.9286,  0.9277,  0.9241,\n",
       "         0.9195,  0.9180,  0.9137,  0.9105,  0.9043,  0.8979,  0.8928,  0.8752,\n",
       "         0.8719,  0.8688,  0.8527,  0.8357], device='cuda:0'),\n",
       "indices=tensor([7383, 1445,  174, 3054, 1283, 3313,  736, 1837, 5547,  908, 7238,   35,\n",
       "        7795, 7681, 6838, 6169, 5350, 7216, 4856, 3537, 2737, 4431, 5316, 1913,\n",
       "        6100, 2034, 4369, 4377, 3875, 3851, 5870, 2144, 1979,  310, 1573, 1746,\n",
       "        7441, 3233, 2838, 3736,   47, 5338, 5971, 2420, 1078, 3101, 7329, 1045,\n",
       "        8157, 7774, 2162,  396, 4450, 4013, 1199, 1847, 5152, 1367, 5149, 7432,\n",
       "        1500, 7840, 1108, 5345, 4141,  317, 1397, 6185, 6128, 3455, 3406, 6877,\n",
       "        3724, 4696, 7759,  823, 4117, 4574, 4070,   70, 5221, 2032, 3362, 5476,\n",
       "        2317, 4027, 5366, 3620, 4464, 1515, 2459,  846, 5789, 6144, 5866, 7905,\n",
       "         586, 3555, 3712,  949], device='cuda:0'))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.topk(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the dist of the combined weights\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(combined.flatten().cpu().numpy(), bins=100)\n",
    "# y ax log\n",
    "plt.title(\"Resid-Dec@Skip-Enc weights\")\n",
    "# plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([4.2925, 1.7148, 1.2443, 0.6057, 0.4973, 0.4901, 0.4660, 0.4499, 0.3807,\n",
       "         0.3700], device='cuda:0'),\n",
       " indices=tensor([1477, 5432, 2995, 7856, 7523, 5225, 1493, 5216, 2743, 5933],\n",
       "        device='cuda:0')),\n",
       " torch.return_types.topk(\n",
       " values=tensor([-4.7403, -1.7134, -0.6850, -0.5077, -0.4975, -0.4813, -0.4678, -0.3732,\n",
       "         -0.3674, -0.3619], device='cuda:0'),\n",
       " indices=tensor([1477, 5432, 5225, 2995, 6262, 2743, 7523, 8154, 2448, 3895],\n",
       "        device='cuda:0')))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.max(0).values.topk(10), combined.min(0).values.topk(10, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([125.2061, 105.0183,  25.0771,   8.0769,   8.0200,   7.4735,   7.1577,\n",
       "           7.1127,   6.5055,   6.4784]),\n",
       " indices=tensor([5224, 6097, 7759, 6353, 2645, 7478, 3580, 1000, 7052,  261])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([55.4205, 27.9668,  7.3213,  7.2092,  5.2458,  4.8454,  4.8416,  4.3618,\n",
       "          4.1407,  3.7042]),\n",
       " indices=tensor([1477, 7383, 1445, 5432, 1837, 3313, 3054,  174, 1283, 3736])))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_activations_res[:1000].max(0).values.topk(10), dictionary_activations_skip[:1000].max(0).values.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two random res_dec-shaped matrix, and combine them like above\n",
    "import torch.nn.functional as F\n",
    "res_dec_rand = F.normalize(torch.randn_like(res_dec), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9936, 0.9938, 0.9769,  ..., 0.7565, 0.6615, 0.7212], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = feature_acts\n",
    "x_hat_prime = skip_feature_x_hat\n",
    "# Step 1: Compute the mean of x_hat_prime along the batch dimension\n",
    "x_hat_prime_mean = torch.mean(x_hat_prime, dim=0)\n",
    "\n",
    "# Step 2: Calculate SS_tot (total sum of squares) for each feature\n",
    "ss_tot = torch.sum((x_hat_prime - x_hat_prime_mean) ** 2, dim=-1)\n",
    "\n",
    "# Step 3: Calculate SS_res (residual sum of squares) for each feature\n",
    "ss_res = torch.sum((x_hat_prime - x_hat) ** 2, dim=-1)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "# set nans to 0\n",
    "# r2[torch.isnan(r2)] = 0\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.2240, 0.0000, 0.4828], device='cuda:0'),\n",
       " tensor([1.1337, 0.0000, 0.4043], device='cuda:0'),\n",
       " tensor([0.0737,    nan, 0.1626], device='cuda:0'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_tot[:3], ss_res[:3], r2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature_mse with log x bins\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(feature_mse.numpy(), bins=np.logspace(-5, 0, 50))\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
