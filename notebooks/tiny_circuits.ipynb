{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sae-circuits/circuits2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/tokenization/tokenization.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  neo_tok_ids_to_ts = torch.load(f\"{current_dir}/neo_tok_ids_to_ts.pt\")\n",
      "/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/tokenization/tokenization.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ts_tok_ids_to_neo = torch.load(f\"{current_dir}/ts_tok_ids_to_neo.pt\")\n",
      "/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/lm.py:403: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TinyModel(\n",
       "  (embed): Embedding(10000, 768)\n",
       "  (torso): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (Q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (K): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (V): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (O): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_inp): HookPoint()\n",
       "        (qs): HookPoint()\n",
       "        (ks): HookPoint()\n",
       "        (vs): HookPoint()\n",
       "        (head_writeouts): HookPoint()\n",
       "        (catted_head_writeouts): HookPoint()\n",
       "        (attn_out): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (read_in): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (write_out): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (mlp_inp): HookPoint()\n",
       "        (mlp_out): HookPoint()\n",
       "      )\n",
       "      (res_attn): HookPoint()\n",
       "      (res_mlp): HookPoint()\n",
       "      (res_final): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jannik's models are not SAE Lens, so use Baukit\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# tiny model is in a different direction, add current one to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Get the parent directory of current_dir\n",
    "parent_dir = str(Path(\"/root/GroupedSAEs/notebooks/tiny_circuits.ipynb\").parent.parent)\n",
    "sys.path.append(parent_dir)\n",
    "# from tiny_model.tiny_model import TinyModel, tokenizer\n",
    "# from nnsight import NNsight\n",
    "from tiny_model import TinyModel, tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyModel().to(device)\n",
    "\n",
    "model.to(torch.bfloat16)\n",
    "# nn_model = NNsight(model)\n",
    "# model.nnsight_proxy = nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading noanabeshima/TinyStoriesV2\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:31<00:00, 645.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to tokenize 0 tokens\n",
      "Number of datapoints w/ 101 tokens: 19929\n",
      "Total Tokens: 2.012829M\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "def download_dataset(dataset_name, tokenizer, max_length=256, num_datapoints=None):\n",
    "    if(num_datapoints):\n",
    "        split_text = f\"train[:{num_datapoints}]\"\n",
    "    else:\n",
    "        split_text = \"train\"\n",
    "    dataset = load_dataset(dataset_name, split=split_text)\n",
    "    print(dataset)\n",
    "    total_failed_tokens = 0\n",
    "    all_tokens = []\n",
    "    for text in tqdm(dataset[\"text\"]):\n",
    "        try:\n",
    "            tokens = [9996] + tokenizer.encode(text)[:max_length]\n",
    "        except:\n",
    "            total_failed_tokens += 1\n",
    "            continue\n",
    "        # only include if it's at least max_length\n",
    "        if len(tokens) == max_length+1:\n",
    "            all_tokens.append(tokens)\n",
    "    print(f\"Failed to tokenize {total_failed_tokens} tokens\")\n",
    "    # convert into a dataset class\n",
    "    return  Dataset.from_dict({\"input_ids\": all_tokens})\n",
    "\n",
    "\n",
    "dataset_name = \"noanabeshima/TinyStoriesV2\"\n",
    "max_seq_length = 100\n",
    "num_datapoints = 40000\n",
    "# max_seq_length = 100\n",
    "# num_datapoints = 500\n",
    "print(f\"Downloading {dataset_name}\")\n",
    "dataset = download_dataset(dataset_name, tokenizer=tokenizer, max_length=max_seq_length, num_datapoints=num_datapoints)\n",
    "true_num_datapoints = len(dataset)\n",
    "# added BOS\n",
    "max_seq_length +=1\n",
    "total_tokens = max_seq_length * true_num_datapoints\n",
    "print(f\"Number of datapoints w/ {max_seq_length} tokens: {true_num_datapoints}\")\n",
    "print(f\"Total Tokens: {total_tokens / 1e6}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "#set torch grad to zero globally\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from tiny_model import parse_mlp_tag, SparseMLP\n",
    "N = 1\n",
    "# activation_names = [f\"A{N}\", f\"T{N}\"]\n",
    "# activation_names = [f\"A{N}\", f\"A{N+1}\"]\n",
    "# activation_names = [f\"T{N}\", f\"T{N+1}\"]\n",
    "activation_names = [f\"Rm{N}_S-3_R1_P0\", f\"T{N}\"]\n",
    "file, _, _, _ = parse_mlp_tag(activation_names[0])\n",
    "sae_res = SparseMLP.from_pretrained(file).to(device).to(torch.bfloat16)\n",
    "file, _, _, _ = parse_mlp_tag(activation_names[1])\n",
    "skip_sae = SparseMLP.from_pretrained(file).to(device).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae_res_decoder = sae_res.decoder.weight.data\n",
    "# sae_skip_encoder = skip_sae.encoder.weight.data\n",
    "# sae_res_decoder = sae_res_decoder / sae_res_decoder.norm(dim=0, keepdim=True)\n",
    "# sae_skip_encoder = sae_skip_encoder / sae_skip_encoder.norm(dim=1, keepdim=True)\n",
    "# sae_res_decoder.shape, sae_skip_encoder.shape\n",
    "# cos_sim = sae_res_decoder.T @ sae_skip_encoder.T\n",
    "# max_cos_sim = cos_sim.max(dim=-1).values.cpu().numpy()\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.hist(max_cos_sim, bins=100)\n",
    "# plt.title(f\"Cosine Similarity between {activation_names[0]}-Dec and {activation_names[1]}-Enc\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "with dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip_first_n_pos = 0\n",
    "# # def get_dictionary_activations(model, dataset, cache_name, max_seq_length, autoencoder, batch_size=32):\n",
    "# num_features, d_model  = sae_res.encoder.weight.data.shape\n",
    "# datapoints = dataset.num_rows\n",
    "# dictionary_activations_res = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "# dictionary_activations_skip = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "# all_dictionary_activations = [dictionary_activations_res, dictionary_activations_skip]\n",
    "# token_list = torch.zeros((datapoints*max_seq_length), dtype=torch.int64)\n",
    "\n",
    "\n",
    "# residual_and_skip_correlation = torch.zeros((num_features, num_features), dtype=torch.int32)\n",
    "# residual_correlation = torch.zeros((num_features, num_features), dtype=torch.int32)\n",
    "# skip_correlation = torch.zeros((num_features, num_features), dtype=torch.int32)\n",
    "\n",
    "# feature_mse = torch.zeros(datapoints*max_seq_length)\n",
    "# feature_var_explained = torch.zeros(datapoints*max_seq_length)\n",
    "\n",
    "# with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "#     for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "#         batch = batch.to(model.device)\n",
    "#         token_list[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "#         for sae_ind, sae in enumerate([sae_res, skip_sae]):\n",
    "#             feature_acts = model[activation_names[sae_ind]](batch)\n",
    "#             feature_acts[:, :skip_first_n_pos] = 0\n",
    "#             feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\" )\n",
    "\n",
    "#             all_dictionary_activations[sae_ind][batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = feature_acts.cpu()\n",
    "#             if sae_ind==0:\n",
    "#                 reconstruction_residual = sae.decoder(feature_acts)\n",
    "#                 first_feature_mask = (feature_acts !=0).float()\n",
    "#                 residual_correlation += torch.mm(first_feature_mask.T, first_feature_mask).int().cpu()\n",
    "#             else:\n",
    "#                 second_feature_mask = (feature_acts !=0).float()\n",
    "#                 skip_correlation += torch.mm(second_feature_mask.T, second_feature_mask).int().cpu()\n",
    "\n",
    "#                 # Calculate the correlation between the residual and skip activations\n",
    "#                 residual_and_skip_correlation += torch.mm(first_feature_mask.T, second_feature_mask).int().cpu()\n",
    "\n",
    "\n",
    "#                 # Check the MSE between the skip features & skip-features-w/-residual\n",
    "#                 skip_feature_x_hat = skip_sae.encoder(reconstruction_residual)\n",
    "#                 skip_feature_mse = torch.mean((skip_feature_x_hat - feature_acts)**2, dim=-1)\n",
    "#                 feature_act_var = torch.var(feature_acts)\n",
    "#                 skip_feature_var_explained = 1 - skip_feature_mse / feature_act_var\n",
    "#                 feature_mse[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = skip_feature_mse.cpu()\n",
    "#                 feature_var_explained[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = skip_feature_var_explained.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "100%|██████████| 50/50 [00:41<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "def compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, nonzero_input):\n",
    "    nonzero_input = nonzero_input.detach().requires_grad_(True)\n",
    "    \n",
    "    masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices].detach()\n",
    "    masked_decoder_bias = sae_res.decoder.bias.detach()\n",
    "    masked_encoder_weight = skip_sae.encoder.weight[output_nonzero_indices, :].detach()\n",
    "    masked_encoder_bias = skip_sae.encoder.bias[output_nonzero_indices].detach()\n",
    "    \n",
    "    decoder_output = F.linear(nonzero_input, masked_decoder_weight, masked_decoder_bias)\n",
    "    encoder_output = F.linear(decoder_output, masked_encoder_weight, masked_encoder_bias)\n",
    "    encoder_output = F.relu(encoder_output)\n",
    "    \n",
    "    all_attribution = []\n",
    "    for i in range(encoder_output.shape[1]):\n",
    "        grad = torch.autograd.grad(encoder_output[:, i].sum(), nonzero_input, retain_graph=True)[0]\n",
    "        # all_attribution (grad * nonzero_input).detach()\n",
    "        attri = (grad*nonzero_input).detach()\n",
    "        attri_sum = attri.sum(-1)\n",
    "        normed_attri = (attri / attri_sum[:, None]).nanmean(dim=0)\n",
    "        all_attribution.append(normed_attri)\n",
    "    return torch.stack(all_attribution, dim=1)\n",
    "\n",
    "# def compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length):\n",
    "device = model.device\n",
    "attribution_sum = None\n",
    "sample_count = 0\n",
    "\n",
    "for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature_acts = model[activation_names[0]](batch)\n",
    "        feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\")\n",
    "\n",
    "        # Identify non-zero input elements\n",
    "        input_nonzero_indices = get_nonzero_indices(feature_acts)\n",
    "\n",
    "        # Perform a forward pass to identify non-zero output elements\n",
    "        relu = torch.nn.ReLU()\n",
    "        initial_output = relu(skip_sae.encoder(sae_res.decoder(feature_acts)) + skip_sae.encoder.bias)\n",
    "        output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "    # print(f\"shape of input activations: {feature_acts.shape}\")\n",
    "    # print(f\"shape of initial output activations: {initial_output.shape}\")\n",
    "    # print(f\"number of nonzero input indices: {input_nonzero_indices.shape}\")\n",
    "    # print(f\"number of nonzero output indices: {output_nonzero_indices.shape}\")\n",
    "\n",
    "    # Extract non-zero input values\n",
    "    nonzero_input = feature_acts[:, input_nonzero_indices]\n",
    "\n",
    "    # Compute attribution\n",
    "    # encoder_output, grad, nonzero_input = compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, \n",
    "    # output_nonzero_indices, nonzero_input)\n",
    "    batch_attribution = compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, \n",
    "    output_nonzero_indices, nonzero_input)\n",
    "\n",
    "    # Assertions for shape verification\n",
    "    # assert batch_attribution.shape == input_nonzero_indices.shape, f\"batch_attribution shape {batch_attribution.shape} doesn't match input_nonzero_indices shape {input_nonzero_indices.shape}\"\n",
    "\n",
    "    # Update attribution sum\n",
    "    if attribution_sum is None:\n",
    "        attribution_sum = torch.zeros(sae_res.decoder.weight.shape[1], skip_sae.encoder.weight.shape[0], device=\"cpu\")\n",
    "        attribution_count = torch.zeros(sae_res.decoder.weight.shape[1], skip_sae.encoder.weight.shape[0], device=\"cpu\")\n",
    "    \n",
    "    # Vectorized approach to add batch_attribution to the correct positions\n",
    "    # output_indices = output_nonzero_indices.cpu().unsqueeze(1).expand(-1, len(input_nonzero_indices))\n",
    "    # input_indices = input_nonzero_indices.cpu().unsqueeze(0).expand(len(output_nonzero_indices), -1)\n",
    "    # attribution_sum[input_nonzero_indices[:, None].cpu(), output_nonzero_indices.cpu()] += batch_attribution.cpu()\n",
    "    # attribution_count[input_nonzero_indices[:, None].cpu(), output_nonzero_indices.cpu()] += 1\n",
    "    # Create a mask for non-zero entries in batch_attribution\n",
    "    non_zero_mask = batch_attribution != 0\n",
    "\n",
    "    # Use this mask to update attribution_sum and attribution_count\n",
    "    attribution_sum[input_nonzero_indices[:, None].cpu(), output_nonzero_indices.cpu()] += batch_attribution.cpu() * non_zero_mask.cpu()\n",
    "    attribution_count[input_nonzero_indices[:, None].cpu(), output_nonzero_indices.cpu()] += non_zero_mask.cpu().to(torch.int)\n",
    "\n",
    "    # sample_count += nonzero_input.shape[0]\n",
    "\n",
    "    # Print out memory usage\n",
    "    # print(f\" Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Normalize the attribution sum by the total number of samples\n",
    "# average_attribution = attribution_sum / sample_count\n",
    "# average_attribution = attribution_sum / attribution_count\n",
    "average_attribution = torch.nan_to_num(attribution_sum / attribution_count, nan=0.0)\n",
    "#TODO: Only divide by nonzero elements.\n",
    "\n",
    "\n",
    "# Usage\n",
    "# average_attribution = compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\n",
    "\n",
    "# print(\"Average Attribution shape:\", average_attribution.shape)\n",
    "# print(\"Average Attribution (first few elements):\\n\", average_attribution[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save attribution\n",
    "torch.save(average_attribution, f\"{activation_names[0]}_{activation_names[1]}_average_attribution.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_attribution = torch.nan_to_num(attribution_sum / attribution_count, nan=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.8145, 0.1130, 0.0169, 0.0161, 0.0139, 0.0074, 0.0058, 0.0057, 0.0041,\n",
       "        0.0038]),\n",
       "indices=tensor([   12,   120,   418,   324,  1506,  1237, 12037,    46,  2945,   113]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_attribution[:, 0].topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9971)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_attribution.isnan().sum() / average_attribution.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-1.8463e-03,  0.0000e+00,  4.7913e-03,  ...,  2.1606e-02,\n",
       "          -3.7670e-05,  1.9165e-02],\n",
       "         [-3.0518e-02,  0.0000e+00,  1.3672e-02,  ..., -3.0151e-02,\n",
       "          -9.4604e-04,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00, -1.6168e-06,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[        nan,         nan, -2.0041e-07,  ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [-1.7041e-03,         nan,  4.2609e-03,  ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [-1.7772e-02,         nan,  4.0830e-03,  ...,         nan,\n",
       "                  nan,         nan],\n",
       "         ...,\n",
       "         [        nan,         nan,         nan,  ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [        nan,         nan,         nan,  ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [        nan,         nan,         nan,  ...,         nan,\n",
       "                  nan,         nan]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_attribution, average_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00, -8.0164e-07,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-8.5205e-02,  0.0000e+00,  2.1304e-01,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-8.8860e-01,  0.0000e+00,  2.0415e-01,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]),\n",
       " tensor([[43., 43., 43.,  ...,  0.,  0.,  0.],\n",
       "         [50., 50., 50.,  ...,  0.,  0.,  0.],\n",
       "         [50., 50., 50.,  ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution_sum, attribution_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix after vectorized assignment:\n",
      "tensor([[0., 1., 0., 2., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 3., 0., 4., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Setup\n",
    "full_matrix = torch.zeros(3, 5)\n",
    "output_nonzero_indices = torch.tensor([0, 2])\n",
    "input_nonzero_indices = torch.tensor([1, 3])\n",
    "batch_attribution = torch.tensor([[1.0, 2.0],\n",
    "                                  [3.0, 4.0]])\n",
    "\n",
    "# Vectorized assignment\n",
    "full_matrix[output_nonzero_indices[:, None], input_nonzero_indices] = batch_attribution\n",
    "\n",
    "print(\"Matrix after vectorized assignment:\")\n",
    "print(full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [    2,     2,     2,  ...,     2,     2,     2],\n",
       "         ...,\n",
       "         [15401, 15401, 15401,  ..., 15401, 15401, 15401],\n",
       "         [16814, 16814, 16814,  ..., 16814, 16814, 16814],\n",
       "         [18383, 18383, 18383,  ..., 18383, 18383, 18383]]),\n",
       " tensor([[    0,     1,     2,  ..., 21295, 21299, 21313],\n",
       "         [    0,     1,     2,  ..., 21295, 21299, 21313],\n",
       "         [    0,     1,     2,  ..., 21295, 21299, 21313],\n",
       "         ...,\n",
       "         [    0,     1,     2,  ..., 21295, 21299, 21313],\n",
       "         [    0,     1,     2,  ..., 21295, 21299, 21313],\n",
       "         [    0,     1,     2,  ..., 21295, 21299, 21313]]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_indices, input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -1.0326,   0.0000,  -0.0539,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ -0.5197,   0.0000,   1.2892,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [-11.3419,   0.0000,   2.8043,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]),\n",
       " tensor([[665., 665., 665.,  ...,   0.,   0.,   0.],\n",
       "         [665., 665., 665.,  ...,   0.,   0.,   0.],\n",
       "         [665., 665., 665.,  ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution_sum, attribution_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan,  ..., nan, nan, nan])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vals = average_attribution.max(-1).values\n",
    "max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0134)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_attribution.count_nonzero() / average_attribution.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summed = average_attribution.sum(dim=0).numpy()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(summed, bins=100)\n",
    "plt.title(\"Sum of Attribution Weights per Feature\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_attribution[average_attribution.isnan()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9961, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(1.0859, device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor(-0.0320, device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape, grad.shape, nonzero_input.shape\n",
    "attri = (grad*nonzero_input).detach()\n",
    "attri_sum = attri.sum(-1)\n",
    "normed_attri = attri / attri_sum[:, None]\n",
    "normed_attri[74].sum(), normed_attri[74].max(), normed_attri[74].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1515, 25000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3555, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [8.3750, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [9.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [9.6250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attribution = []\n",
    "for i in range(encoder_output.shape[1]):\n",
    "    grad = torch.autograd.grad(encoder_output[:, i].sum(), nonzero_input, retain_graph=True)[0]\n",
    "    # all_attribution (grad * nonzero_input).detach()\n",
    "    attri = (grad*nonzero_input).detach()\n",
    "    attri_sum = attri.sum(-1)\n",
    "    normed_attri = (attri / attri_sum[:, None]).nanmean(dim=0)\n",
    "    all_attribution.append(normed_attri)\n",
    "stacked_attr = torch.stack(all_attribution, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1515, 7588]),\n",
       " 895,\n",
       " tensor([0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
       "        dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_input.shape, encoder_output.shape[1], normed_attri.nanmean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stacked_attr[1].sum()\n",
    "v = stacked_attr[7]\n",
    "# plot as hist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(v.cpu().float().numpy(), bins=100)\n",
    "plt.title(f\"Attribution Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grad, attri, attri_sum\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1515, 895, 7588])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/665 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  0%|          | 1/665 [00:02<26:57,  2.44s/it]/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "100%|██████████| 665/665 [09:42<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Attribution shape: torch.Size([25000, 25000])\n",
      "Average Attribution (first few elements):\n",
      " tensor([[-0.0288,  0.0914,  0.2016,  0.0237,  0.0517],\n",
      "        [-0.0288,  0.0914,  0.2016,  0.0237,  0.0517],\n",
      "        [-0.0288,  0.0914,  0.2016,  0.0237,  0.0517],\n",
      "        [-0.0280,  0.0892,  0.2017,  0.0228,  0.0499],\n",
      "        [-0.0288,  0.0914,  0.2016,  0.0237,  0.0517]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "def compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, nonzero_input):\n",
    "    nonzero_input = nonzero_input.detach().requires_grad_(True)\n",
    "    \n",
    "    masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices].detach()\n",
    "    masked_decoder_bias = sae_res.decoder.bias.detach()\n",
    "    masked_encoder_weight = skip_sae.encoder.weight[output_nonzero_indices, :].detach()\n",
    "    masked_encoder_bias = skip_sae.encoder.bias[output_nonzero_indices].detach()\n",
    "    \n",
    "    decoder_output = F.linear(nonzero_input, masked_decoder_weight, masked_decoder_bias)\n",
    "    encoder_output = F.linear(decoder_output, masked_encoder_weight, masked_encoder_bias)\n",
    "    encoder_output = F.relu(encoder_output)\n",
    "    \n",
    "    attribution_sum = torch.zeros_like(nonzero_input)\n",
    "    \n",
    "    for i in range(encoder_output.shape[1]):\n",
    "        grad = torch.autograd.grad(encoder_output[:, i].sum(), nonzero_input, retain_graph=True)[0]\n",
    "        attribution_sum += (grad * nonzero_input).detach()\n",
    "    \n",
    "    return attribution_sum.sum(dim=0)  # Sum over batch dimension\n",
    "\n",
    "def compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length):\n",
    "    device = model.device\n",
    "    attribution_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            feature_acts = model[activation_names[0]](batch)\n",
    "            feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\")\n",
    "\n",
    "            # Identify non-zero input elements\n",
    "            input_nonzero_indices = get_nonzero_indices(feature_acts)\n",
    "\n",
    "            # Perform a forward pass to identify non-zero output elements\n",
    "            relu = torch.nn.ReLU()\n",
    "            initial_output = relu(skip_sae.encoder(sae_res.decoder(feature_acts)) + skip_sae.encoder.bias)\n",
    "            output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "        # print(f\"shape of input activations: {feature_acts.shape}\")\n",
    "        # print(f\"shape of initial output activations: {initial_output.shape}\")\n",
    "        # print(f\"number of nonzero input indices: {input_nonzero_indices.shape}\")\n",
    "        # print(f\"number of nonzero output indices: {output_nonzero_indices.shape}\")\n",
    "\n",
    "        # Extract non-zero input values\n",
    "        nonzero_input = feature_acts[:, input_nonzero_indices]\n",
    "\n",
    "        # Compute attribution\n",
    "        batch_attribution = compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, nonzero_input)\n",
    "\n",
    "        # Assertions for shape verification\n",
    "        assert batch_attribution.shape == input_nonzero_indices.shape, f\"batch_attribution shape {batch_attribution.shape} doesn't match input_nonzero_indices shape {input_nonzero_indices.shape}\"\n",
    "\n",
    "        # Update attribution sum\n",
    "        if attribution_sum is None:\n",
    "            attribution_sum = torch.zeros(skip_sae.encoder.weight.shape[0], sae_res.decoder.weight.shape[1], device=\"cpu\")\n",
    "            attribution_count = torch.zeros(skip_sae.encoder.weight.shape[0], sae_res.decoder.weight.shape[1], device=\"cpu\")\n",
    "        \n",
    "        # Vectorized approach to add batch_attribution to the correct positions\n",
    "        output_indices = output_nonzero_indices.cpu().unsqueeze(1).expand(-1, len(input_nonzero_indices))\n",
    "        input_indices = input_nonzero_indices.cpu().unsqueeze(0).expand(len(output_nonzero_indices), -1)\n",
    "        attribution_sum[output_indices, input_indices] += batch_attribution.cpu()\n",
    "        attribution_count[output_indices, input_indices] += 1\n",
    "\n",
    "        # sample_count += nonzero_input.shape[0]\n",
    "\n",
    "        # Print out memory usage\n",
    "        # print(f\" Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Normalize the attribution sum by the total number of samples\n",
    "    # average_attribution = attribution_sum / sample_count\n",
    "    average_attribution = attribution_sum / attribution_count\n",
    "    #TODO: Only divide by nonzero elements.\n",
    "\n",
    "    return average_attribution\n",
    "\n",
    "# Usage\n",
    "average_attribution = compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\n",
    "\n",
    "print(\"Average Attribution shape:\", average_attribution.shape)\n",
    "print(\"Average Attribution (first few elements):\\n\", average_attribution[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "torch.save(average_attribution, f\"{activation_names[0]}_{activation_names[1]}_average_attribution.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5049, 0.4469, 0.4379, 0.3791, 0.3745, 0.3062, 0.2586, 0.2399, 0.2297,\n",
       "        0.2177])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = average_attribution[0].abs().topk(10).indices\n",
    "average_attribution[0, ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y:\n",
      "tensor([[8., 3.],\n",
      "        [7., 0.],\n",
      "        [0., 3.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Raw attribution:\n",
      "tensor([[[2., 6., 0.],\n",
      "         [0., 0., 3.]],\n",
      "\n",
      "        [[3., 4., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 3.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]], grad_fn=<StackBackward0>)\n",
      "\n",
      "Normalized attribution:\n",
      "tensor([[[0.2500, 0.7500, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000]],\n",
      "\n",
      "        [[0.4286, 0.5714, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Summed normalized attribution:\n",
      "tensor([[0.3393, 0.6607, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5] at index 0 does not match the shape of the indexed tensor [2, 3] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Calculate mean of non-zero outputs\u001b[39;00m\n\u001b[1;32m     45\u001b[0m non_zero_mask \u001b[38;5;241m=\u001b[39m (y\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m epsilon)\n\u001b[0;32m---> 46\u001b[0m mean_non_zero_attribution \u001b[38;5;241m=\u001b[39m \u001b[43msummed_normalized_attribution\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_zero_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMean of non-zero summed normalized attribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_non_zero_attribution)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [5] at index 0 does not match the shape of the indexed tensor [2, 3] at index 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define our simple linear transformation (without transpose)\n",
    "weight = torch.tensor([[1.0, 2.0, 0.0],\n",
    "                       [0.0, 0.0, 3.0]])\n",
    "# bias = torch.tensor([0.5, 1.0])\n",
    "\n",
    "# Input as a batch\n",
    "x = torch.tensor([[2.0, 3.0, 1.0],\n",
    "                  [3.0, 2.0, 0.0],\n",
    "                  [0.0, 0.0, 1.0],\n",
    "                  [0.0, 0.0, 0.0], \n",
    "                  [0.0, 0.0, 0.0]], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = F.linear(x, weight)\n",
    "print(\"Output y:\")\n",
    "print(y)\n",
    "\n",
    "# Calculate attribution\n",
    "attribution_list = []\n",
    "for i in range(y.shape[1]):  # Iterate over output dimensions\n",
    "    grad = torch.autograd.grad(y[:, i].sum(), x, retain_graph=True)[0]\n",
    "    attr = grad * x\n",
    "    attribution_list.append(attr)\n",
    "\n",
    "attribution = torch.stack(attribution_list, dim=1)\n",
    "print(\"\\nRaw attribution:\")\n",
    "print(attribution)\n",
    "\n",
    "# Normalize attribution\n",
    "# Add a small epsilon to avoid division by zero\n",
    "epsilon = 1e-10\n",
    "normalized_attribution = attribution / (y.unsqueeze(-1) + epsilon)\n",
    "print(\"\\nNormalized attribution:\")\n",
    "print(normalized_attribution)\n",
    "\n",
    "# Sum normalized attribution across output dimensions\n",
    "summed_normalized_attribution = normalized_attribution.sum(dim=0)/2\n",
    "print(\"\\nSummed normalized attribution:\")\n",
    "print(summed_normalized_attribution)\n",
    "\n",
    "# Calculate mean of non-zero outputs\n",
    "non_zero_mask = (y.abs().sum(dim=1) > epsilon)\n",
    "mean_non_zero_attribution = summed_normalized_attribution[non_zero_mask].mean(dim=0)\n",
    "print(\"\\nMean of non-zero summed normalized attribution:\")\n",
    "print(mean_non_zero_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3393, 0.6607, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(attribution_list, dim=1)[:3]/ y[:3].unsqueeze(-1)).nanmean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 |  tensor(0)\n",
      "0.1 |  tensor(14683)\n",
      "0.010000000000000002 |  tensor(300834)\n",
      "0.0010000000000000002 |  tensor(2131137)\n",
      "0.00010000000000000002 |  tensor(11495318)\n",
      "1.0000000000000003e-05 |  tensor(39015914)\n",
      "1.0000000000000004e-06 |  tensor(67084179)\n",
      "1.0000000000000004e-07 |  tensor(81507033)\n",
      "1.0000000000000005e-08 |  tensor(87559738)\n",
      "1.0000000000000005e-09 |  tensor(89375789)\n"
     ]
    }
   ],
   "source": [
    "# average_attribution\n",
    "for i in range(0,10):\n",
    "    threshold = 0.1**i\n",
    "    print(threshold, \"| \", (average_attribution.abs() > threshold).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "  2%|▏         | 1/50 [00:03<02:45,  3.37s/it]/root/sae-circuits/circuits2/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "100%|██████████| 50/50 [01:15<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Attribution shape: torch.Size([25000, 25000])\n",
      "Average Attribution (first few elements):\n",
      " tensor([[ 0.0000e+00, -7.8852e-04,  0.0000e+00,  0.0000e+00, -8.0679e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.1500e-06, -3.6827e-01,  2.7705e-02,  1.1033e-02, -4.3740e-03],\n",
      "        [ 0.0000e+00, -3.0780e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Attribution patching w/ zero ablation\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "\n",
    "def compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, interpolated_input, original_input, original_output):\n",
    "    interpolated_input.requires_grad_(True)\n",
    "    \n",
    "    masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices]\n",
    "    masked_decoder_bias = sae_res.decoder.bias\n",
    "    masked_encoder_weight = skip_sae.encoder.weight[output_nonzero_indices, :]\n",
    "    masked_encoder_bias = skip_sae.encoder.bias[output_nonzero_indices]\n",
    "    \n",
    "    decoder_output = F.linear(interpolated_input, masked_decoder_weight, masked_decoder_bias)\n",
    "    encoder_output = F.linear(decoder_output, masked_encoder_weight, masked_encoder_bias)\n",
    "    encoder_output = F.relu(encoder_output)\n",
    "\n",
    "    output_difference = encoder_output - original_output\n",
    "    input_difference = -1*original_input\n",
    "    \n",
    "    attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=interpolated_input.device)\n",
    "    for i in range(len(output_nonzero_indices)):\n",
    "        grad = torch.autograd.grad(output_difference[:, i].sum(), interpolated_input, retain_graph=True)[0]\n",
    "        attribution[i] = (grad * input_difference).sum(dim=0).detach()\n",
    "\n",
    "    return attribution\n",
    "\n",
    "def compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length):\n",
    "    device = model.device\n",
    "    attribution_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        feature_acts = model[activation_names[0]](batch)\n",
    "        feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\")\n",
    "\n",
    "        # Identify non-zero input elements\n",
    "        input_nonzero_indices = get_nonzero_indices(feature_acts)\n",
    "\n",
    "        # Perform a forward pass to identify non-zero output elements\n",
    "        with torch.no_grad():\n",
    "            relu = torch.nn.ReLU()\n",
    "            initial_output = relu(skip_sae.encoder(sae_res.decoder(feature_acts)) + skip_sae.encoder.bias)\n",
    "        output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "        # print(f\"shape of input activations: {feature_acts.shape}\")\n",
    "        # print(f\"shape of initial output activations: {initial_output.shape}\")\n",
    "        # print(f\"number of nonzero input indices: {input_nonzero_indices.shape}\")\n",
    "        # print(f\"number of nonzero output indices: {output_nonzero_indices.shape}\")\n",
    "        # print(\"input-output shape: \", input_nonzero_indices.shape, output_nonzero_indices.shape)\n",
    "\n",
    "        # Extract non-zero input values\n",
    "        nonzero_input = feature_acts[:, input_nonzero_indices]\n",
    "        nonzero_output = initial_output[:, output_nonzero_indices]\n",
    "        \n",
    "        # Integrated Gradients with Zero Ablation\n",
    "        batch_attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=device)\n",
    "        \n",
    "        for alpha in np.linspace(0, 1, 7):\n",
    "            interpolated_input = alpha * nonzero_input\n",
    "            \n",
    "            # Compute attribution for interpolated input\n",
    "            interp_attribution = compute_efficient_attribution(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices, interpolated_input, nonzero_input, nonzero_output)\n",
    "            \n",
    "            batch_attribution += interp_attribution\n",
    "        \n",
    "        # Average the attribution\n",
    "        batch_attribution /= 7\n",
    "\n",
    "        # Assertions for shape verification\n",
    "        assert batch_attribution.shape == (len(output_nonzero_indices), len(input_nonzero_indices)), f\"batch_attribution shape {batch_attribution.shape} doesn't match expected shape {(len(output_nonzero_indices), len(input_nonzero_indices))}\"\n",
    "\n",
    "        # Update attribution sum\n",
    "        if attribution_sum is None:\n",
    "            attribution_sum = torch.zeros(skip_sae.encoder.weight.shape[0], sae_res.decoder.weight.shape[1], device=\"cpu\")\n",
    "        \n",
    "        output_indices = output_nonzero_indices.cpu().unsqueeze(1).expand(-1, len(input_nonzero_indices))\n",
    "        input_indices = input_nonzero_indices.cpu().unsqueeze(0).expand(len(output_nonzero_indices), -1)\n",
    "        attribution_sum[output_indices, input_indices] += batch_attribution.detach().cpu()\n",
    "\n",
    "        sample_count += nonzero_input.shape[0]\n",
    "\n",
    "        # Print out memory usage\n",
    "        # print(f\" Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Normalize the attribution sum by the total number of samples\n",
    "    average_attribution = attribution_sum / sample_count\n",
    "\n",
    "    return average_attribution\n",
    "\n",
    "# Usage\n",
    "average_attribution = compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\n",
    "\n",
    "print(\"Average Attribution shape:\", average_attribution.shape)\n",
    "print(\"Average Attribution (first few elements):\\n\", average_attribution[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the attribution\n",
    "torch.save(average_attribution, \"average_attribution.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans(output):  tensor(931)\n",
      "torch.return_types.topk(\n",
      "values=tensor([9.8407e-04, 9.6671e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([8, 3, 7, 4, 9, 5, 1, 0, 2, 6]))\n",
      "Trans(output):  tensor(351)\n",
      "torch.return_types.topk(\n",
      "values=tensor([8.4457e-04, 2.2192e-04, 1.0401e-04, 5.4360e-05, 4.9184e-05, 4.4101e-05,\n",
      "        4.0298e-05, 3.7457e-05, 3.5323e-05, 3.4636e-05]),\n",
      "indices=tensor([ 129,  128,   25, 1246,  205,  169,  362, 1319, 2630,  323]))\n",
      "Trans(output):  tensor(2405)\n",
      "torch.return_types.topk(\n",
      "values=tensor([8.1942e-04, 1.9559e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([ 8, 54,  7,  4,  9,  1,  0,  5, 10,  2]))\n",
      "Trans(output):  tensor(435)\n",
      "torch.return_types.topk(\n",
      "values=tensor([7.6914e-04, 3.1301e-04, 1.4089e-04, 9.4971e-05, 7.5060e-05, 6.2596e-05,\n",
      "        6.1760e-05, 5.8894e-05, 5.7006e-05, 4.8569e-05]),\n",
      "indices=tensor([   8,  129, 2123,  205,  231, 1122,   54,    4,   74,  567]))\n",
      "Trans(output):  tensor(9)\n",
      "torch.return_types.topk(\n",
      "values=tensor([7.5109e-04, 3.8000e-04, 4.2549e-05, 4.1265e-05, 4.0173e-05, 3.7417e-05,\n",
      "        3.1787e-05, 2.8534e-05, 2.1469e-05, 1.8959e-05]),\n",
      "indices=tensor([ 129,  128,   86, 2630,  352,  631,  408, 1673, 2055,  892]))\n",
      "Trans(output):  tensor(2499)\n",
      "torch.return_types.topk(\n",
      "values=tensor([6.5336e-04, 3.8121e-05, 1.0340e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([  8,  54, 523,   7,   1,   4,   9,   0,   5,  10]))\n",
      "Trans(output):  tensor(236)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.5312e-02, 6.4689e-04, 1.0558e-04, 9.3847e-05, 5.2910e-05, 5.2086e-05,\n",
      "        4.8013e-05, 4.3869e-05, 4.1156e-05, 3.8020e-05]),\n",
      "indices=tensor([   3,  129,   20,    7, 1673, 2892,  205,    0,   23, 1246]))\n",
      "Trans(output):  tensor(900)\n",
      "torch.return_types.topk(\n",
      "values=tensor([3.9611e-03, 5.8316e-04, 9.9738e-05, 6.1951e-05, 3.7634e-05, 1.9746e-05,\n",
      "        1.7707e-05, 1.7665e-05, 1.2520e-05, 1.2479e-05]),\n",
      "indices=tensor([   3,    6,    7,  188,   20,   54, 2123,  205,  415,  944]))\n",
      "Trans(output):  tensor(51)\n",
      "torch.return_types.topk(\n",
      "values=tensor([5.6554e-04, 1.7628e-04, 1.7627e-04, 1.5321e-04, 1.3386e-04, 1.1902e-04,\n",
      "        1.1311e-04, 8.9511e-05, 8.7832e-05, 7.4521e-05]),\n",
      "indices=tensor([ 129,   25,   23, 2055,  169,   28,  128,  267, 1673,  896]))\n",
      "Trans(output):  tensor(3543)\n",
      "torch.return_types.topk(\n",
      "values=tensor([5.5777e-04, 5.9393e-05, 2.1380e-05, 2.1118e-06, 6.8660e-07, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([   8,   54,  231,   74, 1007,    1,    7,    0,    4,    9]))\n",
      "Trans(output):  tensor(537)\n",
      "torch.return_types.topk(\n",
      "values=tensor([5.2363e-04, 1.0278e-04, 9.5263e-05, 8.6687e-05, 2.2868e-05, 1.0650e-05,\n",
      "        9.3263e-06, 8.6104e-06, 6.6010e-06, 6.5268e-06]),\n",
      "indices=tensor([   6,   11,  129,    1,    4, 1993,  254, 1398,  864, 2892]))\n",
      "Trans(output):  tensor(626)\n",
      "torch.return_types.topk(\n",
      "values=tensor([3.7903e-03, 5.2333e-04, 7.2311e-05, 3.2536e-05, 2.8192e-05, 2.6175e-05,\n",
      "        2.4828e-05, 1.9134e-05, 1.7544e-05, 1.5868e-05]),\n",
      "indices=tensor([   3,  129,    8,   25,   74,  169,  352,   20, 2994,  122]))\n",
      "Trans(output):  tensor(14)\n",
      "torch.return_types.topk(\n",
      "values=tensor([1.0729e-02, 4.8631e-04, 1.4410e-04, 1.2351e-04, 9.8570e-05, 6.8639e-05,\n",
      "        5.9007e-05, 5.1911e-05, 4.8197e-05, 4.5847e-05]),\n",
      "indices=tensor([   3,    8,  205,  129,  202,  862,  106, 1506, 1748,   54]))\n",
      "Trans(output):  tensor(1965)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.8039e-04, 1.7274e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([8, 3, 7, 4, 9, 5, 1, 0, 2, 6]))\n",
      "Trans(output):  tensor(1428)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.6359e-04, 4.0825e-05, 1.3592e-08, 7.3088e-09, 6.2830e-09, 3.6972e-09,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([   8,    3, 2346, 2432, 1122, 2952,    7,    1,    4,    9]))\n",
      "Trans(output):  tensor(2622)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.4330e-04, 3.5520e-05, 3.1157e-06, 7.0438e-08, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([  8,   3, 231, 545,   7,   4,   9,   1,   5,   0]))\n",
      "Trans(output):  tensor(995)\n",
      "torch.return_types.topk(\n",
      "values=tensor([3.3593e-03, 4.2389e-04, 3.4778e-05, 2.3092e-05, 1.9446e-05, 1.9378e-05,\n",
      "        1.6156e-05, 1.5433e-05, 1.4264e-05, 1.1860e-05]),\n",
      "indices=tensor([   3,    8, 1923, 2123,  415,  188, 2926,  267,  106, 1607]))\n",
      "Trans(output):  tensor(474)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.2251e-04, 1.0082e-04, 5.2830e-05, 5.2361e-05, 4.5214e-05, 4.1624e-05,\n",
      "        3.5686e-05, 3.5627e-05, 3.2719e-05, 3.0352e-05]),\n",
      "indices=tensor([ 129,    7,  567,  128,  801,  267,  106,  297, 2117, 2865]))\n",
      "Trans(output):  tensor(2883)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.0911e-04, 6.0918e-05, 8.4690e-06, 8.3919e-06, 3.1299e-06, 2.4194e-06,\n",
      "        1.7917e-07, 1.4908e-07, 7.0096e-08, 4.5135e-08]),\n",
      "indices=tensor([   8,   54,  231,    3, 1007,  944, 1008, 9939, 1398,  695]))\n",
      "Trans(output):  tensor(3337)\n",
      "torch.return_types.topk(\n",
      "values=tensor([4.0686e-04, 7.7468e-06, 6.4625e-08, 2.0772e-08, 1.5216e-08, 5.7968e-09,\n",
      "        5.3320e-09, 1.7431e-10, 0.0000e+00, 0.0000e+00]),\n",
      "indices=tensor([   8,   54, 1607,  695, 1649,  188, 1952, 1007,    1,    7]))\n"
     ]
    }
   ],
   "source": [
    "top_ind = average_attribution[4:].max(dim=0).values.topk(20).indices\n",
    "for t_i in top_ind:\n",
    "    print(\"Trans(output): \", t_i)\n",
    "    print(average_attribution[:, t_i].topk(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot as hist\n",
    "import matplotlib.pyplot as plt\n",
    "nz_att = (average_attribution.abs() > 0).sum(dim=0).cpu().numpy()\n",
    "plt.hist(nz_att, bins=np.logspace(0, 3, 100))\n",
    "plt.title(\"Number of non-zero attributions per feature\")\n",
    "# x on log scale\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save average_attribution\n",
    "torch.save(average_attribution, \"average_attribution.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([2.2425e-01, 2.1201e-04, 1.1401e-05, 5.1755e-06, 1.8601e-06, 1.0901e-06,\n",
       "         7.3858e-07, 4.8418e-07, 4.7597e-07, 1.8464e-07]),\n",
       " indices=tensor([    6,     1,     4, 12227,  3353,     8, 11682,  6176, 17684,  1607])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([0.2242, 0.0011, 0.0007, 0.0007, 0.0006, 0.0006, 0.0005, 0.0004, 0.0004,\n",
       "         0.0004]),\n",
       " indices=tensor([  38, 1392, 1246,  768,  661,   34, 1180,  490, 1350,  854])))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_attribution[:, 38].topk(10), average_attribution[6].topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[6, 2]}, size=[6]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Run the toy example\u001b[39;00m\n\u001b[1;32m     90\u001b[0m model \u001b[38;5;241m=\u001b[39m ToyModel()\n\u001b[0;32m---> 91\u001b[0m average_attribution \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_toy_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoy_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Attribution shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_attribution\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Attribution:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, average_attribution)\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mcompute_toy_jacobian\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     70\u001b[0m     interpolated_input \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m batch\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Compute attribution for interpolated input\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     interp_attribution \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficient_attribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_nonzero_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_nonzero_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolated_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     batch_attribution \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m interp_attribution\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Average the attribution\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m, in \u001b[0;36mcompute_efficient_attribution\u001b[0;34m(model, input_nonzero_indices, output_nonzero_indices, interpolated_input)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_nonzero_indices)):\n\u001b[1;32m     47\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(encoder_output[:, i]\u001b[38;5;241m.\u001b[39msum(), interpolated_input, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mattribution\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (grad \u001b[38;5;241m*\u001b[39m interpolated_input)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[input_nonzero_indices]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribution\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[6, 2]}, size=[6]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Toy model\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(3, 4)\n",
    "        self.encoder = nn.Linear(4, 2)\n",
    "        \n",
    "        # Set known weights for predictability\n",
    "        self.decoder.weight.data = torch.tensor([[1., 2., 3.],\n",
    "                                                 [4., 5., 6.],\n",
    "                                                 [7., 8., 9.],\n",
    "                                                 [10., 11., 12.]])\n",
    "        self.decoder.bias.data = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "        \n",
    "        self.encoder.weight.data = torch.tensor([[1., 2., 3., 4.],\n",
    "                                                 [5., 6., 7., 8.]])\n",
    "        self.encoder.bias.data = torch.tensor([0.5, 0.6])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = torch.tensor([[1., 0., 1.],\n",
    "                         [0., 1., 1.],\n",
    "                         [1., 1., 0.]])\n",
    "\n",
    "# Simplified version of our functions\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).nonzero().squeeze(1)\n",
    "\n",
    "def compute_efficient_attribution(model, input_nonzero_indices, output_nonzero_indices, interpolated_input):\n",
    "    interpolated_input.requires_grad_(True)\n",
    "    \n",
    "    decoder_output = model.decoder(interpolated_input)\n",
    "    encoder_output = model.encoder(F.relu(decoder_output))\n",
    "    \n",
    "    attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=interpolated_input.device)\n",
    "    for i in range(len(output_nonzero_indices)):\n",
    "        grad = torch.autograd.grad(encoder_output[:, i].sum(), interpolated_input, retain_graph=True)[0]\n",
    "        attribution[i] = (grad * interpolated_input).sum(dim=0)[input_nonzero_indices]\n",
    "    \n",
    "    return attribution\n",
    "def compute_toy_jacobian(model, data):\n",
    "    device = next(model.parameters()).device\n",
    "    attribution_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    batch = data.to(device)  # Process all data at once\n",
    "    \n",
    "    # Identify non-zero input elements\n",
    "    input_nonzero_indices = get_nonzero_indices(batch)\n",
    "\n",
    "    # Perform a forward pass to identify non-zero output elements\n",
    "    with torch.no_grad():\n",
    "        initial_output = model(batch)\n",
    "    output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "    # Integrated Gradients with Zero Ablation\n",
    "    batch_attribution = torch.zeros(len(output_nonzero_indices), len(input_nonzero_indices), device=device)\n",
    "    \n",
    "    for alpha in np.linspace(0, 1, 7):\n",
    "        interpolated_input = alpha * batch\n",
    "        \n",
    "        # Compute attribution for interpolated input\n",
    "        interp_attribution = compute_efficient_attribution(model, input_nonzero_indices, output_nonzero_indices, interpolated_input)\n",
    "        \n",
    "        batch_attribution += interp_attribution\n",
    "    \n",
    "    # Average the attribution\n",
    "    batch_attribution /= 7\n",
    "\n",
    "    # Update attribution sum\n",
    "    attribution_sum = torch.zeros(model.encoder.weight.shape[0], model.decoder.weight.shape[1], device=device)\n",
    "    attribution_sum[output_nonzero_indices, input_nonzero_indices] = batch_attribution\n",
    "\n",
    "    # Normalize the attribution sum by the total number of samples\n",
    "    average_attribution = attribution_sum / batch.shape[0]\n",
    "\n",
    "    return average_attribution\n",
    "\n",
    "# Run the toy example\n",
    "model = ToyModel()\n",
    "average_attribution = compute_toy_jacobian(model, toy_data)\n",
    "\n",
    "print(\"Average Attribution shape:\", average_attribution.shape)\n",
    "print(\"Average Attribution:\\n\", average_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/root/GroupedSAEs/groups/lib/python3.11/site-packages/tiny_model/sparse_mlp.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input activations: torch.Size([816, 25000])\n",
      "shape of initial output activations: torch.Size([816, 25000])\n",
      "number of nonzero input indices: torch.Size([100])\n",
      "number of nonzero output indices: torch.Size([100])\n",
      "torch.Size([100]) torch.Size([100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.81 GiB. GPU 0 has a total capacity of 47.54 GiB of which 16.14 GiB is free. Process 3126992 has 31.39 GiB memory in use. Of the allocated memory 28.33 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m average_jacobian\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m average_jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_efficient_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_sae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Jacobian shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_jacobian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Jacobian (first few elements):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, average_jacobian[:\u001b[38;5;241m5\u001b[39m, :\u001b[38;5;241m5\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 72\u001b[0m, in \u001b[0;36mcompute_efficient_jacobian\u001b[0;34m(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\u001b[0m\n\u001b[1;32m     68\u001b[0m nonzero_input \u001b[38;5;241m=\u001b[39m feature_acts[:, input_nonzero_indices]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Compute Jacobian\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# return input_nonzero_indices, nonzero_input\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonzero_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Update Jacobian sum\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jacobian_sum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/autograd/functional.py:817\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    814\u001b[0m                 jac_i_el\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mzeros_like(inp_el))\n\u001b[1;32m    816\u001b[0m     jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 817\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    818\u001b[0m             torch\u001b[38;5;241m.\u001b[39mstack(jac_i_el, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    819\u001b[0m                 out\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m+\u001b[39m inputs[el_idx]\u001b[38;5;241m.\u001b[39msize()  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (el_idx, jac_i_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jac_i)\n\u001b[1;32m    822\u001b[0m         ),\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    825\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m _grad_postprocess(jacobian, create_graph)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(jacobian, (is_outputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/autograd/functional.py:818\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    814\u001b[0m                 jac_i_el\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mzeros_like(inp_el))\n\u001b[1;32m    816\u001b[0m     jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 818\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjac_i_el\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    819\u001b[0m                 out\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m+\u001b[39m inputs[el_idx]\u001b[38;5;241m.\u001b[39msize()  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (el_idx, jac_i_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jac_i)\n\u001b[1;32m    822\u001b[0m         ),\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    825\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m _grad_postprocess(jacobian, create_graph)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tuple_postprocess(jacobian, (is_outputs_tuple, is_inputs_tuple))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.81 GiB. GPU 0 has a total capacity of 47.54 GiB of which 16.14 GiB is free. Process 3126992 has 31.39 GiB memory in use. Of the allocated memory 28.33 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_nonzero_indices(tensor, threshold=1e-6):\n",
    "    return (tensor.abs() > threshold).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "def create_masked_sae_to_sae_function(sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices):\n",
    "    # set input_nonzero_indices to the first N indices\n",
    "    masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices]\n",
    "    masked_decoder_bias = sae_res.decoder.bias\n",
    "\n",
    "    masked_encoder_weight = skip_sae.encoder.weight[output_nonzero_indices, :]\n",
    "    masked_encoder_bias = skip_sae.encoder.bias[output_nonzero_indices]\n",
    "\n",
    "    def masked_sae_to_sae_function(nonzero_input):\n",
    "        decoder_output = F.linear(nonzero_input, masked_decoder_weight, masked_decoder_bias)\n",
    "        # TODO: Add layernorm for GPT-2 if needed\n",
    "        encoder_output = F.linear(decoder_output, masked_encoder_weight, masked_encoder_bias)\n",
    "        # add ReLU\n",
    "        encoder_output = F.relu(encoder_output)\n",
    "        return encoder_output\n",
    "\n",
    "    return masked_sae_to_sae_function\n",
    "\n",
    "def compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length):\n",
    "    device = model.device\n",
    "    jacobian_sum = None\n",
    "    sample_count = 0\n",
    "\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        feature_acts = model[activation_names[0]](batch)\n",
    "        feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\")\n",
    "\n",
    "        # return feature_acts\n",
    "\n",
    "        # Identify non-zero input elements\n",
    "        input_nonzero_indices = get_nonzero_indices(feature_acts)\n",
    "\n",
    "        # Perform a forward pass to identify non-zero output elements\n",
    "        with torch.no_grad():\n",
    "            # initial_output = skip_sae.encoder(sae_res.decoder(feature_acts))\n",
    "            # add bias and ReLU\n",
    "            relu = torch.nn.ReLU()\n",
    "            initial_output = relu(skip_sae.encoder(sae_res.decoder(feature_acts)) + skip_sae.encoder.bias)\n",
    "        output_nonzero_indices = get_nonzero_indices(initial_output)\n",
    "\n",
    "        N = 100\n",
    "        input_nonzero_indices = torch.arange(N)\n",
    "        output_nonzero_indices = torch.arange(N)\n",
    "        # return initial_output\n",
    "        print(f\"shape of input activations: {feature_acts.shape}\")\n",
    "        print(f\"shape of initial output activations: {initial_output.shape}\")\n",
    "        print(f\"number of nonzero input indices: {input_nonzero_indices.shape}\")\n",
    "        print(f\"number of nonzero output indices: {output_nonzero_indices.shape}\")\n",
    "\n",
    "        print(output_nonzero_indices.shape, input_nonzero_indices.shape)\n",
    "\n",
    "        # Create the masked function\n",
    "        masked_function = create_masked_sae_to_sae_function(\n",
    "            sae_res, skip_sae, input_nonzero_indices, output_nonzero_indices\n",
    "        )\n",
    "\n",
    "        # Extract non-zero input values\n",
    "        nonzero_input = feature_acts[:, input_nonzero_indices]\n",
    "\n",
    "        # Compute Jacobian\n",
    "        # return input_nonzero_indices, nonzero_input\n",
    "        jacobian = torch.autograd.functional.jacobian(masked_function, nonzero_input)\n",
    "\n",
    "        # Update Jacobian sum\n",
    "        if jacobian_sum is None:\n",
    "            jacobian_sum = torch.zeros(skip_sae.encoder.weight.shape[0], sae_res.decoder.weight.shape[1], device=\"cpu\")\n",
    "        \n",
    "        jacobian_sum[output_nonzero_indices.unsqueeze(1).cpu(), input_nonzero_indices.cpu()] += jacobian.sum(dim=0).cpu()\n",
    "        sample_count += jacobian.shape[0]\n",
    "        # print out memory\n",
    "        print(f\" Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Normalize the Jacobian sum by the total number of samples\n",
    "    average_jacobian = jacobian_sum / sample_count.cpu()\n",
    "\n",
    "    return average_jacobian\n",
    "\n",
    "# Usage\n",
    "average_jacobian = compute_efficient_jacobian(model, sae_res, skip_sae, dl, activation_names, batch_size, max_seq_length)\n",
    "\n",
    "print(\"Average Jacobian shape:\", average_jacobian.shape)\n",
    "print(\"Average Jacobian (first few elements):\\n\", average_jacobian[:5, :5])\n",
    "print(\"Token list shape:\", token_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1632, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_nonzero_indices = average_jacobian\n",
    "masked_decoder_weight = sae_res.decoder.weight[:, input_nonzero_indices]\n",
    "masked_decoder_bias = sae_res.decoder.bias\n",
    "F.linear(nz_inp, masked_decoder_weight, masked_decoder_bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 4\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msae_to_sae_function\u001b[39m(feature_act):\n\u001b[1;32m      7\u001b[0m     res_act \u001b[38;5;241m=\u001b[39m sae_res\u001b[38;5;241m.\u001b[39mdecoder(feature_act)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "\n",
    "def sae_to_sae_function(feature_act):\n",
    "    res_act = sae_res.decoder(feature_act)\n",
    "    #TODO: for gpt2, add layernorm\n",
    "    return skip_sae.encoder(res_act)\n",
    "\n",
    "for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "    batch = batch.to(model.device)\n",
    "    token_list[batch_ind*batch_size*max_seq_length:(batch_ind+1)*batch_size*max_seq_length] = rearrange(batch, \"b s -> (b s)\")\n",
    "    feature_acts = model[activation_names[0]](batch)\n",
    "    feature_acts = rearrange(feature_acts, \"b s n -> (b s) n\" )\n",
    "\n",
    "    # calculate Jacobian, sum over batch dimension\n",
    "    # jacobian = torch.autograd.functional.jacobian(sae_to_sae_function, feature_acts)\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Layer L0: 31, Second Layer L0: 0\n",
      "First Layer features_per_pos: tensor([ 14.,  76., 101.,  37.,  71.,  46., 110.,  62.,  31., 133., 137., 115.,\n",
      "         97., 193., 199., 129.,  66.,  87., 126.,  83.]) \n",
      " Second Layer features_per_pos: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[BEGIN]Once upon a time, there was a reliable otter named Ollie. He lived in'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L0_first = (dictionary_activations_res[:10000] !=0).float().sum(-1).mean().item()\n",
    "first_N_pos = (dictionary_activations_res[:20] !=0).float().sum(-1)\n",
    "L0_sec = (dictionary_activations_skip[:10000] !=0).float().sum(-1).mean()\n",
    "sec_N_pos = (dictionary_activations_skip[:20] !=0).float().sum(-1)\n",
    "print(f\"First Layer L0: {L0_first:.0f}, Second Layer L0: {L0_sec:.0f}\")\n",
    "print(f\"First Layer features_per_pos: {first_N_pos} \\n Second Layer features_per_pos: {sec_N_pos}\")\n",
    "tokenizer.decode(dataset[0][\"input_ids\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('nnsight').setLevel(logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.backends.cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:25<01:15, 25.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad shape torch.Size([32, 51, 25000])\n",
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:49<02:27, 49.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad shape torch.Size([32, 51, 25000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, saved_grad\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m clean_upstream\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dl)\n\u001b[1;32m     68\u001b[0m us_attrib \u001b[38;5;241m=\u001b[39m us_attrib\u001b[38;5;241m/\u001b[39mnum_batches\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def get_upstream_attrs(model, upstream_tags, target_tag, target_feat_idx, batch_size=6, train_size=3000):\n",
    "'''\n",
    "Get the attribution of features earlier in the network.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "upstream_attribs = {}\n",
    "upstream_tags = [activation_names[0]]\n",
    "target_tag = activation_names[1]\n",
    "# for upstream_tag, upstream_mlp in model.get_upstream(target_tag).items():\n",
    "for upstream_tag in upstream_tags:\n",
    "    nn_model.wipe_sparse()\n",
    "    nn_model.register_sparse([upstream_tag, target_tag])\n",
    "    upstream_mlp = nn_model.sparse_mlps[upstream_tag]\n",
    "    target = nn_model.sparse_mlps[target_tag].act\n",
    "    # upstream_mlp = sae_res\n",
    "    # target = skip_sae\n",
    "    \n",
    "    \n",
    "    num_us_feats = upstream_mlp.encoder.weight.shape[0]\n",
    "    upstream = upstream_mlp.act\n",
    "\n",
    "    us_attrib = torch.zeros((num_us_feats, num_us_feats))\n",
    "\n",
    "    # for batch_idx in tqdm(range(0, doc_ids.shape[0]-batch_size, batch_size), desc=f'Getting {upstream_tag} attribution'):\n",
    "    for batch_ind, batch in enumerate(tqdm(dl)):\n",
    "        batch = batch.to(nn_model.device)\n",
    "        # clean_ids, noise_ids = batch_ids[...,0], batch_ids[...,1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with nn_model.trace(batch) as tracer:\n",
    "                clean_upstream = upstream.output.save()\n",
    "                clean_target = target.output.save()\n",
    "            clean_upstream = clean_upstream.detach()\n",
    "        \n",
    "        batch_attrib = torch.zeros_like(us_attrib).to(nn_model.device)\n",
    "        clean_upstream.requires_grad = True\n",
    "        noise_upstream = torch.zeros_like(clean_upstream)\n",
    "        for alpha in np.linspace(0, 1, 2):\n",
    "            print(alpha)\n",
    "            with nn_model.trace(batch) as tracer:\n",
    "                upstream.output = clean_upstream + alpha*(noise_upstream - clean_upstream).detach()\n",
    "                # upstream_grad = upstream.output.grad.save()\n",
    "                saved_og_loss = ((clean_target.detach() - target.output)**2).sum(dim=[0,1]).save()\n",
    "                # saved_og_loss.backward()\n",
    "                # Compute gradients with respect to the upstream output\n",
    "                saved_og_loss = ((clean_target.detach() - target.output[:,:,target_feat_idx])**2).sum(dim=[0,1]).save()\n",
    "\n",
    "                saved_grad = upstream_grad.save()\n",
    "\n",
    "                sample_attrib = (upstream_grad*(noise_upstream - clean_upstream).detach()).sum(dim=[0,1]).save()\n",
    "                # saved_og_loss = ((clean_target.detach() - target.output)**2).sum().save()\n",
    "                # sample_attrib = ((clean_upstream.detach()).sum(dim=[0,1])\n",
    "                # sample_attrib = upstream.output.save()\n",
    "            batch_attrib += sample_attrib\n",
    "            \n",
    "        us_attrib += batch_attrib.detach().cpu()\n",
    "        print(\"grad shape\", saved_grad.shape)\n",
    "        del clean_upstream\n",
    "        torch.cuda.empty_cache()\n",
    "    num_batches = len(dl)\n",
    "    us_attrib = us_attrib/num_batches\n",
    "    upstream_attribs[upstream_tag] = us_attrib.clone().cpu()\n",
    "\n",
    "\n",
    "nn_model.wipe_sparse()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# attrs = get_upstream_attrs(model, upstream_tags=['A0', 'M0', 'A1', 'M1'], target_tag='A2', target_feat_idx=3049, train_size=1_000, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero outputs: [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute gradients for all non-zero outputs simultaneously\u001b[39;00m\n\u001b[1;32m     42\u001b[0m selected_outputs \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m, non_zero_outputs[:, \u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 43\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_zero_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, jacobian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, jacobian)\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/autograd/functional.py:674\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    671\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 674\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[1;32m    676\u001b[0m     outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute gradients for all non-zero outputs simultaneously\u001b[39;00m\n\u001b[1;32m     42\u001b[0m selected_outputs \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m0\u001b[39m, non_zero_outputs[:, \u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 43\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, non_zero_outputs[:, \u001b[38;5;241m1\u001b[39m]], sparse_values)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, jacobian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJacobian:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, jacobian)\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mSimpleNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GroupedSAEs/groups/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 10x10)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "input_size = 10\n",
    "hidden_size = 10\n",
    "output_size = 5\n",
    "net = SimpleNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Create sparse input tensor\n",
    "non_zero_inputs = [1, 4, 7]  # Indices of non-zero inputs\n",
    "sparse_values = torch.randn(len(non_zero_inputs), requires_grad=True)\n",
    "x = torch.zeros(1, input_size)\n",
    "x[0, non_zero_inputs] = sparse_values\n",
    "\n",
    "# Forward pass\n",
    "y_pred = net(x)\n",
    "\n",
    "# Identify non-zero outputs\n",
    "non_zero_outputs = torch.nonzero(y_pred.abs() > 1e-6).squeeze()\n",
    "if non_zero_outputs.dim() == 0:  # Handle case when there's only one non-zero output\n",
    "    non_zero_outputs = non_zero_outputs.unsqueeze(0)\n",
    "\n",
    "print(\"Non-zero outputs:\", non_zero_outputs.tolist())\n",
    "\n",
    "# Compute gradients for all non-zero outputs simultaneously\n",
    "selected_outputs = y_pred[0, non_zero_outputs[:, 1]]\n",
    "jacobian = torch.autograd.functional.jacobian(lambda x: net(x.unsqueeze(0))[0, non_zero_outputs[:, 1]], sparse_values)\n",
    "\n",
    "print(\"Jacobian shape:\", jacobian.shape)\n",
    "print(\"Jacobian:\\n\", jacobian)\n",
    "\n",
    "# Interpret the results\n",
    "print(\"\\nGradient interpretation:\")\n",
    "for i, (_, output_idx) in enumerate(non_zero_outputs):\n",
    "    print(f\"Output {output_idx + 1}:\")\n",
    "    for j, input_idx in enumerate(non_zero_inputs):\n",
    "        print(f\"  ∂y{output_idx + 1}/∂x{input_idx + 1} = {jacobian[i, j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    }
   ],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total res = first_only + cross_connection - intersection\n",
      "1197 = 938 + 411 - 152\n",
      "Total skip = first_only + cross_connection - intersection\n",
      "1193 = 762 + 580 - 149\n"
     ]
    }
   ],
   "source": [
    "# I need a specific set of information for each node.\n",
    "'''\n",
    "0. Remove features that activate < 10 times\n",
    "1. Each input's jaccard sim\n",
    "2. Each output's jaccard sim\n",
    "3. Eachl input's jaccard sim to each output\n",
    "3.2 Weight similarity of input-to-output\n",
    "4. Each nz_features top-5 common tokens\n",
    "'''\n",
    "\n",
    "def jaccard_similarity(co_occurrences, total_occurrences_1, total_occurrences_2):\n",
    "    # Union of occurrences\n",
    "    union_occurrences = total_occurrences_1 + total_occurrences_2.T - co_occurrences\n",
    "    # Jaccard similarity\n",
    "    jacc_sim = co_occurrences / union_occurrences\n",
    "    return jacc_sim\n",
    "\n",
    "\n",
    "\n",
    "# Remove features that activate < 10 times\n",
    "activation_threshold = 10\n",
    "frequency_threshold = 0.1\n",
    "# cross_connection = \"correlation\"\n",
    "cross_connection = \"cos_sim\"\n",
    "\n",
    "# Residual features\n",
    "resid_diag = residual_correlation.diag()\n",
    "nz_res_features = resid_diag >  activation_threshold\n",
    "# Also remove high frequency features\n",
    "res_freq = resid_diag / total_tokens\n",
    "nz_res_features = nz_res_features & (res_freq < frequency_threshold)\n",
    "nz_res_corr = residual_correlation[nz_res_features][:, nz_res_features]\n",
    "\n",
    "# Repeat for skip\n",
    "skip_diag = skip_correlation.diag()\n",
    "nz_skip_features = skip_diag > activation_threshold\n",
    "skip_freq = skip_diag / total_tokens\n",
    "nz_skip_features = nz_skip_features & (skip_freq < frequency_threshold)\n",
    "nz_skip_corr = skip_correlation[nz_skip_features][:, nz_skip_features]\n",
    "\n",
    "# Repeat for residual to skip\n",
    "# switch statement for cross_connection\n",
    "if cross_connection == \"correlation\":\n",
    "    nz_res_skip_corr = residual_and_skip_correlation[nz_res_features][:, nz_skip_features]\n",
    "elif cross_connection == \"cos_sim\":\n",
    "    nz_res_skip_corr = cos_sim[nz_res_features][:, nz_skip_features].cpu()\n",
    "\n",
    "# Calculate the jaccard similarity between each feature\n",
    "total_occur_res = resid_diag[nz_res_features][:, None]\n",
    "total_occur_skip = skip_diag[nz_skip_features][:, None]\n",
    "jacc_sim_res_dim0_original = jaccard_similarity(total_occurrences_1 = total_occur_res, total_occurrences_2 = total_occur_res, co_occurrences = nz_res_corr)\n",
    "jacc_sim_skip_dim0_original = jaccard_similarity(total_occurrences_1 = total_occur_skip, total_occurrences_2 = total_occur_skip, co_occurrences = nz_skip_corr)\n",
    "if(cross_connection == \"correlation\"):\n",
    "    jacc_sim_res_to_skip_original = jaccard_similarity(total_occurrences_1 = total_occur_res, total_occurrences_2 = total_occur_skip, co_occurrences = nz_res_skip_corr)\n",
    "elif(cross_connection == \"cos_sim\"):\n",
    "    jacc_sim_res_to_skip_original = nz_res_skip_corr\n",
    "\n",
    "# save the jaccard diag\n",
    "# save the diagonals\n",
    "jacc_res_diag = jacc_sim_res_dim0_original.diag()\n",
    "jacc_skip_diag = jacc_sim_skip_dim0_original.diag()\n",
    "\n",
    "# set the diagonal to zero\n",
    "jacc_sim_res_dim0_original.fill_diagonal_(0)\n",
    "jacc_sim_skip_dim0_original.fill_diagonal_(0)\n",
    "\n",
    "# Find index of features above a correlation threshold\n",
    "correlation_threshold = 0.55\n",
    "cos_sim_threshold = 0.45\n",
    "\n",
    "#TODO: set threshold to be a quantile (or a max number of nodes)\n",
    "# Set upper triangle to lower triangle\n",
    "res_index_above_corr = (jacc_sim_res_dim0_original > correlation_threshold).sum(0).nonzero()[:, 0]\n",
    "skip_index_above_corr = (jacc_sim_skip_dim0_original > correlation_threshold).sum(0).nonzero()[:, 0]\n",
    "extra_res_nodes = (jacc_sim_res_to_skip_original > cos_sim_threshold).sum(1).nonzero()[:, 0]\n",
    "extra_skip_nodes = (jacc_sim_res_to_skip_original > cos_sim_threshold).sum(0).nonzero()[:, 0]\n",
    "\n",
    "# index into the above features\n",
    "jacc_sim_res_dim0 = jacc_sim_res_dim0_original[res_index_above_corr][:, res_index_above_corr]\n",
    "jacc_sim_skip_dim0 = jacc_sim_skip_dim0_original[skip_index_above_corr][:, skip_index_above_corr]\n",
    "jacc_sim_res_to_skip = jacc_sim_res_to_skip_original[extra_res_nodes][:, extra_skip_nodes]\n",
    "\n",
    "nz_res_ind = nz_res_features.nonzero()[:, 0]\n",
    "combined_res_ind = torch.tensor(list(set(res_index_above_corr.tolist() + extra_res_nodes.tolist())))\n",
    "global_id_res_group = nz_res_ind[combined_res_ind]\n",
    "res_only_global_id = nz_res_ind[res_index_above_corr]\n",
    "cross_connection_res_global_id = nz_res_ind[extra_res_nodes.cpu()]\n",
    "\n",
    "# skip\n",
    "nz_skip_ind = nz_skip_features.nonzero()[:, 0]\n",
    "combined_skip_ind = torch.tensor(list(set(skip_index_above_corr.tolist() + extra_skip_nodes.tolist())))\n",
    "global_id_skip_group = nz_skip_ind[combined_skip_ind]\n",
    "skip_only_global_id = nz_skip_ind[skip_index_above_corr]\n",
    "cross_connection_skip_global_id = nz_skip_ind[extra_skip_nodes.cpu()]\n",
    "\n",
    "# Logging\n",
    "num_of_res_intersection = len(set(res_index_above_corr.tolist()).intersection(set(extra_res_nodes.tolist())))\n",
    "num_of_skip_intersection = len(set(skip_index_above_corr.tolist()).intersection(set(extra_skip_nodes.tolist())))\n",
    "print(f\"Total res = first_only + cross_connection - intersection\")\n",
    "print(f\"{len(global_id_res_group)} = {len(res_only_global_id)} + {len(cross_connection_res_global_id)} - {num_of_res_intersection}\")\n",
    "print(f\"Total skip = first_only + cross_connection - intersection\")\n",
    "print(f\"{len(global_id_skip_group)} = {len(skip_only_global_id)} + {len(cross_connection_skip_global_id)} - {num_of_skip_intersection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot res_freq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# do log bins\n",
    "# plt.hist(res_freq, bins=np.logspace(np.log10(1e-5),np.log10(1e-0), 50))\n",
    "plt.hist(skip_freq, bins=np.logspace(np.log10(1e-6),np.log10(1e-0), 50))\n",
    "plt.xlabel(\"Frequency of Activation\")\n",
    "plt.ylabel(\"Number of Features\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total zeroed out: 0\n",
      " Total zeroed out: 0\n",
      " Total zeroed out: 4\n",
      "Res Connections: 2184\n",
      "Skip Connections: 1720\n",
      "Cross Connections: 619\n"
     ]
    }
   ],
   "source": [
    "# TODO change display corre threshold\n",
    "# TODO: check if triu-> tril fixes things\n",
    "# alt, just allow duplicates (by erasing diagonal?)\n",
    "def get_connection_weights(jacc_sim, global_id_x, global_id_y, display_correlation_threshold=0.4, max_connections=10):\n",
    "    # Remove all connections that have more than max_conn (they clog up display)\n",
    "    above_threshold = jacc_sim > display_correlation_threshold\n",
    "    # (above_threshold.sum(0) > max_num).nonzero(), above_threshold.sum(1) > max_num\n",
    "    too_high_rows = (above_threshold.sum(1) > max_connections).nonzero()[:, 0]\n",
    "    too_high_cols = (above_threshold.sum(0) > max_connections).nonzero()[:, 0]\n",
    "    print(f\" Total zeroed out: {len(too_high_rows) + len(too_high_cols)}\")\n",
    "    # Set those to zero \n",
    "    jacc_sim[too_high_rows] = 0\n",
    "    jacc_sim[:, too_high_cols] = 0\n",
    "    row, col = (jacc_sim > display_correlation_threshold).nonzero().T\n",
    "    weight = jacc_sim[row, col]\n",
    "    row = global_id_x[row]\n",
    "    col = global_id_y[col]\n",
    "    # if(len(row) > max_length):\n",
    "    #     print(f\"Too many connections, truncating to {max_length}\")\n",
    "    #     row = row[:max_length]\n",
    "    #     col = col[:max_length]\n",
    "    #     weight = weight[:max_length]\n",
    "    return [[row[i].item(), col[i].item(), weight[i].item()] for i in range(len(row))]\n",
    "\n",
    "res_connections = get_connection_weights(jacc_sim_res_dim0, res_only_global_id, res_only_global_id)\n",
    "skip_connections = get_connection_weights(jacc_sim_skip_dim0, skip_only_global_id, skip_only_global_id)\n",
    "cross_connections = get_connection_weights(jacc_sim_res_to_skip, cross_connection_res_global_id, cross_connection_skip_global_id, display_correlation_threshold= 0.25)\n",
    "print(f\"Res Connections: {len(res_connections)}\")\n",
    "print(f\"Skip Connections: {len(skip_connections)}\")\n",
    "print(f\"Cross Connections: {len(cross_connections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1197/1197 [00:20<00:00, 57.00it/s]\n",
      "100%|██████████| 1193/1193 [00:19<00:00, 59.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# nz_dict_act_res is size (tokens, features)\n",
    "\n",
    "# For each feature, get the top-5 tokens by occurance\n",
    "def get_top_tokens(dictionary_activations, token_list, tokenizer):\n",
    "    feature_labels = []\n",
    "    feature_hovertips = []\n",
    "    for feature in tqdm(dictionary_activations.T):\n",
    "        nz_feature_ind = feature != 0\n",
    "        total_nz = nz_feature_ind.sum()\n",
    "        specific_tokens = token_list[nz_feature_ind].numpy()\n",
    "        counter = Counter(specific_tokens).most_common(5)\n",
    "        hovertip_str = \"\"\n",
    "        for token_loop_ind, (token, count) in enumerate(counter):\n",
    "            token_str = tokenizer.decode(token.item())\n",
    "            percentage = count / total_nz\n",
    "            token_str = token_str.replace(\"\\n\", \"\\\\n\").replace(\" \", \"_\")\n",
    "            # token_str = token_str.replace(\"Ġ\", \"_\")\n",
    "            hovertip_str += f\"{token_str}: {percentage:.0%}\"\n",
    "            if(token_loop_ind == 0):\n",
    "                feature_labels.append(token_str)\n",
    "        feature_hovertips.append(hovertip_str)\n",
    "    # replace \\n w/ \\\\newline\n",
    "    # feature_hovertips = [hovertip.replace(\"\\n\", \"\\\\n\") for hovertip in feature_hovertips]\n",
    "    # feature_labels = [label.replace(\"\\n\", \"\\\\n\") for label in feature_labels]\n",
    "    # replace 'Ġ' w/ _\n",
    "    # feature_hovertips = [hovertip.replace(\"Ġ\", \"_\") for hovertip in feature_hovertips]\n",
    "    # feature_labels = [label.replace(\"Ġ\", \"_\") for label in feature_labels]\n",
    "\n",
    "\n",
    "    return feature_labels, feature_hovertips\n",
    "\n",
    "# Get the top-5 tokens for each feature\n",
    "# token_list, dictionary_activations_res, dictionary_activations_skip\n",
    "nz_dict_act_res = dictionary_activations_res[:, global_id_res_group].numpy()\n",
    "nz_dict_act_skip = dictionary_activations_skip[:, global_id_skip_group].numpy()\n",
    "\n",
    "res_feature_labels, res_feature_hovertips = get_top_tokens(nz_dict_act_res, token_list, tokenizer)\n",
    "skip_feature_labels, skip_feature_hovertips = get_top_tokens(nz_dict_act_skip, token_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Generate HTML content with correct variable substitution\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        body, html {{\n",
    "            margin: 0;\n",
    "            height: 100%;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .tooltip {{\n",
    "            position: absolute;\n",
    "            text-align: center;\n",
    "            width: auto;\n",
    "            height: auto;\n",
    "            padding: 5px;\n",
    "            font: 12px sans-serif;\n",
    "            background: lightsteelblue;\n",
    "            border: 0px;\n",
    "            border-radius: 8px;\n",
    "            pointer-events: none;\n",
    "            opacity: 0;\n",
    "            transition: opacity 0.3s;\n",
    "        }}\n",
    "        .zoomable {{\n",
    "            cursor: grab;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"100%\" height=\"100%\" class=\"zoomable\"></svg>\n",
    "    <script>\n",
    "        const resFeatureLabels = {res_feature_labels};\n",
    "        const skipFeatureLabels = {skip_feature_labels};\n",
    "\n",
    "        const resFeatureHovertips = {res_feature_hovertips};\n",
    "        const skipFeatureHovertips = {skip_feature_hovertips};\n",
    "\n",
    "        const resGlobalID = {res_global_id};\n",
    "        const skipGlobalID = {skip_global_id};\n",
    "\n",
    "        const res_connections = {res_connections};\n",
    "        const skip_connections = {skip_connections};\n",
    "        const cross_connections = {cross_connections};\n",
    "\n",
    "        const nodes = [];\n",
    "\n",
    "        resGlobalID.forEach((global_id, i) => {{\n",
    "            const full_id_name = `res_${{global_id}}`;\n",
    "            nodes.push({{ id: full_id_name, group: 'res', color: 'blue', defaultLabel: resFeatureLabels[i], hoverLabel: resFeatureHovertips[i]+ full_id_name}});\n",
    "        }});\n",
    "\n",
    "        skipGlobalID.forEach((global_id, i) => {{\n",
    "            const full_id_name = `skip_${{global_id}}`;\n",
    "            nodes.push({{ id: full_id_name, group: 'skip', color: 'red', defaultLabel: skipFeatureLabels[i], hoverLabel: skipFeatureHovertips[i]+full_id_name }});\n",
    "        }});\n",
    "\n",
    "\n",
    "        const links = [];\n",
    "        res_connections.forEach(([res_ind_x, res_ind_y, weight]) => {{\n",
    "            const source = `res_${{res_ind_x}}`;\n",
    "            const target = `res_${{res_ind_y}}`;\n",
    "            links.push({{ source: source, target: target, value: weight, color: 'blue' }});\n",
    "        }});\n",
    "\n",
    "        skip_connections.forEach(([skip_ind_x, skip_ind_y, weight]) => {{\n",
    "            const source = `skip_${{skip_ind_x}}`;\n",
    "            const target = `skip_${{skip_ind_y}}`;\n",
    "            links.push({{ source: source, target: target, value: weight, color: 'red' }});\n",
    "        }});\n",
    "\n",
    "        cross_connections.forEach(([res_ind_x, skip_ind_y, weight]) => {{\n",
    "            const source = `res_${{res_ind_x}}`;\n",
    "            const target = `skip_${{skip_ind_y}}`;\n",
    "            links.push({{ source: source, target: target, value: weight, color: 'purple' }});\n",
    "        }});\n",
    "\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = window.innerWidth,\n",
    "            height = window.innerHeight;\n",
    "\n",
    "        const tooltip = d3.select(\"body\").append(\"div\")\n",
    "            .attr(\"class\", \"tooltip\");\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(1))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-30))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2))\n",
    "            .force(\"x\", d3.forceX(width / 2).strength(0.09))\n",
    "            .force(\"y\", d3.forceY(height / 2).strength(0.09));\n",
    "\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(Math.abs(d.value)))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 5)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", (event, d) => {{\n",
    "                tooltip.transition().duration(100).style(\"opacity\", 0.9);\n",
    "                tooltip.html(d.hoverLabel)\n",
    "                    .style(\"left\", (event.pageX + 5) + \"px\")\n",
    "                    .style(\"top\", (event.pageY - 28) + \"px\");\n",
    "            }})\n",
    "            .on(\"mouseout\", () => {{\n",
    "                tooltip.transition().duration(350).style(\"opacity\", 0);\n",
    "            }});\n",
    "\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        svg.call(d3.zoom().on(\"zoom\", (event) => {{\n",
    "            svg.attr(\"transform\", event.transform);\n",
    "        }}));\n",
    "\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    res_feature_labels=json.dumps(res_feature_labels),\n",
    "    skip_feature_labels=json.dumps(skip_feature_labels),\n",
    "    res_feature_hovertips=json.dumps(res_feature_hovertips),\n",
    "    skip_feature_hovertips=json.dumps(skip_feature_hovertips),\n",
    "    res_global_id=json.dumps(global_id_res_group.tolist()),\n",
    "    skip_global_id=json.dumps(global_id_skip_group.tolist()),\n",
    "    res_connections=json.dumps(res_connections),\n",
    "    skip_connections=json.dumps(skip_connections),\n",
    "    cross_connections=json.dumps(cross_connections),\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('tiny_graph.html', 'w') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3292, device='cuda:0'), torch.Size([25000, 25000]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim[61].mean(), cos_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([0.3292, 0.2422, 0.2302, 0.2213, 0.2169, 0.2168, 0.2141, 0.2138, 0.2094,\n",
       "         0.2081], device='cuda:0'),\n",
       " indices=tensor([   61, 11614,  6768, 10452, 10741, 10453,  7194,  6326, 12940,  7447],\n",
       "        device='cuda:0')),\n",
       " tensor(0.1370, device='cuda:0'))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim.mean(dim=-1).topk(10), cos_sim.mean(dim=-1)[4288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3292, device='cuda:0') tensor(0.3417, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bos_cos_sim = sae_res_decoder[:, 61] @ sae_skip_encoder.T\n",
    "print(bos_cos_sim.mean(), bos_cos_sim.median())\n",
    "random_other_point = sae_res_decoder[:, 62] @ sae_skip_encoder.T\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(bos_cos_sim.cpu().numpy(), bins=100, alpha=0.7, label=\"BOS_T0_61\")\n",
    "plt.hist(random_other_point.cpu().numpy(), bins=100, alpha=0.7, label=\"Random_T0_62\")\n",
    "plt.title(\"Cosine Similarity of T0-Dec w/ all T1-Enc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Commented out example data, assume these are defined earlier\n",
    "# jacc_sim_res_dim0 = torch.load('path_to_res_dim0_tensor.pt')  # Replace with your actual tensor path\n",
    "# jacc_sim_skip_dim0 = torch.load('path_to_skip_dim0_tensor.pt')  # Replace with your actual tensor path\n",
    "# jacc_sim_res_to_skip = torch.load('path_to_res_to_skip_tensor.pt')  # Replace with your actual tensor path\n",
    "\n",
    "# # Set a threshold for filtering weak connections\n",
    "# threshold = 0.8\n",
    "\n",
    "# def filter_connections(matrix, threshold):\n",
    "#     indices = torch.nonzero(matrix >= threshold, as_tuple=False)\n",
    "#     filtered_connections = []\n",
    "#     for i in range(matrix.size(0)):\n",
    "#         connections = [[j.item(), matrix[i, j].item()] for j in indices[indices[:, 0] == i][:, 1]]\n",
    "#         filtered_connections.append(connections)\n",
    "#     return filtered_connections\n",
    "\n",
    "# filtered_within_res = filter_connections(jacc_sim_res_dim0, threshold)\n",
    "# filtered_within_skip = filter_connections(jacc_sim_skip_dim0, threshold)\n",
    "# filtered_res_to_skip = filter_connections(jacc_sim_res_to_skip, threshold)\n",
    "\n",
    "# def get_connected_nodes(filtered_connections):\n",
    "#     connected_nodes = set()\n",
    "#     for i, connections in enumerate(filtered_connections):\n",
    "#         if connections:\n",
    "#             connected_nodes.add(i)\n",
    "#             for j, _ in connections:\n",
    "#                 connected_nodes.add(j)\n",
    "#     return connected_nodes\n",
    "\n",
    "# connected_res_nodes = get_connected_nodes(filtered_within_res)\n",
    "# connected_skip_nodes = get_connected_nodes(filtered_within_skip)\n",
    "# connected_res_to_skip_nodes = set()\n",
    "# for i, connections in enumerate(filtered_res_to_skip):\n",
    "#     if connections:\n",
    "#         connected_res_to_skip_nodes.add(i)\n",
    "#         for j, _ in connections:\n",
    "#             connected_skip_nodes.add(j)\n",
    "\n",
    "# connected_res_nodes = connected_res_nodes.union(connected_res_to_skip_nodes)\n",
    "\n",
    "# Derive shapes from the tensor sizes\n",
    "# res_feature_labels = [\"Res_Feature_\" + str(i) for i in range(jacc_sim_res_dim0.size(0)) if i in connected_res_nodes]\n",
    "# skip_feature_labels = [\"Skip_Feature_\" + str(i) for i in range(jacc_sim_skip_dim0.size(0)) if i in connected_skip_nodes]\n",
    "# res_feature_hovertips = [\"Res_Hover_\" + str(i) for i in range(jacc_sim_res_dim0.size(0)) if i in connected_res_nodes]\n",
    "# skip_feature_hovertips = [\"Skip_Hover_\" + str(i) for i in range(jacc_sim_skip_dim0.size(0)) if i in connected_skip_nodes]\n",
    "\n",
    "# Generate HTML content with correct variable substitution\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        body, html {{\n",
    "            margin: 0;\n",
    "            height: 100%;\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .hover-label {{\n",
    "            visibility: hidden;\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "            pointer-events: none;\n",
    "            background-color: white;\n",
    "            padding: 2px;\n",
    "            border-radius: 2px;\n",
    "        }}\n",
    "        .zoomable {{\n",
    "            cursor: grab;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"100%\" height=\"100%\" class=\"zoomable\"></svg>\n",
    "    <script>\n",
    "        const resFeatureLabels = {res_feature_labels};\n",
    "        const skipFeatureLabels = {skip_feature_labels};\n",
    "        const resFeatureHovertips = {res_feature_hovertips};\n",
    "        const skipFeatureHovertips = {skip_feature_hovertips};\n",
    "        const withinResCorrelation = {within_res_correlation};\n",
    "        const withinSkipCorrelation = {within_skip_correlation};\n",
    "        const resSkipCorrelation = {res_skip_correlation};\n",
    "\n",
    "        const nodes = [];\n",
    "        const nodeIdMap = new Map();\n",
    "\n",
    "        resFeatureLabels.forEach((label, i) => {{\n",
    "            const nodeId = `res_${{i}}`;\n",
    "            nodes.push({{ id: nodeId, group: 'res', color: 'blue', defaultLabel: label, hoverLabel: resFeatureHovertips[i] }});\n",
    "            nodeIdMap.set(i, nodeId);\n",
    "        }});\n",
    "\n",
    "        skipFeatureLabels.forEach((label, i) => {{\n",
    "            const nodeId = `skip_${{i}}`;\n",
    "            nodes.push({{ id: nodeId, group: 'skip', color: 'red', defaultLabel: label, hoverLabel: skipFeatureHovertips[i] }});\n",
    "            nodeIdMap.set(i, nodeId);\n",
    "        }});\n",
    "\n",
    "        const links = [];\n",
    "        withinResCorrelation.forEach((connections, i) => {{\n",
    "            if (!Array.isArray(connections) || connections.length === 0) {{\n",
    "                return;\n",
    "            }}\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                const source = nodeIdMap.get(i);\n",
    "                const target = nodeIdMap.get(j);\n",
    "                links.push({{ source: source, target: target, value: value, color: 'blue' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        withinSkipCorrelation.forEach((connections, i) => {{\n",
    "            if (!Array.isArray(connections) || connections.length === 0) {{\n",
    "                return;\n",
    "            }}\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                const source = nodeIdMap.get(i);\n",
    "                const target = nodeIdMap.get(j);\n",
    "                links.push({{ source: source, target: target, value: value, color: 'red' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        resSkipCorrelation.forEach((connections, i) => {{\n",
    "            if (!Array.isArray(connections) || connections.length === 0) {{\n",
    "                return;\n",
    "            }}\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                const source = nodeIdMap.get(i);\n",
    "                const target = nodeIdMap.get(j + resFeatureLabels.length);\n",
    "                links.push({{ source: source, target: target, value: value, color: 'green' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = window.innerWidth,\n",
    "            height = window.innerHeight;\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(1))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-30))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2))\n",
    "            .force(\"x\", d3.forceX(width / 2).strength(0.1))\n",
    "            .force(\"y\", d3.forceY(height / 2).strength(0.1));\n",
    "\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(Math.abs(d.value)))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 5)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 1);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"visible\");\n",
    "            }})\n",
    "            .on(\"mouseout\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 0.6);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"hidden\");\n",
    "            }});\n",
    "\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"hover-labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"hover-label\")\n",
    "            .attr(\"id\", d => `hover-label-${{d.id}}`)\n",
    "            .attr(\"dy\", -30)\n",
    "            .text(d => d.hoverLabel);\n",
    "\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".hover-label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        svg.call(d3.zoom().on(\"zoom\", (event) => {{\n",
    "            svg.attr(\"transform\", event.transform);\n",
    "        }}));\n",
    "\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    res_feature_labels=json.dumps(res_feature_labels),\n",
    "    skip_feature_labels=json.dumps(skip_feature_labels),\n",
    "    res_feature_hovertips=json.dumps(res_feature_hovertips),\n",
    "    skip_feature_hovertips=json.dumps(skip_feature_hovertips),\n",
    "    within_res_correlation=json.dumps(filtered_within_res),\n",
    "    within_skip_correlation=json.dumps(filtered_within_skip),\n",
    "    res_skip_correlation=json.dumps(filtered_res_to_skip)\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('graph.html', 'w') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2539, 0.5762711763381958), (3130, 1.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_within_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Your actual data should be assigned to these variables\n",
    "# res_feature_labels = [...]  # Replace with your actual data\n",
    "# skip_feature_labels = [...]  # Replace with your actual data\n",
    "# jacc_sim_res_dim0 = torch.tensor([...])  # Replace with your actual tensor\n",
    "# jacc_sim_skip_dim0 = torch.tensor([...])  # Replace with your actual tensor\n",
    "# jacc_sim_res_to_skip = torch.tensor([...])  # Replace with your actual tensor\n",
    "\n",
    "# Set a threshold for filtering weak connections\n",
    "threshold = 0.1\n",
    "\n",
    "def filter_connections(matrix, threshold):\n",
    "    filtered_connections = []\n",
    "    for i in tqdm(range(matrix.size(0))):\n",
    "        connections = []\n",
    "        for j in range(matrix.size(1)):\n",
    "            if matrix[i, j] >= threshold:\n",
    "                connections.append((j, matrix[i, j].item()))\n",
    "        filtered_connections.append(connections)\n",
    "    return filtered_connections\n",
    "\n",
    "# Filter the matrices based on the threshold\n",
    "filtered_within_res = filter_connections(jacc_sim_res_dim0, threshold)\n",
    "filtered_within_skip = filter_connections(jacc_sim_skip_dim0, threshold)\n",
    "filtered_res_to_skip = filter_connections(jacc_sim_res_to_skip, threshold)\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .hover-label {{\n",
    "            visibility: hidden;\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "            pointer-events: none;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"1600\" height=\"900\"></svg>\n",
    "    <script>\n",
    "        // Data for the graph\n",
    "        const resFeatureLabels = {res_feature_labels};\n",
    "        const skipFeatureLabels = {skip_feature_labels};\n",
    "        const withinResCorrelation = {within_res_correlation};\n",
    "        const withinSkipCorrelation = {within_skip_correlation};\n",
    "        const resSkipCorrelation = {res_skip_correlation};\n",
    "\n",
    "        // Create nodes\n",
    "        const nodes = [];\n",
    "        for (let i = 0; i < resFeatureLabels.length; i++) {{\n",
    "            nodes.push({{ id: `res_${{i}}`, group: 'res', color: 'blue', defaultLabel: resFeatureLabels[i], hoverLabel: `Hover info for {{resFeatureLabels[i]}}` }});\n",
    "        }}\n",
    "        for (let i = 0; i < skipFeatureLabels.length; i++) {{\n",
    "            nodes.push({{ id: `skip_${{i}}`, group: 'skip', color: 'red', defaultLabel: skipFeatureLabels[i], hoverLabel: `Hover info for {{skipFeatureLabels[i]}}` }});\n",
    "        }}\n",
    "\n",
    "        // Create links (only for non-zero weights)\n",
    "        const links = [];\n",
    "        withinResCorrelation.forEach((connections, i) => {{\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                links.push({{ source: `res_${{i}}`, target: `res_${{j}}`, value: value, color: 'blue' }});\n",
    "            }});\n",
    "        }});\n",
    "        withinSkipCorrelation.forEach((connections, i) => {{\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                links.push({{ source: `skip_${{i}}`, target: `skip_${{j}}`, value: value, color: 'red' }});\n",
    "            }});\n",
    "        }});\n",
    "        resSkipCorrelation.forEach((connections, i) => {{\n",
    "            connections.forEach(([j, value]) => {{\n",
    "                links.push({{ source: `res_${{i}}`, target: `skip_${{j}}`, value: value, color: 'green' }});\n",
    "            }});\n",
    "        }});\n",
    "\n",
    "        // Set up SVG and simulation\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = +svg.attr(\"width\"),\n",
    "            height = +svg.attr(\"height\");\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(d => Math.exp(-d.value)))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-500))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
    "\n",
    "        // Add links\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(Math.abs(d.value)))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        // Add nodes\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 5)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 1);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"visible\");\n",
    "            }})\n",
    "            .on(\"mouseout\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 0.6);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"hidden\");\n",
    "            }});\n",
    "\n",
    "        // Add visible labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        // Add hover labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"hover-labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"hover-label\")\n",
    "            .attr(\"id\", d => `hover-label-${{d.id}}`)\n",
    "            .attr(\"dy\", -30)\n",
    "            .text(d => d.hoverLabel);\n",
    "\n",
    "        // Update simulation on each tick\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".hover-label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        // Drag functions\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    res_feature_labels=res_feature_labels,\n",
    "    skip_feature_labels=skip_feature_labels,\n",
    "    within_res_correlation=filtered_within_res,\n",
    "    within_skip_correlation=filtered_within_skip,\n",
    "    res_skip_correlation=filtered_res_to_skip\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('graph.html', 'w') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_within_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2020103 connections above 0.01\n",
      " 1856736 connections above 0.01\n",
      " 2166155 connections above 0.01\n",
      "------------------------------------------------------------\n",
      " 982590 connections above 0.05\n",
      " 590663 connections above 0.05\n",
      " 1078060 connections above 0.05\n",
      "------------------------------------------------------------\n",
      " 395397 connections above 0.1\n",
      " 228890 connections above 0.1\n",
      " 467454 connections above 0.1\n",
      "------------------------------------------------------------\n",
      " 107448 connections above 0.2\n",
      " 55632 connections above 0.2\n",
      " 116926 connections above 0.2\n",
      "------------------------------------------------------------\n",
      " 47336 connections above 0.3\n",
      " 20386 connections above 0.3\n",
      " 43322 connections above 0.3\n",
      "------------------------------------------------------------\n",
      " 25744 connections above 0.4\n",
      " 9977 connections above 0.4\n",
      " 21734 connections above 0.4\n",
      "------------------------------------------------------------\n",
      " 15718 connections above 0.5\n",
      " 6015 connections above 0.5\n",
      " 13693 connections above 0.5\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0.01,0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    print(f\" {(jacc_sim_skip_dim0 > threshold).sum()} connections above {threshold}\")\n",
    "    print(f\" {(jacc_sim_res_dim0 > threshold).sum()} connections above {threshold}\")\n",
    "    print(f\" {(jacc_sim_res_to_skip > threshold).sum()} connections above {threshold}\")\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example data\n",
    "num_features = 5\n",
    "within_res_correlation = torch.randn(num_features, num_features).tolist()\n",
    "res_skip_correlation = torch.randn(num_features, num_features).tolist()\n",
    "within_skip_correlation = torch.randn(num_features, num_features).tolist()\n",
    "default_labels = [f'Feature_{i}' for i in range(num_features)]\n",
    "hover_labels = [f'Hover info for Feature_{i}' for i in range(num_features)]\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Feature Correlation Graph</title>\n",
    "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
    "    <style>\n",
    "        .node {{\n",
    "            stroke: #fff;\n",
    "            stroke-width: 1.5px;\n",
    "            opacity: 0.6;\n",
    "        }}\n",
    "        .link {{\n",
    "            stroke: #999;\n",
    "            stroke-opacity: 0.6;\n",
    "        }}\n",
    "        .label {{\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "        }}\n",
    "        .hover-label {{\n",
    "            visibility: hidden;\n",
    "            font: 12px sans-serif;\n",
    "            text-anchor: middle;\n",
    "            pointer-events: none;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <svg width=\"960\" height=\"600\"></svg>\n",
    "    <script>\n",
    "        // Data for the graph\n",
    "        const numFeatures = {num_features};\n",
    "        const withinResCorrelation = {within_res_correlation};\n",
    "        const resSkipCorrelation = {res_skip_correlation};\n",
    "        const withinSkipCorrelation = {within_skip_correlation};\n",
    "        const defaultLabels = {default_labels};\n",
    "        const hoverLabels = {hover_labels};\n",
    "\n",
    "        // Create nodes\n",
    "        const nodes = [];\n",
    "        for (let i = 0; i < numFeatures; i++) {{\n",
    "            nodes.push({{ id: `res_${{i}}`, group: 'res', color: 'blue', defaultLabel: defaultLabels[i], hoverLabel: hoverLabels[i] }});\n",
    "            nodes.push({{ id: `skip_${{i}}`, group: 'skip', color: 'red', defaultLabel: defaultLabels[i], hoverLabel: hoverLabels[i] }});\n",
    "        }}\n",
    "\n",
    "        // Create links\n",
    "        const links = [];\n",
    "        for (let i = 0; i < numFeatures; i++) {{\n",
    "            for (let j = 0; j < numFeatures; j++) {{\n",
    "                if (i !== j) {{\n",
    "                    links.push({{ source: `res_${{i}}`, target: `res_${{j}}`, value: withinResCorrelation[i][j], color: 'blue' }});\n",
    "                    links.push({{ source: `skip_${{i}}`, target: `skip_${{j}}`, value: withinSkipCorrelation[i][j], color: 'red' }});\n",
    "                }}\n",
    "                links.push({{ source: `res_${{i}}`, target: `skip_${{j}}`, value: resSkipCorrelation[i][j], color: 'green' }});\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "        // Set up SVG and simulation\n",
    "        const svg = d3.select(\"svg\"),\n",
    "            width = +svg.attr(\"width\"),\n",
    "            height = +svg.attr(\"height\");\n",
    "\n",
    "        const simulation = d3.forceSimulation(nodes)\n",
    "            .force(\"link\", d3.forceLink(links).id(d => d.id).distance(50).strength(1))\n",
    "            .force(\"charge\", d3.forceManyBody().strength(-400))\n",
    "            .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
    "\n",
    "        // Add links\n",
    "        const link = svg.append(\"g\")\n",
    "            .attr(\"class\", \"links\")\n",
    "            .selectAll(\"line\")\n",
    "            .data(links)\n",
    "            .enter().append(\"line\")\n",
    "            .attr(\"stroke-width\", d => Math.sqrt(d.value))\n",
    "            .attr(\"stroke\", d => d.color);\n",
    "\n",
    "        // Add nodes\n",
    "        const node = svg.append(\"g\")\n",
    "            .attr(\"class\", \"nodes\")\n",
    "            .selectAll(\"circle\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"circle\")\n",
    "            .attr(\"r\", 10)\n",
    "            .attr(\"fill\", d => d.color)\n",
    "            .attr(\"opacity\", 0.6)\n",
    "            .call(d3.drag()\n",
    "                .on(\"start\", dragstarted)\n",
    "                .on(\"drag\", dragged)\n",
    "                .on(\"end\", dragended))\n",
    "            .on(\"mouseover\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 1);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"visible\");\n",
    "            }})\n",
    "            .on(\"mouseout\", function(event, d) {{\n",
    "                d3.select(this).attr(\"opacity\", 0.6);\n",
    "                svg.select(`#hover-label-${{d.id}}`).style(\"visibility\", \"hidden\");\n",
    "            }});\n",
    "\n",
    "        // Add visible labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"label\")\n",
    "            .attr(\"dy\", -15)\n",
    "            .text(d => d.defaultLabel);\n",
    "\n",
    "        // Add hover labels\n",
    "        svg.append(\"g\")\n",
    "            .attr(\"class\", \"hover-labels\")\n",
    "            .selectAll(\"text\")\n",
    "            .data(nodes)\n",
    "            .enter().append(\"text\")\n",
    "            .attr(\"class\", \"hover-label\")\n",
    "            .attr(\"id\", d => `hover-label-${{d.id}}`)\n",
    "            .attr(\"dy\", -30)\n",
    "            .text(d => d.hoverLabel);\n",
    "\n",
    "        // Update simulation on each tick\n",
    "        simulation.on(\"tick\", () => {{\n",
    "            link\n",
    "                .attr(\"x1\", d => d.source.x)\n",
    "                .attr(\"y1\", d => d.source.y)\n",
    "                .attr(\"x2\", d => d.target.x)\n",
    "                .attr(\"y2\", d => d.target.y);\n",
    "\n",
    "            node\n",
    "                .attr(\"cx\", d => d.x)\n",
    "                .attr(\"cy\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "\n",
    "            svg.selectAll(\".hover-label\")\n",
    "                .attr(\"x\", d => d.x)\n",
    "                .attr(\"y\", d => d.y);\n",
    "        }});\n",
    "\n",
    "        // Drag functions\n",
    "        function dragstarted(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0.3).restart();\n",
    "            d.fx = d.x;\n",
    "            d.fy = d.y;\n",
    "        }}\n",
    "\n",
    "        function dragged(event, d) {{\n",
    "            d.fx = event.x;\n",
    "            d.fy = event.y;\n",
    "        }}\n",
    "\n",
    "        function dragended(event, d) {{\n",
    "            if (!event.active) simulation.alphaTarget(0);\n",
    "            d.fx = null;\n",
    "            d.fy = null;\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Generate the HTML content\n",
    "html_content = html_template.format(\n",
    "    num_features=num_features,\n",
    "    within_res_correlation=within_res_correlation,\n",
    "    res_skip_correlation=res_skip_correlation,\n",
    "    within_skip_correlation=within_skip_correlation,\n",
    "    default_labels=default_labels,\n",
    "    hover_labels=hover_labels\n",
    ")\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('graph.html', 'w') as f:\n",
    "    f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 8192])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now let's just naively multiply the decoder weight by the encoder weight\n",
    "res_dec = sae_res.decoder.weight.data\n",
    "skip_enc = skip_sae.encoder.weight.data\n",
    "combined = skip_enc @ res_dec\n",
    "# res_dec = sae_res.encoder.weight.data\n",
    "# skip_dec = skip_sae.decoder.weight.data\n",
    "# combined = res_dec@ skip_dec\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_writing -= W_writing.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'd like to fold in layer norm\n",
    "ln_mlp = model.gpt_neox.layers[layer].post_attention_layernorm\n",
    "# 1. center the weights that write into res\n",
    "centered_decoder = res_dec - res_dec.mean(dim=1, keepdim=True)\n",
    "# 2. Add in layer norm weight to the read in weight\n",
    "ln_w = ln_mlp.weight.data\n",
    "ln_b = ln_mlp.bias.data\n",
    "\n",
    "# Get weights and biases of the next linear layer\n",
    "W = skip_sae.encoder.weight.data.T  # The next linear layer weights\n",
    "# B = skip_sae.mag_bias.data  # The next linear layer biases\n",
    "B = skip_sae.gate_bias.data  # The next linear layer biases\n",
    "\n",
    "# Calculate W_eff and B_eff\n",
    "W_eff = torch.diag(ln_w) @ W\n",
    "B_eff = B + ln_b @ W\n",
    "\n",
    "# Centering the reading weights\n",
    "W_eff -= W_eff.mean(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 8192]), torch.Size([512, 8192]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centered_decoder.shape, W_eff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Point 1\n",
      "False\n",
      "fold_in_test: 0.53, MSE LN: 0.42\n",
      "Data Point 2\n",
      "False\n",
      "fold_in_test: 0.22, MSE LN: 0.26\n",
      "Data Point 3\n",
      "False\n",
      "fold_in_test: 0.49, MSE LN: 0.51\n",
      "Data Point 4\n",
      "False\n",
      "fold_in_test: 0.78, MSE LN: 0.66\n",
      "Data Point 5\n",
      "False\n",
      "fold_in_test: 0.48, MSE LN: 0.25\n",
      "Data Point 6\n",
      "False\n",
      "fold_in_test: 0.48, MSE LN: 0.54\n",
      "Data Point 7\n",
      "False\n",
      "fold_in_test: 0.60, MSE LN: 0.70\n",
      "Data Point 8\n",
      "False\n",
      "fold_in_test: 0.36, MSE LN: 0.35\n",
      "Data Point 9\n",
      "False\n",
      "fold_in_test: 0.40, MSE LN: 0.34\n",
      "Data Point 10\n",
      "False\n",
      "fold_in_test: 0.93, MSE LN: 0.68\n",
      "Data Point 11\n",
      "False\n",
      "fold_in_test: 0.92, MSE LN: 0.88\n",
      "Data Point 12\n",
      "False\n",
      "fold_in_test: 0.36, MSE LN: 0.32\n",
      "Data Point 13\n",
      "False\n",
      "fold_in_test: 0.93, MSE LN: 0.94\n",
      "Data Point 14\n",
      "False\n",
      "fold_in_test: 0.98, MSE LN: 1.08\n",
      "Data Point 15\n",
      "False\n",
      "fold_in_test: 0.81, MSE LN: 0.87\n",
      "Data Point 16\n",
      "False\n",
      "fold_in_test: 0.34, MSE LN: 0.38\n",
      "Data Point 17\n",
      "False\n",
      "fold_in_test: 0.44, MSE LN: 0.71\n",
      "Data Point 18\n",
      "False\n",
      "fold_in_test: 2.45, MSE LN: 2.28\n",
      "Data Point 19\n",
      "False\n",
      "fold_in_test: 0.47, MSE LN: 0.49\n"
     ]
    }
   ],
   "source": [
    "ln_mlp = model.gpt_neox.layers[2].post_attention_layernorm\n",
    "\n",
    "for d_point_ind in range(1,20):\n",
    "    print(f\"Data Point {d_point_ind}\")\n",
    "    f1 = dictionary_activations_res[d_point_ind].to(device)\n",
    "    f2 = dictionary_activations_skip[d_point_ind].to(device)\n",
    "    f2_hat = torch.clamp(combined @ f1, min=0)\n",
    "    test_f2 = torch.clamp(skip_enc @ ln_mlp(res_dec@f1), min=0)\n",
    "    new_f2 = torch.clamp(skip_sae.encode(ln_mlp(sae_res.decode(f1))), min=0)\n",
    "    fold_in_test = torch.clamp((W_eff.T @ centered_decoder @ f1) + B_eff, min=0) \n",
    "    print(torch.allclose(test_f2, f2_hat, atol=1e-5))\n",
    "    top_ind = f2.topk(5).indices\n",
    "    # for ind in top_ind:\n",
    "    #     print(f\"{fold_in_test[ind]:.2f}, {test_f2[ind]:.2f}, {f2[ind]:.2f}\")\n",
    "    mse = torch.mean((f2[top_ind] - new_f2[top_ind])**2)\n",
    "    mse_ln = torch.mean((f2[top_ind] - test_f2[top_ind])**2)\n",
    "    print(f\"fold_in_test: {mse:.2f}, MSE LN: {mse_ln:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Point 0\n",
      "False\n",
      "19.93, 4.91, 18.92\n",
      "7.09, 1.42, 4.08\n",
      "5.26, 1.19, 2.86\n",
      "5.00, 1.00, 2.73\n",
      "4.04, 0.76, 2.53\n",
      "3.57, 0.69, 2.49\n",
      "4.49, 0.83, 2.47\n",
      "4.04, 0.82, 2.46\n",
      "3.16, 0.65, 2.27\n",
      "3.43, 0.46, 2.26\n",
      "3.73, 0.68, 2.26\n",
      "3.67, 0.78, 2.06\n",
      "3.16, 0.66, 1.88\n",
      "3.16, 0.62, 1.87\n",
      "3.45, 0.52, 1.82\n",
      "Data Point 1\n",
      "False\n",
      "-12.38, -38.03, 2.54\n",
      "1.54, 4.47, 1.85\n",
      "-0.70, -3.41, 0.76\n",
      "0.08, 0.11, 0.26\n",
      "-0.45, -1.92, 0.24\n",
      "-0.89, -3.04, 0.21\n",
      "0.22, 0.24, 0.21\n",
      "0.85, 1.73, 0.20\n",
      "0.46, 0.84, 0.20\n",
      "0.34, 0.13, 0.19\n",
      "0.18, 0.20, 0.19\n",
      "0.33, 0.59, 0.17\n",
      "0.36, 0.59, 0.17\n",
      "0.35, 0.56, 0.17\n",
      "0.42, 0.77, 0.17\n",
      "Data Point 2\n",
      "False\n",
      "1.35, 4.18, 1.89\n",
      "0.13, 0.07, 0.30\n",
      "0.14, 0.28, 0.27\n",
      "0.15, -0.02, 0.24\n",
      "-0.06, -0.07, 0.22\n",
      "0.14, 0.39, 0.19\n",
      "0.00, 0.12, 0.19\n",
      "0.20, 0.31, 0.19\n",
      "-0.07, -0.31, 0.18\n",
      "0.42, 0.65, 0.18\n",
      "0.19, 0.38, 0.18\n",
      "0.20, 0.26, 0.17\n",
      "0.16, 0.21, 0.17\n",
      "0.19, 0.28, 0.16\n",
      "0.21, 0.45, 0.15\n",
      "Data Point 3\n",
      "False\n",
      "-9.75, -32.72, 3.05\n",
      "1.00, 3.33, 1.38\n",
      "-0.02, -1.41, 0.87\n",
      "0.39, 0.76, 0.58\n",
      "0.44, 0.75, 0.45\n",
      "0.26, 0.01, 0.41\n",
      "0.28, 0.41, 0.32\n",
      "-0.48, -1.96, 0.27\n",
      "0.15, 0.58, 0.22\n",
      "0.21, 0.41, 0.22\n",
      "0.52, 0.94, 0.21\n",
      "0.43, 0.65, 0.20\n",
      "-0.11, -0.01, 0.20\n",
      "0.19, 0.24, 0.19\n",
      "-0.52, -1.84, 0.19\n",
      "Data Point 4\n",
      "False\n",
      "-10.06, -31.18, 1.87\n",
      "0.92, 3.30, 1.33\n",
      "-0.31, -1.99, 0.74\n",
      "0.12, 0.05, 0.48\n",
      "0.55, 0.96, 0.30\n",
      "0.08, -0.44, 0.25\n",
      "0.20, 0.29, 0.24\n",
      "0.11, 0.36, 0.23\n",
      "-0.59, -2.19, 0.22\n",
      "0.13, 0.30, 0.22\n",
      "0.23, 0.22, 0.20\n",
      "-0.11, -0.65, 0.20\n",
      "0.27, 0.38, 0.20\n",
      "0.47, 0.64, 0.19\n",
      "-0.26, -1.44, 0.19\n",
      "Data Point 5\n",
      "False\n",
      "-7.05, -23.88, 5.85\n",
      "1.17, 3.56, 1.54\n",
      "-0.61, -2.14, 0.38\n",
      "-0.58, -1.85, 0.29\n",
      "0.46, 0.79, 0.28\n",
      "-0.36, -1.41, 0.27\n",
      "0.44, 0.65, 0.27\n",
      "0.35, 0.47, 0.25\n",
      "0.38, 0.54, 0.24\n",
      "0.11, -0.08, 0.24\n",
      "0.12, 0.14, 0.23\n",
      "0.41, 0.62, 0.23\n",
      "-0.31, -1.31, 0.23\n",
      "-0.50, -1.83, 0.20\n",
      "0.20, 0.05, 0.20\n",
      "Data Point 6\n",
      "False\n",
      "-8.88, -29.72, 2.93\n",
      "0.33, -0.34, 1.39\n",
      "1.02, 3.65, 1.34\n",
      "0.32, 0.41, 0.60\n",
      "-0.08, -1.11, 0.52\n",
      "-0.48, -1.99, 0.48\n",
      "0.10, -0.24, 0.40\n",
      "-0.09, -0.81, 0.35\n",
      "0.55, 1.72, 0.34\n",
      "-4.67, -14.83, 0.32\n",
      "0.08, -0.41, 0.30\n",
      "0.28, 0.73, 0.28\n",
      "0.13, 0.61, 0.25\n",
      "0.34, 0.41, 0.25\n",
      "0.30, 0.47, 0.24\n",
      "Data Point 7\n",
      "False\n",
      "1.71, 4.41, 2.08\n",
      "0.46, 0.54, 0.43\n",
      "0.32, 0.09, 0.41\n",
      "-0.30, -1.78, 0.33\n",
      "0.25, 0.43, 0.32\n",
      "0.22, 0.22, 0.22\n",
      "0.26, 0.29, 0.19\n",
      "0.17, -0.01, 0.19\n",
      "-0.38, -0.23, 0.18\n",
      "0.33, 0.37, 0.17\n",
      "0.47, 0.53, 0.16\n",
      "0.45, 0.57, 0.15\n",
      "0.42, 0.58, 0.15\n",
      "0.46, 0.50, 0.15\n",
      "-0.11, -0.38, 0.14\n",
      "Data Point 8\n",
      "False\n",
      "1.48, 4.10, 1.90\n",
      "-0.61, -2.27, 0.50\n",
      "1.13, 2.01, 0.44\n",
      "0.64, 1.08, 0.34\n",
      "0.39, 0.40, 0.33\n",
      "0.10, 0.66, 0.29\n",
      "0.56, 0.98, 0.27\n",
      "0.41, 0.58, 0.23\n",
      "0.05, -0.20, 0.23\n",
      "0.59, 0.87, 0.22\n",
      "0.03, -0.03, 0.22\n",
      "0.13, 0.28, 0.21\n",
      "0.21, 0.21, 0.21\n",
      "0.35, 0.48, 0.20\n",
      "0.26, 0.27, 0.19\n",
      "Data Point 9\n",
      "False\n",
      "1.68, 3.79, 1.94\n",
      "0.64, 1.62, 0.47\n",
      "0.28, 0.17, 0.37\n",
      "0.26, 0.45, 0.36\n",
      "0.27, 0.34, 0.33\n",
      "0.42, 0.71, 0.31\n",
      "-0.66, -2.50, 0.25\n",
      "0.60, 1.01, 0.25\n",
      "0.55, 1.02, 0.24\n",
      "-0.89, -2.42, 0.24\n",
      "0.43, 0.40, 0.24\n",
      "-0.35, -0.45, 0.23\n",
      "0.31, 0.42, 0.22\n",
      "0.47, 0.80, 0.21\n",
      "0.33, 0.43, 0.20\n",
      "Data Point 10\n",
      "False\n",
      "0.85, 3.38, 1.26\n",
      "-0.03, -1.16, 1.08\n",
      "-0.13, -1.04, 0.59\n",
      "0.40, 0.98, 0.49\n",
      "0.44, 0.69, 0.48\n",
      "-0.21, -1.18, 0.45\n",
      "0.39, 0.16, 0.43\n",
      "0.52, 1.56, 0.39\n",
      "-0.79, -2.72, 0.34\n",
      "0.43, 0.75, 0.33\n",
      "0.72, 0.93, 0.29\n",
      "0.17, 0.46, 0.29\n",
      "0.51, 0.96, 0.28\n",
      "0.58, 0.54, 0.28\n",
      "0.14, -0.03, 0.27\n",
      "Data Point 11\n",
      "False\n",
      "-9.34, -30.71, 3.21\n",
      "-4.19, -13.50, 1.34\n",
      "0.65, 2.99, 1.02\n",
      "0.39, 0.74, 0.63\n",
      "-0.46, -2.25, 0.61\n",
      "0.52, 0.68, 0.54\n",
      "0.28, 0.41, 0.54\n",
      "-0.32, -1.46, 0.53\n",
      "-0.41, -1.81, 0.52\n",
      "-0.50, -1.81, 0.50\n",
      "-0.67, -2.41, 0.48\n",
      "-0.12, -0.70, 0.47\n",
      "-0.26, -1.43, 0.45\n",
      "-0.34, -1.64, 0.43\n",
      "0.05, -0.11, 0.42\n",
      "Data Point 12\n",
      "False\n",
      "1.18, 3.97, 1.63\n",
      "-0.26, -2.18, 0.76\n",
      "0.40, 0.56, 0.50\n",
      "-0.53, -2.17, 0.47\n",
      "0.19, 0.12, 0.30\n",
      "0.65, 1.18, 0.25\n",
      "-0.55, -1.99, 0.25\n",
      "0.08, 0.08, 0.22\n",
      "-0.69, -2.78, 0.22\n",
      "0.49, 0.84, 0.21\n",
      "0.36, 0.44, 0.21\n",
      "0.27, 0.37, 0.17\n",
      "-0.04, -0.15, 0.16\n",
      "0.30, 0.41, 0.16\n",
      "0.11, 0.19, 0.16\n",
      "Data Point 13\n",
      "False\n",
      "1.34, 4.10, 1.63\n",
      "0.40, -0.58, 1.46\n",
      "-13.92, -39.73, 0.65\n",
      "0.55, 0.86, 0.34\n",
      "-0.31, -1.90, 0.31\n",
      "-0.74, -3.15, 0.27\n",
      "0.39, 1.17, 0.23\n",
      "0.47, 0.30, 0.22\n",
      "0.15, 0.20, 0.21\n",
      "0.40, 0.48, 0.21\n",
      "0.39, 0.27, 0.21\n",
      "0.45, 0.58, 0.21\n",
      "0.19, 0.06, 0.20\n",
      "0.09, -0.20, 0.20\n",
      "0.27, 0.27, 0.19\n",
      "Data Point 14\n",
      "False\n",
      "-7.25, -19.81, 5.56\n",
      "0.82, 3.11, 1.28\n",
      "-4.81, -11.78, 1.09\n",
      "-0.04, -1.40, 1.02\n",
      "0.25, 0.17, 0.53\n",
      "-0.51, -1.62, 0.52\n",
      "0.68, 0.45, 0.40\n",
      "0.89, 0.91, 0.39\n",
      "0.78, 0.56, 0.35\n",
      "0.89, 0.95, 0.33\n",
      "-0.19, -1.26, 0.33\n",
      "-0.14, -1.01, 0.32\n",
      "0.38, 0.73, 0.31\n",
      "0.73, 0.91, 0.30\n",
      "0.80, 0.82, 0.28\n",
      "Data Point 15\n",
      "False\n",
      "-10.44, -29.31, 2.68\n",
      "1.29, 3.78, 1.61\n",
      "-0.22, 0.23, 0.41\n",
      "0.61, 0.59, 0.36\n",
      "0.17, -0.11, 0.32\n",
      "0.21, 0.32, 0.29\n",
      "0.58, 0.95, 0.28\n",
      "0.83, 1.23, 0.28\n",
      "0.36, 0.71, 0.27\n",
      "0.55, 0.62, 0.27\n",
      "0.58, 0.80, 0.26\n",
      "0.37, 0.65, 0.23\n",
      "-0.72, -2.31, 0.22\n",
      "0.55, 0.58, 0.21\n",
      "0.58, 0.92, 0.19\n",
      "Data Point 16\n",
      "False\n",
      "-3.86, -16.25, 7.56\n",
      "0.73, 2.81, 1.41\n",
      "-3.64, -11.39, 1.32\n",
      "-0.14, 0.50, 0.49\n",
      "-0.27, -0.99, 0.43\n",
      "0.56, 0.56, 0.39\n",
      "0.10, -0.25, 0.34\n",
      "-0.05, -0.75, 0.34\n",
      "-0.41, -1.61, 0.34\n",
      "-0.09, -0.49, 0.34\n",
      "0.70, 1.06, 0.34\n",
      "0.40, 0.98, 0.33\n",
      "0.89, 1.42, 0.32\n",
      "0.55, 0.95, 0.31\n",
      "0.32, 0.51, 0.31\n",
      "Data Point 17\n",
      "False\n",
      "0.89, 2.80, 1.41\n",
      "-6.89, -22.78, 1.08\n",
      "0.93, 1.45, 0.71\n",
      "-0.13, -1.06, 0.63\n",
      "-0.51, -1.84, 0.63\n",
      "0.83, 1.27, 0.62\n",
      "-0.29, -1.49, 0.53\n",
      "-0.20, -1.11, 0.47\n",
      "0.59, 0.65, 0.47\n",
      "0.02, -0.76, 0.46\n",
      "-0.14, -0.82, 0.40\n",
      "0.47, 0.68, 0.39\n",
      "-0.64, -2.88, 0.38\n",
      "0.39, 0.89, 0.37\n",
      "-0.16, -1.07, 0.37\n",
      "Data Point 18\n",
      "False\n",
      "-4.09, -18.97, 4.17\n",
      "0.59, 0.02, 1.47\n",
      "0.65, 2.71, 1.24\n",
      "0.08, -0.50, 0.73\n",
      "0.18, -0.57, 0.61\n",
      "-0.38, -1.18, 0.57\n",
      "-0.33, -1.75, 0.56\n",
      "-0.26, -1.80, 0.54\n",
      "-0.08, -1.15, 0.51\n",
      "-0.14, -0.92, 0.45\n",
      "-0.25, -1.35, 0.43\n",
      "-0.03, -0.95, 0.41\n",
      "0.33, 0.07, 0.40\n",
      "-0.13, -1.15, 0.40\n",
      "-0.08, -0.87, 0.39\n",
      "Data Point 19\n",
      "False\n",
      "-2.98, -17.91, 8.70\n",
      "-2.74, -11.49, 2.95\n",
      "1.02, 1.60, 1.96\n",
      "0.70, 3.07, 1.07\n",
      "0.43, 0.28, 0.96\n",
      "-0.44, -2.05, 0.68\n",
      "-0.22, -1.47, 0.61\n",
      "-0.23, -1.28, 0.59\n",
      "-0.29, -1.37, 0.58\n",
      "-0.06, -0.78, 0.55\n",
      "0.34, 0.87, 0.54\n",
      "-0.34, -1.71, 0.49\n",
      "0.26, 0.37, 0.49\n",
      "-0.33, -1.61, 0.47\n",
      "-0.64, -2.35, 0.46\n"
     ]
    }
   ],
   "source": [
    "ln_mlp = model.gpt_neox.layers[2].post_attention_layernorm\n",
    "\n",
    "for d_point_ind in range(20):\n",
    "    print(f\"Data Point {d_point_ind}\")\n",
    "    f1 = dictionary_activations_res[d_point_ind].to(device)\n",
    "    f2 = dictionary_activations_skip[d_point_ind].to(device)\n",
    "    f2_hat = combined @ f1\n",
    "    test_f2 = skip_enc@ ln_mlp(res_dec@f1)\n",
    "    print(torch.allclose(test_f2, f2_hat, atol=1e-5))\n",
    "    top_ind = f2.topk(15).indices\n",
    "    for ind in top_ind:\n",
    "        print(f\"{f2_hat[ind]:.2f}, {test_f2[ind]:.2f}, {f2[ind]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([18.9184,  4.0784,  2.8593,  2.7319,  2.5296,  2.4910,  2.4732,  2.4591,\n",
       "         2.2727,  2.2623,  2.2605,  2.0583,  1.8783,  1.8740,  1.8242,  1.8031,\n",
       "         1.7778,  1.7658,  1.7558,  1.7549,  1.7464,  1.7331,  1.7292,  1.7254,\n",
       "         1.6944,  1.6513,  1.6329,  1.6224,  1.6099,  1.6048,  1.5902,  1.5876,\n",
       "         1.5840,  1.5729,  1.5443,  1.5320,  1.5310,  1.5199,  1.5186,  1.5115,\n",
       "         1.4694,  1.4676,  1.4634,  1.4531,  1.4170,  1.3952,  1.3948,  1.3786,\n",
       "         1.3783,  1.3693,  1.3407,  1.3399,  1.3214,  1.2816,  1.2780,  1.2769,\n",
       "         1.2681,  1.2581,  1.2279,  1.2220,  1.2199,  1.2103,  1.1947,  1.1850,\n",
       "         1.1832,  1.1656,  1.1646,  1.1210,  1.1034,  1.1017,  1.0879,  1.0501,\n",
       "         1.0356,  1.0296,  0.9993,  0.9948,  0.9869,  0.9855,  0.9817,  0.9657,\n",
       "         0.9584,  0.9549,  0.9480,  0.9450,  0.9389,  0.9286,  0.9277,  0.9241,\n",
       "         0.9195,  0.9180,  0.9137,  0.9105,  0.9043,  0.8979,  0.8928,  0.8752,\n",
       "         0.8719,  0.8688,  0.8527,  0.8357], device='cuda:0'),\n",
       "indices=tensor([7383, 1445,  174, 3054, 1283, 3313,  736, 1837, 5547,  908, 7238,   35,\n",
       "        7795, 7681, 6838, 6169, 5350, 7216, 4856, 3537, 2737, 4431, 5316, 1913,\n",
       "        6100, 2034, 4369, 4377, 3875, 3851, 5870, 2144, 1979,  310, 1573, 1746,\n",
       "        7441, 3233, 2838, 3736,   47, 5338, 5971, 2420, 1078, 3101, 7329, 1045,\n",
       "        8157, 7774, 2162,  396, 4450, 4013, 1199, 1847, 5152, 1367, 5149, 7432,\n",
       "        1500, 7840, 1108, 5345, 4141,  317, 1397, 6185, 6128, 3455, 3406, 6877,\n",
       "        3724, 4696, 7759,  823, 4117, 4574, 4070,   70, 5221, 2032, 3362, 5476,\n",
       "        2317, 4027, 5366, 3620, 4464, 1515, 2459,  846, 5789, 6144, 5866, 7905,\n",
       "         586, 3555, 3712,  949], device='cuda:0'))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.topk(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the dist of the combined weights\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(combined.flatten().cpu().numpy(), bins=100)\n",
    "# y ax log\n",
    "plt.title(\"Resid-Dec@Skip-Enc weights\")\n",
    "# plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([4.2925, 1.7148, 1.2443, 0.6057, 0.4973, 0.4901, 0.4660, 0.4499, 0.3807,\n",
       "         0.3700], device='cuda:0'),\n",
       " indices=tensor([1477, 5432, 2995, 7856, 7523, 5225, 1493, 5216, 2743, 5933],\n",
       "        device='cuda:0')),\n",
       " torch.return_types.topk(\n",
       " values=tensor([-4.7403, -1.7134, -0.6850, -0.5077, -0.4975, -0.4813, -0.4678, -0.3732,\n",
       "         -0.3674, -0.3619], device='cuda:0'),\n",
       " indices=tensor([1477, 5432, 5225, 2995, 6262, 2743, 7523, 8154, 2448, 3895],\n",
       "        device='cuda:0')))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.max(0).values.topk(10), combined.min(0).values.topk(10, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([125.2061, 105.0183,  25.0771,   8.0769,   8.0200,   7.4735,   7.1577,\n",
       "           7.1127,   6.5055,   6.4784]),\n",
       " indices=tensor([5224, 6097, 7759, 6353, 2645, 7478, 3580, 1000, 7052,  261])),\n",
       " torch.return_types.topk(\n",
       " values=tensor([55.4205, 27.9668,  7.3213,  7.2092,  5.2458,  4.8454,  4.8416,  4.3618,\n",
       "          4.1407,  3.7042]),\n",
       " indices=tensor([1477, 7383, 1445, 5432, 1837, 3313, 3054,  174, 1283, 3736])))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_activations_res[:1000].max(0).values.topk(10), dictionary_activations_skip[:1000].max(0).values.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two random res_dec-shaped matrix, and combine them like above\n",
    "import torch.nn.functional as F\n",
    "res_dec_rand = F.normalize(torch.randn_like(res_dec), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9936, 0.9938, 0.9769,  ..., 0.7565, 0.6615, 0.7212], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = feature_acts\n",
    "x_hat_prime = skip_feature_x_hat\n",
    "# Step 1: Compute the mean of x_hat_prime along the batch dimension\n",
    "x_hat_prime_mean = torch.mean(x_hat_prime, dim=0)\n",
    "\n",
    "# Step 2: Calculate SS_tot (total sum of squares) for each feature\n",
    "ss_tot = torch.sum((x_hat_prime - x_hat_prime_mean) ** 2, dim=-1)\n",
    "\n",
    "# Step 3: Calculate SS_res (residual sum of squares) for each feature\n",
    "ss_res = torch.sum((x_hat_prime - x_hat) ** 2, dim=-1)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "# set nans to 0\n",
    "# r2[torch.isnan(r2)] = 0\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.2240, 0.0000, 0.4828], device='cuda:0'),\n",
       " tensor([1.1337, 0.0000, 0.4043], device='cuda:0'),\n",
       " tensor([0.0737,    nan, 0.1626], device='cuda:0'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_tot[:3], ss_res[:3], r2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature_mse with log x bins\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(feature_mse.numpy(), bins=np.logspace(-5, 0, 50))\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
